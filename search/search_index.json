{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Container-Native Storage Hands-on Lab\n#\n\n\nThis lab guide is designed for self-paced walk-through of the \nContainer-Native Storage 3.6 on OpenShift 3.7\n hosted on \nredhat.qwkilab.com\n. This lab gives you access to a fully functional OpenShift Container Platform 3.7 environment hosted on AWS.\n\nThis is not a tutorial on OpenShift Container Platform. Basic familiarity is assumed.\n\nCompleting the entire lab content will typically take between 2 and 4 hours.\n\n\n1. Pre-requisites\n#\n\n\nFor this lab you need to fulfil the following pre-requisites:\n\n\n\n\n\n\nWorkstation with Internet access\n\n\n\n\n\n\nSSH client supporting authentication with a private key\n\n\n\n\n\n\n2. How to start your Test Drive\n#\n\n\nTo start the Test Drive press \n button in the top bar.\n\n\nIn total you can start the lab \n5\n times. Then your quota of free lab environments has reached it\ns limit. Contact us if you want to run even more labs.\n\n\n3. Download Access Key\n#\n\n\nImmediately after pressing the \n button, the download links for the SSH keys will become active (blue) in the left hand pane in a section labeled \nConnection Details\n:\n\n\n\n\nDownload the \nPEM\n key to your computer if you are using regular OpenSSH on the command line with Linux or macOS. Choose \nDownload PPK\n if you are using PuTTY on Windows.\n\n\n4. Wait for lab provisioning to complete\n#\n\n\nWhen you started your Test Drive the lab environment provision process initiated in the background. During provisioning you can monitor progress in the top bar above this guide:\n\n\n\n\nIn total provisioning usually takes about 5 minutes and should not exceed 10 minutes.\n\nWhen your lab infrastructure is ready this progress bar disappears. Upon launch you will see a button to end your lab session and a countdown until automatic shutdown:\n\n\n\n\n5. Get the lab access credentials\n#\n\n\nWhen lab provisioning is finished successfully lab access data will displayed on the left side of this screen.\n\n\n\n\nThere is a copy-to-clipboard button next to each field to easily obtain the following data:\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nExample Value\n\n\n\n\n\n\n\n\n\n\nIP to Lab Environment\n\n\nMasterNodePublicIP\n\n\ne.g. \n34.226.81.9\n\n\n\n\n\n\nSSH user on master node\n\n\nSSHLoginUser\n\n\nec2-user\n\n\n\n\n\n\n(After OpenShift deployment) URL to OpenShift UI\n\n\nOpenShiftLoginURL\n\n\ne.g. \nhttps://34.226.81.9.nip.io:8443/\n\n\n\n\n\n\n\n\n6. Access the jump server\n#\n\n\n To login via SSH download the PEM file to your machine and change the permissions:\n\n\nchmod 0400 ~/Downloads/\npem-file-name\n\n\n\n\n\n\n Then login with the ssh client like this:\n\n\nssh -i ~/Downloads/\npem-file-name\n -l ec2-user \nMasterNodePublicIP\n\n\n\n\n\n\nThe Master node also functions as your \njumpserver\n. Once you are logged on to it you have access to all systems with the \nec2-user\n who has passwordless authentication and passwordless sudo capabilities set up everywhere.\n\n\n\n\n7. Lab Environment Overview\n#\n\n\nThe lab environment has 10 nodes in total with stable, internal IP addresses and \n/etc/hosts\n-based DNS resolution. All nodes have internet access. Only the Master can be reached from the Internet via it\ns public IP however.\n\n\n\n\n\n\n\n\nHost\n\n\nInteral FQDN\n\n\nInternal IP address\n\n\n\n\n\n\n\n\n\n\nOpenShift Master\n\n\nmaster.lab\n\n\n10.0.1.100\n\n\n\n\n\n\nOpenShift Infra Node 1\n\n\ninfra-1.lab\n\n\n10.0.2.101\n\n\n\n\n\n\nOpenShift Infra Node 2\n\n\ninfra-2.lab\n\n\n10.0.3.102\n\n\n\n\n\n\nOpenShift Infra Node 3\n\n\ninfra-3.lab\n\n\n10.0.4.103\n\n\n\n\n\n\nOpenShift App Node 1\n\n\nnode-1.lab\n\n\n10.0.2.201\n\n\n\n\n\n\nOpenShift App Node 2\n\n\nnode-2.lab\n\n\n10.0.3.202\n\n\n\n\n\n\nOpenShift App Node 3\n\n\nnode-3.lab\n\n\n10.0.4.203\n\n\n\n\n\n\nOpenShift App Node 4\n\n\nnode-4.lab\n\n\n10.0.4.204\n\n\n\n\n\n\nOpenShift App Node 5\n\n\nnode-5.lab\n\n\n10.0.3.205\n\n\n\n\n\n\nOpenShift App Node 6\n\n\nnode-6.lab\n\n\n10.0.4.206\n\n\n\n\n\n\n\n\nYou don\nt need to remember this - this is purely informational.\n\nThis labs topology roughly looks like this:\n\n\n\n\n\n\n8. Lab tips\n#\n\n\nYou might these hints useful:\n\n\n\n\nalmost all of the commands/files don\nt contain environment-specific content, there is a \ncopy-to-clipboard\n button on the right end of every code block for your convenience\n\n\nthis lab uses nip.io for DNS resolution of the public IP specific to your lab, keep that in mind when working with URLs in the lab\n\n\nyou can skip modules in this lab, unless stated otherwise in the \nOverview\n section they are independent of each other\n\n\nif you want to look at this lab guide in a separate window: this documentation is hosted at \nhttps://dmesser.github.io/ocp-3.7-cns-3.6-lab\n\n\n\n\nHave fun!", 
            "title": "Overview"
        }, 
        {
            "location": "/#welcome-to-container-native-storage-hands-on-lab", 
            "text": "This lab guide is designed for self-paced walk-through of the  Container-Native Storage 3.6 on OpenShift 3.7  hosted on  redhat.qwkilab.com . This lab gives you access to a fully functional OpenShift Container Platform 3.7 environment hosted on AWS. \nThis is not a tutorial on OpenShift Container Platform. Basic familiarity is assumed. \nCompleting the entire lab content will typically take between 2 and 4 hours.", 
            "title": "Welcome to Container-Native Storage Hands-on Lab"
        }, 
        {
            "location": "/#1-pre-requisites", 
            "text": "For this lab you need to fulfil the following pre-requisites:    Workstation with Internet access    SSH client supporting authentication with a private key", 
            "title": "1. Pre-requisites"
        }, 
        {
            "location": "/#2-how-to-start-your-test-drive", 
            "text": "To start the Test Drive press   button in the top bar.  In total you can start the lab  5  times. Then your quota of free lab environments has reached it s limit. Contact us if you want to run even more labs.", 
            "title": "2. How to start your Test Drive"
        }, 
        {
            "location": "/#3-download-access-key", 
            "text": "Immediately after pressing the   button, the download links for the SSH keys will become active (blue) in the left hand pane in a section labeled  Connection Details :   Download the  PEM  key to your computer if you are using regular OpenSSH on the command line with Linux or macOS. Choose  Download PPK  if you are using PuTTY on Windows.", 
            "title": "3. Download Access Key"
        }, 
        {
            "location": "/#4-wait-for-lab-provisioning-to-complete", 
            "text": "When you started your Test Drive the lab environment provision process initiated in the background. During provisioning you can monitor progress in the top bar above this guide:   In total provisioning usually takes about 5 minutes and should not exceed 10 minutes. \nWhen your lab infrastructure is ready this progress bar disappears. Upon launch you will see a button to end your lab session and a countdown until automatic shutdown:", 
            "title": "4. Wait for lab provisioning to complete"
        }, 
        {
            "location": "/#5-get-the-lab-access-credentials", 
            "text": "When lab provisioning is finished successfully lab access data will displayed on the left side of this screen.   There is a copy-to-clipboard button next to each field to easily obtain the following data:     Type  Name  Example Value      IP to Lab Environment  MasterNodePublicIP  e.g.  34.226.81.9    SSH user on master node  SSHLoginUser  ec2-user    (After OpenShift deployment) URL to OpenShift UI  OpenShiftLoginURL  e.g.  https://34.226.81.9.nip.io:8443/", 
            "title": "5. Get the lab access credentials"
        }, 
        {
            "location": "/#6-access-the-jump-server", 
            "text": "To login via SSH download the PEM file to your machine and change the permissions:  chmod 0400 ~/Downloads/ pem-file-name    Then login with the ssh client like this:  ssh -i ~/Downloads/ pem-file-name  -l ec2-user  MasterNodePublicIP   The Master node also functions as your  jumpserver . Once you are logged on to it you have access to all systems with the  ec2-user  who has passwordless authentication and passwordless sudo capabilities set up everywhere.", 
            "title": "6. Access the jump server"
        }, 
        {
            "location": "/#7-lab-environment-overview", 
            "text": "The lab environment has 10 nodes in total with stable, internal IP addresses and  /etc/hosts -based DNS resolution. All nodes have internet access. Only the Master can be reached from the Internet via it s public IP however.     Host  Interal FQDN  Internal IP address      OpenShift Master  master.lab  10.0.1.100    OpenShift Infra Node 1  infra-1.lab  10.0.2.101    OpenShift Infra Node 2  infra-2.lab  10.0.3.102    OpenShift Infra Node 3  infra-3.lab  10.0.4.103    OpenShift App Node 1  node-1.lab  10.0.2.201    OpenShift App Node 2  node-2.lab  10.0.3.202    OpenShift App Node 3  node-3.lab  10.0.4.203    OpenShift App Node 4  node-4.lab  10.0.4.204    OpenShift App Node 5  node-5.lab  10.0.3.205    OpenShift App Node 6  node-6.lab  10.0.4.206     You don t need to remember this - this is purely informational. \nThis labs topology roughly looks like this:", 
            "title": "7. Lab Environment Overview"
        }, 
        {
            "location": "/#8-lab-tips", 
            "text": "You might these hints useful:   almost all of the commands/files don t contain environment-specific content, there is a  copy-to-clipboard  button on the right end of every code block for your convenience  this lab uses nip.io for DNS resolution of the public IP specific to your lab, keep that in mind when working with URLs in the lab  you can skip modules in this lab, unless stated otherwise in the  Overview  section they are independent of each other  if you want to look at this lab guide in a separate window: this documentation is hosted at  https://dmesser.github.io/ocp-3.7-cns-3.6-lab   Have fun!", 
            "title": "8. Lab tips"
        }, 
        {
            "location": "/module-1-install/", 
            "text": "Overview\n\n\nThis module introduces you to the new deployment routine for CNS which is integrated in the installer of OpenShift Container Platform. You may refer back to this chapter for a before-after comparison after completing Module 5.\n\nThere are no pre-requisites for this module.\n\n\n\n\nInstalling OpenShift with CNS\n#\n\n\nYour lab environment has an installation of \nOpenShift Container Platform 3.7\n with \nContainer-Native Storage 3.6\n staged. It will take some time to deploy. So as a first step, if not already done, log on to the master node as \nec2-user\n with the SSH key you downloaded earlier:\n\n\nssh -i ~/Downloads/\npem-file-name\n -l ec2-user \nMasterNodePublicIP\n\n\n\n\n\n\nThen kick of the installation routine with \nopenshift-ansible\n\n\nansible-playbook /usr/share/openshift-ansible/playbooks/byo/config.yml\n\n\n\n\n\nIt consists of:\n\n\n\n\n1 Master node, running the master services and the router\n\n\n3 Infra nodes, running the registry\n\n\n6 App nodes, without any pods\n\n\n\n\nThe Master is the only node that\ns accessible via a public IP. All CLI commands in this lab will be done from this node.\n\n\nOpenShift has been configured with local authentication, the following users are defined:\n\n\n\n\n\n\n\n\nUser name\n\n\nPassword\n\n\nOpenShift privileges\n\n\n\n\n\n\n\n\n\n\noperator\n\n\nr3dh4t\n\n\ncluster-admin\n\n\n\n\n\n\ndeveloper\n\n\nr3dh4t\n\n\nregular user\n\n\n\n\n\n\n\n\n\n\nExploring the Lab configuration\n#\n\n\nLogin via the CLI\n#\n\n\nIn the previous chapter you logged on to the master node as \nec2-user\n.\n\n\n From here continue to log on to OpenShift using the \noc\n client:\n\n\noc login -u operator\n\n\n\n\n\nUse \nr3dh4t\n as the password.\n\n\nAuthentication required for https://master.lab:8443 (openshift)\nUsername: operator\nPassword:\nLogin successful.\n\nYou have access to the following projects and can switch between them with \noc project \nprojectname\n:\n\n  * default\n    kube-public\n    kube-system\n    logging\n    management-infra\n    openshift\n    openshift-infra\n\nUsing project \ndefault\n.\n\n\n\n\n\n Switch to the \ndefault\n namespace:\n\n\noc project default\n\n\n\n\n\nReviewing deployed components\n#\n\n\n Get a general status of the components deployed in this project:\n\n\noc status\n\n\n\n\n\nYou will see that there is a router, an internal container registry and a web-console for that registry deployed:\n\n\nIn project default on server https://master.lab:8443\n\nhttps://docker-registry-default.cloudapps.52.28.118.20.nip.io (passthrough) (svc/docker-registry)\n  dc/docker-registry deploys mirror.lab:5555/openshift3/ose-docker-registry:v3.6.173.0.21\n    deployment #1 deployed 12 minutes ago - 1 pod\n\nsvc/kubernetes - 172.30.0.1 ports 443-\n8443, 53-\n8053, 53-\n8053\n\nhttps://registry-console-default.cloudapps.52.28.118.20.nip.io (passthrough) (svc/registry-console)\n  dc/registry-console deploys mirror.lab:5555/openshift3/registry-console:v3.6\n    deployment #1 deployed 10 minutes ago - 1 pod\n\nsvc/router - 172.30.131.155 ports 80, 443, 1936\n  dc/router deploys mirror.lab:5555/openshift3/ose-haproxy-router:v3.6.173.0.21\n    deployment #1 deployed 10 minutes ago - 1 pod\n\n\n\n\n\n Display all available nodes in the system\n\n\noc get nodes\n\n\n\n\n\nYou should see 9 nodes in \nREADY\n state:\n\n\nNAME          STATUS                     AGE       VERSION\ninfra-1.lab   Ready                      14m       v1.6.1+5115d708d7\ninfra-2.lab   Ready                      14m       v1.6.1+5115d708d7\ninfra-3.lab   Ready                      14m       v1.6.1+5115d708d7\nmaster.lab    Ready,SchedulingDisabled   19m       v1.6.1+5115d708d7\nnode-1.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-2.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-3.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-4.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-5.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-6.lab    Ready                      14m       v1.6.1+5115d708d7\n\n\n\n\n\n A slight variant of that command will show us some tags (called \nlabels\n):\n\n\noc get nodes --show-labels\n\n\n\n\n\nYou should see that 3 node have the label \nrole=infra\n applied whereas the other 6 have \nrole=apps\n set. The master node has scheduling disabled so it is not burdened with additional workloads.\n\n\nNAME          STATUS                     AGE       VERSION             LABELS\ninfra-1.lab   Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra-1.lab,role=infra\ninfra-2.lab   Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra-2.lab,role=infra\ninfra-3.lab   Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra-3.lab,role=infra\nmaster.lab    Ready,SchedulingDisabled   26m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.lab,role=master\nnode-1.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node-1.lab,role=app\nnode-2.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node-2.lab,role=app\nnode-3.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node-3.lab,role=app\nnode-4.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-4.lab,role=app\nnode-5.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-5.lab,role=app\nnode-6.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-6.lab,role=app\n\n\n\n\n\nReview Registry configuration\n#\n\n\n Examine the configuration of the OpenShift internal registry:\n\n\noc describe deploymentconfig/docker-registry\n\n\n\n\n\nThere is a single instance of the registry because there is so far no shared storage in this environment:\n\n\nName:       docker-registry\nNamespace:  default\nCreated:    16 minutes ago\nLabels:     docker-registry=default\nAnnotations:    \nnone\n\nLatest Version: 1\nSelector:   docker-registry=default\nReplicas:   1\nTriggers:   Config\nStrategy:   Rolling\nTemplate:\nPod Template:\n  Labels:       docker-registry=default\n  Service Account:  registry\n  Containers:\n   registry:\n    Image:  mirror.lab:5555/openshift3/ose-docker-registry:v3.6.173.0.21\n    Port:   5000/TCP\n    Requests:\n      cpu:  100m\n      memory:   256Mi\n    Liveness:   http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3\n    Readiness:  http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\n    Environment:\n      REGISTRY_HTTP_ADDR:                   :5000\n      REGISTRY_HTTP_NET:                    tcp\n      REGISTRY_HTTP_SECRET:                 hrwyZRlJiB48Ep0XI5qER2KgJFEW8wE1bxz7jJrSgiU=\n      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:    false\n      OPENSHIFT_DEFAULT_REGISTRY:               docker-registry.default.svc:5000\n      REGISTRY_HTTP_TLS_KEY:                    /etc/secrets/registry.key\n      REGISTRY_HTTP_TLS_CERTIFICATE:                /etc/secrets/registry.crt\n    Mounts:\n      /etc/secrets from registry-certificates (rw)\n\n      /registry from registry-storage (rw)\n\n  Volumes:\n\n   registry-storage:\n\n    Type:   EmptyDir (a temporary directory that shares a pod\ns lifetime)\n\n    Medium:\n   registry-certificates:\n    Type:   Secret (a volume populated by a Secret)\n    SecretName: registry-certificates\n    Optional:   false\n\nDeployment #1 (latest):\n    Name:       docker-registry-1\n    Created:    16 minutes ago\n    Status:     Complete\n    Replicas:   1 current / 1 desired\n    Selector:   deployment=docker-registry-1,deploymentconfig=docker-registry,docker-registry=default\n    Labels:     docker-registry=default,openshift.io/deployment-config.name=docker-registry\n    Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed\n\nEvents:\n  FirstSeen LastSeen    Count   From                SubObjectPath   Type        Reason          Message\n  --------- --------    -----   ----                -------------   --------    ------          -------\n  16m       16m     1   deploymentconfig-controller         Normal      DeploymentCreatedCreated new replication controller \ndocker-registry-1\n for version 1\n\n\n\n\n\nThe highlighted lines above show that the OpenShift internal registry is currently using storage of \nemptyDir\n to store container images that developers and build process create in OpenShift.\n\n\n!!! Note:\n\n\nemptyDir\n is the simplest type of storage available in OpenShift and offers no redundancy, because it is just backed by the local filesystem of the host the mounting pod is running on.\n\n    More importantly this storage cannot be shared by multiple pods on different hosts and is by design ephemeral.\n\n    This prevents the registry from being scaled out and you will loose all the data should the registry pod experience problems and needs to be restarted, or it\ns configuration is altered and a new instance is deployed.\n\n\nClearly you cannot run like this in production. In Module 5 you will see how we can provide proper storage for the registry.\n\n\nAbsence of Logging and Metrics services\n#\n\n\nIn this deployment OpenShift has been deployed without Logging and Metrics services.\n\n\n Verify there are no Logging services currently deployed:\n\n\noc get all -n logging\n\n\n\n\n\nLog aggregation components will normally be deployed in the \nlogging\n namespace. Right now there should be nothing:\n\n\nNo resources found.\n\n\n\n\n\n Similarly note that there are no Metrics/monitoring components deployed either:\n\n\noc get all -n openshift-infra\n\n\n\n\n\nFor operating OpenShift in production environments these services however are very valuable. By nature they do not need shared storage like CNS can provide. They however do \nbenefit from the scale-out capabilities of CNS\n which typically offers capacity beyond what\ns available in a single infrastructure node - these nodes will run logging and Metrics components in OpenShift.\n\n\nAlthough not supported until the official release of CNS 3.6, in Module 5 you will get a general idea of how Logging and Metrics are deployed with CNS.\n\n\nLogin via the Web UI\n#\n\n\nLastly, you can also review the environment by logging on to the Web UI of OpenShift. You can retrieve the URL from this lab environments user interface as described in the \nOverview section\n section.\n\n\n Point your browser to the URL and log in as user \noperator\n with password \nr3dh4t\n:\n\n\n\n\n(click on the screenshot for better resolution)\n\n\nWhen successful you should see the default projects/namespaces that come with a standard OpenShift installation.", 
            "title": "Module 1 - Combined OpenShift and CNS Installation"
        }, 
        {
            "location": "/module-1-install/#installing-openshift-with-cns", 
            "text": "Your lab environment has an installation of  OpenShift Container Platform 3.7  with  Container-Native Storage 3.6  staged. It will take some time to deploy. So as a first step, if not already done, log on to the master node as  ec2-user  with the SSH key you downloaded earlier:  ssh -i ~/Downloads/ pem-file-name  -l ec2-user  MasterNodePublicIP   Then kick of the installation routine with  openshift-ansible  ansible-playbook /usr/share/openshift-ansible/playbooks/byo/config.yml  It consists of:   1 Master node, running the master services and the router  3 Infra nodes, running the registry  6 App nodes, without any pods   The Master is the only node that s accessible via a public IP. All CLI commands in this lab will be done from this node.  OpenShift has been configured with local authentication, the following users are defined:     User name  Password  OpenShift privileges      operator  r3dh4t  cluster-admin    developer  r3dh4t  regular user", 
            "title": "Installing OpenShift with CNS"
        }, 
        {
            "location": "/module-1-install/#exploring-the-lab-configuration", 
            "text": "", 
            "title": "Exploring the Lab configuration"
        }, 
        {
            "location": "/module-1-install/#login-via-the-cli", 
            "text": "In the previous chapter you logged on to the master node as  ec2-user .   From here continue to log on to OpenShift using the  oc  client:  oc login -u operator  Use  r3dh4t  as the password.  Authentication required for https://master.lab:8443 (openshift)\nUsername: operator\nPassword:\nLogin successful.\n\nYou have access to the following projects and can switch between them with  oc project  projectname :\n\n  * default\n    kube-public\n    kube-system\n    logging\n    management-infra\n    openshift\n    openshift-infra\n\nUsing project  default .   Switch to the  default  namespace:  oc project default", 
            "title": "Login via the CLI"
        }, 
        {
            "location": "/module-1-install/#reviewing-deployed-components", 
            "text": "Get a general status of the components deployed in this project:  oc status  You will see that there is a router, an internal container registry and a web-console for that registry deployed:  In project default on server https://master.lab:8443\n\nhttps://docker-registry-default.cloudapps.52.28.118.20.nip.io (passthrough) (svc/docker-registry)\n  dc/docker-registry deploys mirror.lab:5555/openshift3/ose-docker-registry:v3.6.173.0.21\n    deployment #1 deployed 12 minutes ago - 1 pod\n\nsvc/kubernetes - 172.30.0.1 ports 443- 8443, 53- 8053, 53- 8053\n\nhttps://registry-console-default.cloudapps.52.28.118.20.nip.io (passthrough) (svc/registry-console)\n  dc/registry-console deploys mirror.lab:5555/openshift3/registry-console:v3.6\n    deployment #1 deployed 10 minutes ago - 1 pod\n\nsvc/router - 172.30.131.155 ports 80, 443, 1936\n  dc/router deploys mirror.lab:5555/openshift3/ose-haproxy-router:v3.6.173.0.21\n    deployment #1 deployed 10 minutes ago - 1 pod   Display all available nodes in the system  oc get nodes  You should see 9 nodes in  READY  state:  NAME          STATUS                     AGE       VERSION\ninfra-1.lab   Ready                      14m       v1.6.1+5115d708d7\ninfra-2.lab   Ready                      14m       v1.6.1+5115d708d7\ninfra-3.lab   Ready                      14m       v1.6.1+5115d708d7\nmaster.lab    Ready,SchedulingDisabled   19m       v1.6.1+5115d708d7\nnode-1.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-2.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-3.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-4.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-5.lab    Ready                      14m       v1.6.1+5115d708d7\nnode-6.lab    Ready                      14m       v1.6.1+5115d708d7   A slight variant of that command will show us some tags (called  labels ):  oc get nodes --show-labels  You should see that 3 node have the label  role=infra  applied whereas the other 6 have  role=apps  set. The master node has scheduling disabled so it is not burdened with additional workloads.  NAME          STATUS                     AGE       VERSION             LABELS\ninfra-1.lab   Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra-1.lab,role=infra\ninfra-2.lab   Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra-2.lab,role=infra\ninfra-3.lab   Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=infra-3.lab,role=infra\nmaster.lab    Ready,SchedulingDisabled   26m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=master.lab,role=master\nnode-1.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node-1.lab,role=app\nnode-2.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node-2.lab,role=app\nnode-3.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,glusterfs=storage-host,kubernetes.io/hostname=node-3.lab,role=app\nnode-4.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-4.lab,role=app\nnode-5.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-5.lab,role=app\nnode-6.lab    Ready                      21m       v1.6.1+5115d708d7   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node-6.lab,role=app", 
            "title": "Reviewing deployed components"
        }, 
        {
            "location": "/module-1-install/#review-registry-configuration", 
            "text": "Examine the configuration of the OpenShift internal registry:  oc describe deploymentconfig/docker-registry  There is a single instance of the registry because there is so far no shared storage in this environment:  Name:       docker-registry\nNamespace:  default\nCreated:    16 minutes ago\nLabels:     docker-registry=default\nAnnotations:     none \nLatest Version: 1\nSelector:   docker-registry=default\nReplicas:   1\nTriggers:   Config\nStrategy:   Rolling\nTemplate:\nPod Template:\n  Labels:       docker-registry=default\n  Service Account:  registry\n  Containers:\n   registry:\n    Image:  mirror.lab:5555/openshift3/ose-docker-registry:v3.6.173.0.21\n    Port:   5000/TCP\n    Requests:\n      cpu:  100m\n      memory:   256Mi\n    Liveness:   http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3\n    Readiness:  http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\n    Environment:\n      REGISTRY_HTTP_ADDR:                   :5000\n      REGISTRY_HTTP_NET:                    tcp\n      REGISTRY_HTTP_SECRET:                 hrwyZRlJiB48Ep0XI5qER2KgJFEW8wE1bxz7jJrSgiU=\n      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:    false\n      OPENSHIFT_DEFAULT_REGISTRY:               docker-registry.default.svc:5000\n      REGISTRY_HTTP_TLS_KEY:                    /etc/secrets/registry.key\n      REGISTRY_HTTP_TLS_CERTIFICATE:                /etc/secrets/registry.crt\n    Mounts:\n      /etc/secrets from registry-certificates (rw)       /registry from registry-storage (rw)   Volumes:    registry-storage:     Type:   EmptyDir (a temporary directory that shares a pod s lifetime)     Medium:\n   registry-certificates:\n    Type:   Secret (a volume populated by a Secret)\n    SecretName: registry-certificates\n    Optional:   false\n\nDeployment #1 (latest):\n    Name:       docker-registry-1\n    Created:    16 minutes ago\n    Status:     Complete\n    Replicas:   1 current / 1 desired\n    Selector:   deployment=docker-registry-1,deploymentconfig=docker-registry,docker-registry=default\n    Labels:     docker-registry=default,openshift.io/deployment-config.name=docker-registry\n    Pods Status:    1 Running / 0 Waiting / 0 Succeeded / 0 Failed\n\nEvents:\n  FirstSeen LastSeen    Count   From                SubObjectPath   Type        Reason          Message\n  --------- --------    -----   ----                -------------   --------    ------          -------\n  16m       16m     1   deploymentconfig-controller         Normal      DeploymentCreatedCreated new replication controller  docker-registry-1  for version 1  The highlighted lines above show that the OpenShift internal registry is currently using storage of  emptyDir  to store container images that developers and build process create in OpenShift.  !!! Note:  emptyDir  is the simplest type of storage available in OpenShift and offers no redundancy, because it is just backed by the local filesystem of the host the mounting pod is running on. \n    More importantly this storage cannot be shared by multiple pods on different hosts and is by design ephemeral. \n    This prevents the registry from being scaled out and you will loose all the data should the registry pod experience problems and needs to be restarted, or it s configuration is altered and a new instance is deployed.  Clearly you cannot run like this in production. In Module 5 you will see how we can provide proper storage for the registry.", 
            "title": "Review Registry configuration"
        }, 
        {
            "location": "/module-1-install/#absence-of-logging-and-metrics-services", 
            "text": "In this deployment OpenShift has been deployed without Logging and Metrics services.   Verify there are no Logging services currently deployed:  oc get all -n logging  Log aggregation components will normally be deployed in the  logging  namespace. Right now there should be nothing:  No resources found.   Similarly note that there are no Metrics/monitoring components deployed either:  oc get all -n openshift-infra  For operating OpenShift in production environments these services however are very valuable. By nature they do not need shared storage like CNS can provide. They however do  benefit from the scale-out capabilities of CNS  which typically offers capacity beyond what s available in a single infrastructure node - these nodes will run logging and Metrics components in OpenShift.  Although not supported until the official release of CNS 3.6, in Module 5 you will get a general idea of how Logging and Metrics are deployed with CNS.", 
            "title": "Absence of Logging and Metrics services"
        }, 
        {
            "location": "/module-1-install/#login-via-the-web-ui", 
            "text": "Lastly, you can also review the environment by logging on to the Web UI of OpenShift. You can retrieve the URL from this lab environments user interface as described in the  Overview section  section.   Point your browser to the URL and log in as user  operator  with password  r3dh4t :   (click on the screenshot for better resolution)  When successful you should see the default projects/namespaces that come with a standard OpenShift installation.", 
            "title": "Login via the Web UI"
        }, 
        {
            "location": "/module-2-deploy-cns/", 
            "text": "Overview\n\n\nIn this module you will set up Container-Native Storage (CNS) in your OpenShift environment. You will use this in later modules to dynamically provision storage to be available to workloads in OpenShift.\n\nCNS is GlusterFS running in containers, orchestrated by OpenShift via a REST API. GlusterFS in turn is backed by local storage available to the OpenShift nodes.\n\nThis module has no pre-requisites.\n\n\n\n\nAll of the following tasks are carried out as the \nec2-user\n from the master node. For copy\npaste convenience we will omit the shell prompt unless necessary.\n\n\n Make sure you are logged on as the \nec2-user\n to the master node:\n\n\nhostname -f\n\n\n\n\n\nAs the output indicates, you should be on the master node:\n\n\nmaster.lab\n\n\n\n\n\n First ensure you have the correct openshift-ansible version installed on the system.\n\n\nyum list installed openshift-ansible\n\n\n\n\n\nA version higher than or equals to \n3.6.173.0.5-3\n is required to utilize the \nOpenShift Advanced Installer\n to deploy CNS.\n\n\nInstalled Packages\nopenshift-ansible.noarch                   3.6.173.0.21-2.git.0.44a4038.el7                    @rhel-7-server-ose-3.6-rpms\n\n\n\n\n\n\n\nReview the Ansible Inventory Configuration\n#\n\n\n\n\nHint\n\n\nAs of OpenShift Container Platform 3.6 it\ns possible to deploy CNS using \nopenshift-ansible\n - the \nadvanced installation method\n of OpenShift. The method of using the \ncns-deploy\n utility to install CNS components in an existing OpenShift cluster remains available but is not covered in this lab.\n\n\n\n\nInstalling CNS with \nopenshift-ansible\n means all configuration options for CNS are now managed in the Ansible inventory, a text file by which the installer determines what should be installed and where.\n\n\nAn inventory file with the correct settings for CNS has been provided for you in \n/etc/ansible/ocp-with-glusterfs\n\n\n/etc/ansible/ocp-with-glusterfs:\n\n\n[OSEv3:children]\n\n\nmasters\n\n\nnodes\n\n\nglusterfs\n\n\n\n\n[OSEv3:vars]\n\n\ndeployment_type\n=\nopenshift-enterprise\n\n\ncontainerized\n=\ntrue\n\n\nopenshift_image_tag\n=\nv3.6.173.0.21\n\n\nopenshift_master_identity_providers\n=\n[{\nname\n: \nhtpasswd\n, \nlogin\n: \ntrue\n, \nchallenge\n: \ntrue\n, \nkind\n: \nHTPasswdPasswordIdentityProvider\n, \nfilename\n: \n/etc/origin/master/htpasswd\n}]\n\n\nopenshift_master_htpasswd_users\n=\n{\ndeveloper\n: \n$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/\n,\noperator\n: \n$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/\n}\n\n\nopenshift_master_default_subdomain\n=\ncloudapps.35.158.172.55.nip.io\n\n\nopenshift_router_selector\n=\nrole=master\n\n\nopenshift_registry_selector\n=\nrole=infra\n\n\nopenshift_metrics_install_metrics\n=\nfalse\n\n\nopenshift_metrics_hawkular_hostname\n=\nhawkular-metrics.{{ openshift_master_default_subdomain }}\n\n\nopenshift_metrics_cassandra_storage_type\n=\npv\n\n\nopenshift_metrics_cassandra_pvc_size\n=\n10Gi\n\n\nopenshift_logging_install_logging\n=\nfalse\n\n\nopenshift_logging_es_pvc_size\n=\n10Gi\n\n\nopenshift_logging_es_pvc_dynamic\n=\ntrue\n\n\nopenshift_storage_glusterfs_namespace\n=\napp-storage\n\n\nopenshift_storage_glusterfs_image\n=\nrhgs3/rhgs-server-rhel7\n\n\nopenshift_storage_glusterfs_version\n=\n3.2.0-7\n\n\nopenshift_storage_glusterfs_heketi_image\n=\nrhgs3/rhgs-volmanager-rhel7\n\n\nopenshift_storage_glusterfs_heketi_version\n=\n3.2.0-11\n\n\nopenshift_docker_additional_registries\n=\nmirror.lab:5555\n\n\nopenshift_docker_insecure_registries\n=\nmirror.lab:5555\n\n\noreg_url\n=\nhttp://mirror.lab:5555/openshift3/ose-${component}:${version}\n\n\nopenshift_examples_modify_imagestreams\n=\ntrue\n\n\nopenshift_disable_check\n=\ndisk_availability,memory_availability\n\n\n\n[masters]\n\n\nmaster.lab openshift_public_hostname\n=\n35.158.172.55.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=35.158.172.55\n\n\n\n[masters:vars]\n\n\nopenshift_schedulable\n=\ntrue\n\n\nopenshift_node_labels\n=\n{\nrole\n: \nmaster\n}\n\n\n\n[nodes]\n\n\nmaster.lab openshift_public_hostname\n=\n35.158.172.55.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=35.158.172.55\n\n\ninfra-1.lab openshift_hostname\n=\ninfra-1.lab openshift_ip=10.0.2.101 openshift_node_labels=\n{\nrole\n: \ninfra\n}\n\n\ninfra-2.lab openshift_hostname\n=\ninfra-2.lab openshift_ip=10.0.3.102 openshift_node_labels=\n{\nrole\n: \ninfra\n}\n\n\ninfra-3.lab openshift_hostname\n=\ninfra-3.lab openshift_ip=10.0.4.103 openshift_node_labels=\n{\nrole\n: \ninfra\n}\n\n\nnode-1.lab openshift_hostname\n=\nnode-1.lab openshift_ip=10.0.2.201 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-2.lab openshift_hostname\n=\nnode-2.lab openshift_ip=10.0.3.202 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-3.lab openshift_hostname\n=\nnode-3.lab openshift_ip=10.0.4.203 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-4.lab openshift_hostname\n=\nnode-4.lab openshift_ip=10.0.4.204 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\n\n[glusterfs]\n\n\nnode-1.lab glusterfs_ip\n=\n10.0.2.201 glusterfs_zone=1 glusterfs_devices=\n[ \n/dev/xvdc\n ]\n\n\nnode-2.lab glusterfs_ip\n=\n10.0.3.202 glusterfs_zone=2 glusterfs_devices=\n[ \n/dev/xvdc\n ]\n\n\nnode-3.lab glusterfs_ip\n=\n10.0.4.203 glusterfs_zone=3 glusterfs_devices=\n[ \n/dev/xvdc\n ]\n\n\n\n\n\n\nThe highlighted lines show the settings relevant for CNS deployment. In summary what is provided is:\n\n\n\n\na hostgroup called \n[glusterfs]\n is created with all those OpenShift nodes that are designed to run CNS\n\n\n(optional)\n a custom name for the namespace is provided in which the CNS pods will live\n\n\n(optional)\n a specific name and version of the required container images to be used\n\n\ninformation about available block devices, zone and \n(optionally)\n IP addresses for GlusterFS traffic for each host in the \n[glusterfs]\n group\n\n\n\n\nIn every environment the following pre-requisites need to be met:\n\n\n\n\nthe designated nodes have a valid Red Hat Gluster Storage Subscription\n\n\nthe device names in \nglusterfs_devices\n should contain no data or filesystem/LVM structures\n\n\nthere need to be at least 3 nodes in the \n[glusterfs]\n host group and these should also be part of the \n[nodes]\n group\n\n\n\n\n\n\nWhat is the zone ID for?\n\n\nA zone identifies a failure domain in GlusterFS. In CNS data is by default always replicated 3 times. Reflecting these failure domains by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.\n\n\nCNS will also work without zone definitions, but it\ns less smart. This information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes.\n\nAn example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.\n\n\nIn this lab environment we have 3 different zones, because the nodes are residing in 3 distinct AWS Availability Zones.\n\n\n\n\n\n\nRun the installer\n#\n\n\n First ensure that from an Ansible-perspective the required nodes are reachable:\n\n\nansible -i /etc/ansible/ocp-with-glusterfs glusterfs -m ping\n\n\n\n\n\nAll 3 OpenShift application nodes should respond:\n\n\nnode-3.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-1.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\nnode-2.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\n\n\n\n\n\n Run the CNS installation playbook that ships as part of \nopenshift-ansible\n:\n\n\nansible-playbook -i /etc/ansible/ocp-with-glusterfs \\\n/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-glusterfs/config.yml\n\n\n\n\n\n!!! Danger \nImportant\n:\n\n    In this lab exercise you are directly invoking the CNS-related playbooks of \nopenshift-ansible\n. This is \nnot supported\n in production as of yet.\n\n    The supported way to deploy CNS with \nopenshift-ansible\n is to include the configuration in the inventory file from the very beginning and deploy it with OpenShift. Special care has been taken in this lab so that it works with an existint OCP deployment.\n\n\nOfficial support for post-deploy CNS installation with this method is planned for one of the next minor releases.\n\n\n\n\n\nThe installation will take approximately 4-5 minutes. In the meantime proceed with the next paragraph.\n\n\n\n\nWhat happens in the background\n#\n\n\nCNS provides software-defined storage hosted on OpenShift used by OpenShift. In particular it\ns based on \nRed Hat Gluster Storage\n running in OpenShift pods with direct access the host\ns network and storage device.\n\nGluster effectively virtualizes the local storage capacity of each node into a flat namespace providing scale-out, federated file storage transparently as a single mount point across the network.\n\n\nDuring the deployment you can either use the web console or the CLI tools to monitor what\ns created.\n\n\nWhen logging to the Web UI as \noperator\n, selecting the project called \napp-storage\n and navigating to \nApplications\n \n \nPods\n it will look similar to this:\n\n\n\n\nWhen done, going back to the \nOverview\n page it should look like this:\n\n\n\n\nThe pods named \nglusterfs-...\n are running GlusterFS in containers which have super-privileged access to the block device(s) and networking device (shares same IP address, reserves certain ports) of the container host:\n\n\n\n\nAt least 3, but potentially more, GlusterFS pods form a cluster (a \nTrusted Storage Pool\n in GlusterFS terminology) across which storage will transparently be replicated in a synchronous fashion.\n\n\nAfter some time you will see a 4th pod come up. \nheketi\n is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our lab environment heketi may be scheduled on one of the OpenShift App nodes. In production \nheketi\n should be configured to run on OpenShift Infrastructure nodes.\n\n\n\n\n!!! Tip:\n\n    For the extra curious - or if you still need to beat some time - here is a more detailed list of actions that are performed:\n\n\n- an OpenShift namespace is selected / created for the CNS pods\n- the permission to run CNS pods in `privileged` mode is added to the *ServiceAccount* used by CNS\n- a JSON structure is created that lays out a map of OpenShift nodes that will run CNS (called a `topology`), including information about their network and available disk devices\n- passwords are generated for user and administrative accounts of the CNS API server (`heketi`)\n- a set of templates are used to create an intermediary instance of *heketi*\n- the designated CNS nodes are labeled with a specific key-value pair\n- a `DaemonSet` configuration is created that causes a CNS pod to launch on every node matching that key-value pair\n- the intermediary instance of *heketi* uses the JSON-formatted topology to initialize the CNS pods (creating LVM and directory structures on the supplied block devices)\n- the intermediary instance of *heketi* is used to initiate GlusterFS peering so the CNS pods form a GlusterFS *Trusted Storage Pool*\n- the intermediary instance of *heketi* is used to create a GlusterFS volume to host the *heketi*-internal database (based on `BoltDB`)\n- a copy of the database of the intermediary *heketi* instance is created on that volume\n- the intermediary instance is terminated and a new *heketi* instance is deployed mounting the GlusterFS volume and the database\n- a `Service` is created in OpenShift to expose the API of *heketi* to the network\n- a `Route` is created in OpenShift to make the *heketi* pod reachable from the outside\n\n\n\n\n\n\n\nBy now the installation should have completed successfully with output similar to the below:\n\n\nPLAY RECAP ***************************************************************************************************************\ninfra-1.lab                : ok=72   changed=3    unreachable=0    failed=0\ninfra-2.lab                : ok=72   changed=3    unreachable=0    failed=0\ninfra-3.lab                : ok=72   changed=3    unreachable=0    failed=0\nlocalhost                  : ok=9    changed=0    unreachable=0    failed=0\nmaster.lab                 : ok=137  changed=34   unreachable=0    failed=0\nnode-1.lab                 : ok=86   changed=4    unreachable=0    failed=0\nnode-2.lab                 : ok=86   changed=4    unreachable=0    failed=0\nnode-3.lab                 : ok=86   changed=4    unreachable=0    failed=0\nnode-4.lab                 : ok=72   changed=3    unreachable=0    failed=0\n\nWednesday 20 September 2017  11:44:29 +0000 (0:00:00.157)       0:04:33.834 ***\n===============================================================================\nopenshift_storage_glusterfs : Wait for GlusterFS pods ------------------ 83.80s\nopenshift_storage_glusterfs : Wait for deploy-heketi pod --------------- 31.64s\nopenshift_version : Get available atomic-openshift version ------------- 19.64s\nopenshift_storage_glusterfs : Wait for heketi pod ---------------------- 10.90s\nopenshift_storage_glusterfs : Wait for copy job to finish -------------- 10.88s\nopenshift_storage_glusterfs : Delete deploy resources ------------------- 5.20s\nopenshift_storage_glusterfs : Load heketi topology ---------------------- 4.81s\nopenshift_facts : Ensure various deps are installed --------------------- 4.57s\nopenshift_storage_glusterfs : Create heketi DB volume ------------------- 3.55s\nopenshift_version : Get available atomic-openshift version -------------- 3.17s\nopenshift_storage_glusterfs : Deploy deploy-heketi pod ------------------ 3.06s\nopenshift_storage_glusterfs : Deploy heketi pod ------------------------- 3.04s\nopenshift_storage_glusterfs : Label GlusterFS nodes --------------------- 2.14s\nopenshift_storage_glusterfs : Deploy GlusterFS pods --------------------- 1.75s\nopenshift_docker_facts : Set docker facts ------------------------------- 1.61s\nopenshift_storage_glusterfs : Add service accounts to privileged SCC ---- 1.44s\nopenshift_storage_glusterfs : Verify target namespace exists ------------ 1.43s\nopenshift_docker_facts : Set docker facts ------------------------------- 1.39s\nopenshift_facts : Gather Cluster facts and set is_containerized if needed --- 1.25s\nopenshift_storage_glusterfs : Create heketi service account ------------- 1.08s\n\n\n\n\n\nNotice there are 0 failed tasks on any host.\n\n\n\n\nTest Container-native Storage\n#\n\n\nAt this stage you have deployed CNS. Let\u2019s verify all components are in place.\n\n\n If not already there on the CLI on the Master node change back to the \napp-storage\n namespace:\n\n\noc project app-storage\n\n\n\n\n\n First, verify that a new OpenShift \nStorageClass\n has been created:\n\n\noc get storageclass\n\n\n\n\n\nThe \nStorageClass\n is used later in OpenShift request storage from CNS:\n\n\nNAME                TYPE\nglusterfs-storage   kubernetes.io/glusterfs\n\n\n\n\n\n Next, list all running pods:\n\n\noc get pods -o wide\n\n\n\n\n\nYou should see all pods up and running. Highlighted below are pods that run GlusterFS containerized sharing the IP of the OpenShift node they are running on.\n\n\nNAME                      READY     STATUS    RESTARTS   AGE       IP           NODE\n\nglusterfs-storage-6p5zh   1/1       Running   0          57m       10.0.4.203   node-3.lab\n\nglusterfs-storage-9mx29   1/1       Running   0          57m       10.0.3.202   node-2.lab\n\nglusterfs-storage-ww7s2   1/1       Running   0          57m       10.0.2.201   node-1.lab\n\nheketi-storage-1-h27cg    1/1       Running   0          55m       10.131.2.4   node-4.lab\n\n\n\n\n\n\n\nNote\n\n\nThe exact pod names will be different in your environment, since they are auto-generated. Also the \nheketi\n pod might run on another node with another IP.\n\n\n\n\nTo expose heketi\u2019s API a \nService\n named \nheketi\n has been generated in OpenShift.\n\n\n Check the \nService\n with:\n\n\noc get service/heketi-storage\n\n\n\n\n\nThe output should look similar to the below:\n\n\nNAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nheketi-storage   172.30.225.240   \nnone\n        8080/TCP   1h\n\n\n\n\n\nTo also use heketi outside of OpenShift in addition to the \nService\n a \nRoute\n has been deployed.\n\n\n Display the route with:\n\n\noc get route/heketi-storage\n\n\n\n\n\nThe output should look similar to the below:\n\n\nNAME             HOST/PORT                                                   PATH      SERVICES         PORT      TERMINATION   WILDCARD\nheketi-storage   heketi-storage-app-storage.cloudapps.35.158.172.55.nip.io             heketi-storage   \nall\n                   None\n\n\n\n\n\nBased on this \nheketi\n will be available on the heketi API URL, in this example:\n\nhttp://\nheketi-storage-app-storage.cloudapps.35.158.172.55.nip.io\n\n\n!!! Note:\n\n    In your environment the URL will be slightly different. It will contain the public IPv4 address of your deployment, dynamically resolved by the nip.io service.\n\n\n You may verify this trivial health check using \ncurl\n (and an in-line \noc\n command to dynamically retrieve the \nroute\n for easy copy\npaste):\n\n\ncurl http://$(oc get route/heketi-storage -o jsonpath=\n{.spec.host}\n)/hello\n\n\n\n\n\nThis should say:\n\n\nHello from Heketi\n\n\n\n\n\nThis verifies heketi is running. To ensure it\ns functional and has been set up with authentication we are going to query it with the heketi CLI client.\n\n\nFirst, the client needs to know the heketi API URL above and the password for the built-in \nadmin\n user.\n\n\n\n\n View the generated \nadmin\n password for \nheketi\n from the pod configuration using \nYOUR specific pod name\n, e.g.\n\n\noc describe pod/heketi-storage-1-h27cg | grep HEKETI_ADMIN_KEY\n\n\n\n\n\nExample output:\n\n\nHEKETI_ADMIN_KEY:           sV7MHQ7S08N7ONJz1nnt/l/wBSK3L3w0xaEqDzG3YM4=\n\n\n\n\n\nThis information, the heketi user name, the API URL, and the password is needed whenever you want to use the \nheketi-cli\n client. So it\ns a good idea to store this in environment variables.\n\n\n Enter the following lines in your shell to conveniently retrieve and store the heketi API URL, the password of of the \nadmin\n user and the user name set to \nadmin\n:\n\n\nHEKETI_POD=$(oc get pods -l glusterfs=heketi-storage-pod -o jsonpath=\n{.items[0].metadata.name}\n)\nexport HEKETI_CLI_SERVER=http://$(oc get route/heketi-storage -o jsonpath=\n{.spec.host}\n)\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=$(oc get pod/$HEKETI_POD -o jsonpath=\n{.spec.containers[0].env[?(@.name==\nHEKETI_ADMIN_KEY\n)].value}\n)\n\n\n\n\n\n You are now able to use the heketi CLI tool:\n\n\nheketi-cli cluster list\n\n\n\n\n\nThis should list at least one cluster by it\ns UUID:\n\n\nClusters:\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\n!!! Note:\n\n    The UUID is auto-generated and will be different for you.\n\n\n Use the \nUUID unique to your environment\n and obtain more information about it:\n\n\nheketi-cli cluster info fb67f97166c58f161b85201e1fd9b8e\n\n\n\n\n\nThere should be 3 nodes and 1 volume, again displayed with their UUIDs.\n\n\nCluster id: fb67f97166c58f161b85201e1fd9b8ed\nNodes:\n22cbcd136fa40ffe766a13f305cc1e3b\nbfc006b571e85a083118054233bfb16d\nc5979019ac13b9fe02f4e4e2dc6d62cb\nVolumes:\n2415fba2b9364a65711da2a8311a663a\n\n\n\n\n\n To display a comprehensive overview of everything heketi knows about query it\ns topology:\n\n\nheketi-cli topology info\n\n\n\n\n\nYou will get lengthy output that shows what nodes and disk devices CNS has used to deploy a containerised GlusterFS cluster.\n\n\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\nVolumes:\n\nName: heketidbstorage\nSize: 2\nId: 2415fba2b9364a65711da2a8311a663a\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nMount: 10.0.2.201:heketidbstorage\nMount Options: backup-volfile-servers=10.0.3.202,10.0.4.203\nDurability Type: replicate\nReplica: 3\nSnapshot: Disabled\n\nBricks:\n  Id: 55851d8ab270112c07ab7a38d55c8045\n  Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n  Size (GiB): 2\n  Node: bfc006b571e85a083118054233bfb16d\n  Device: 41b8a921f8e6d31cb04c7dd35b6b4cf2\n\n  Id: 67161e0e607c38677a0ef3f617b8dc1e\n  Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n  Size (GiB): 2\n  Node: 22cbcd136fa40ffe766a13f305cc1e3b\n  Device: 8ea71174529a35f41fc0d1b288da6299\n\n  Id: a8bf049dcea2d5245b64a792d4b85e6b\n  Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick\n  Size (GiB): 2\n  Node: c5979019ac13b9fe02f4e4e2dc6d62cb\n  Device: 2a49883a5cb39c3b845477ff85a729ba\n\n\nNodes:\n\nNode Id: 22cbcd136fa40ffe766a13f305cc1e3b\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 2\nManagement Hostname: node-2.lab\nStorage Hostname: 10.0.3.102\nDevices:\nId:8ea71174529a35f41fc0d1b288da6299   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:67161e0e607c38677a0ef3f617b8dc1e   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n\nNode Id: bfc006b571e85a083118054233bfb16d\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 3\nManagement Hostname: node-3.lab\nStorage Hostname: 10.0.4.103\nDevices:\nId:41b8a921f8e6d31cb04c7dd35b6b4cf2   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:55851d8ab270112c07ab7a38d55c8045   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n\nNode Id: c5979019ac13b9fe02f4e4e2dc6d62cb\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 1\nManagement Hostname: node-1.lab\nStorage Hostname: 10.0.2.101\nDevices:\nId:2a49883a5cb39c3b845477ff85a729ba   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:a8bf049dcea2d5245b64a792d4b85e6b   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick\n\n\n\n\n\n\n\nSummary\n#\n\n\nWith this we have deployed a simple 3 node CNS cluster on top of OpenShift running as regular pods on app nodes. We have also verified that the API service is running and has correctly recognized the storage cluster.", 
            "title": "Module 2 - Deploying Container-Native Storage"
        }, 
        {
            "location": "/module-2-deploy-cns/#review-the-ansible-inventory-configuration", 
            "text": "Hint  As of OpenShift Container Platform 3.6 it s possible to deploy CNS using  openshift-ansible  - the  advanced installation method  of OpenShift. The method of using the  cns-deploy  utility to install CNS components in an existing OpenShift cluster remains available but is not covered in this lab.   Installing CNS with  openshift-ansible  means all configuration options for CNS are now managed in the Ansible inventory, a text file by which the installer determines what should be installed and where.  An inventory file with the correct settings for CNS has been provided for you in  /etc/ansible/ocp-with-glusterfs  /etc/ansible/ocp-with-glusterfs:  [OSEv3:children]  masters  nodes  glusterfs   [OSEv3:vars]  deployment_type = openshift-enterprise  containerized = true  openshift_image_tag = v3.6.173.0.21  openshift_master_identity_providers = [{ name :  htpasswd ,  login :  true ,  challenge :  true ,  kind :  HTPasswdPasswordIdentityProvider ,  filename :  /etc/origin/master/htpasswd }]  openshift_master_htpasswd_users = { developer :  $apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/ , operator :  $apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/ }  openshift_master_default_subdomain = cloudapps.35.158.172.55.nip.io  openshift_router_selector = role=master  openshift_registry_selector = role=infra  openshift_metrics_install_metrics = false  openshift_metrics_hawkular_hostname = hawkular-metrics.{{ openshift_master_default_subdomain }}  openshift_metrics_cassandra_storage_type = pv  openshift_metrics_cassandra_pvc_size = 10Gi  openshift_logging_install_logging = false  openshift_logging_es_pvc_size = 10Gi  openshift_logging_es_pvc_dynamic = true  openshift_storage_glusterfs_namespace = app-storage  openshift_storage_glusterfs_image = rhgs3/rhgs-server-rhel7  openshift_storage_glusterfs_version = 3.2.0-7  openshift_storage_glusterfs_heketi_image = rhgs3/rhgs-volmanager-rhel7  openshift_storage_glusterfs_heketi_version = 3.2.0-11  openshift_docker_additional_registries = mirror.lab:5555  openshift_docker_insecure_registries = mirror.lab:5555  oreg_url = http://mirror.lab:5555/openshift3/ose-${component}:${version}  openshift_examples_modify_imagestreams = true  openshift_disable_check = disk_availability,memory_availability  [masters]  master.lab openshift_public_hostname = 35.158.172.55.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=35.158.172.55  [masters:vars]  openshift_schedulable = true  openshift_node_labels = { role :  master }  [nodes]  master.lab openshift_public_hostname = 35.158.172.55.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=35.158.172.55  infra-1.lab openshift_hostname = infra-1.lab openshift_ip=10.0.2.101 openshift_node_labels= { role :  infra }  infra-2.lab openshift_hostname = infra-2.lab openshift_ip=10.0.3.102 openshift_node_labels= { role :  infra }  infra-3.lab openshift_hostname = infra-3.lab openshift_ip=10.0.4.103 openshift_node_labels= { role :  infra }  node-1.lab openshift_hostname = node-1.lab openshift_ip=10.0.2.201 openshift_node_labels= { role :  app }  node-2.lab openshift_hostname = node-2.lab openshift_ip=10.0.3.202 openshift_node_labels= { role :  app }  node-3.lab openshift_hostname = node-3.lab openshift_ip=10.0.4.203 openshift_node_labels= { role :  app }  node-4.lab openshift_hostname = node-4.lab openshift_ip=10.0.4.204 openshift_node_labels= { role :  app }  [glusterfs]  node-1.lab glusterfs_ip = 10.0.2.201 glusterfs_zone=1 glusterfs_devices= [  /dev/xvdc  ]  node-2.lab glusterfs_ip = 10.0.3.202 glusterfs_zone=2 glusterfs_devices= [  /dev/xvdc  ]  node-3.lab glusterfs_ip = 10.0.4.203 glusterfs_zone=3 glusterfs_devices= [  /dev/xvdc  ]   The highlighted lines show the settings relevant for CNS deployment. In summary what is provided is:   a hostgroup called  [glusterfs]  is created with all those OpenShift nodes that are designed to run CNS  (optional)  a custom name for the namespace is provided in which the CNS pods will live  (optional)  a specific name and version of the required container images to be used  information about available block devices, zone and  (optionally)  IP addresses for GlusterFS traffic for each host in the  [glusterfs]  group   In every environment the following pre-requisites need to be met:   the designated nodes have a valid Red Hat Gluster Storage Subscription  the device names in  glusterfs_devices  should contain no data or filesystem/LVM structures  there need to be at least 3 nodes in the  [glusterfs]  host group and these should also be part of the  [nodes]  group    What is the zone ID for?  A zone identifies a failure domain in GlusterFS. In CNS data is by default always replicated 3 times. Reflecting these failure domains by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.  CNS will also work without zone definitions, but it s less smart. This information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes. \nAn example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.  In this lab environment we have 3 different zones, because the nodes are residing in 3 distinct AWS Availability Zones.", 
            "title": "Review the Ansible Inventory Configuration"
        }, 
        {
            "location": "/module-2-deploy-cns/#run-the-installer", 
            "text": "First ensure that from an Ansible-perspective the required nodes are reachable:  ansible -i /etc/ansible/ocp-with-glusterfs glusterfs -m ping  All 3 OpenShift application nodes should respond:  node-3.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-1.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\nnode-2.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}   Run the CNS installation playbook that ships as part of  openshift-ansible :  ansible-playbook -i /etc/ansible/ocp-with-glusterfs \\\n/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-glusterfs/config.yml  !!! Danger  Important : \n    In this lab exercise you are directly invoking the CNS-related playbooks of  openshift-ansible . This is  not supported  in production as of yet. \n    The supported way to deploy CNS with  openshift-ansible  is to include the configuration in the inventory file from the very beginning and deploy it with OpenShift. Special care has been taken in this lab so that it works with an existint OCP deployment.  Official support for post-deploy CNS installation with this method is planned for one of the next minor releases.  The installation will take approximately 4-5 minutes. In the meantime proceed with the next paragraph.", 
            "title": "Run the installer"
        }, 
        {
            "location": "/module-2-deploy-cns/#what-happens-in-the-background", 
            "text": "CNS provides software-defined storage hosted on OpenShift used by OpenShift. In particular it s based on  Red Hat Gluster Storage  running in OpenShift pods with direct access the host s network and storage device. \nGluster effectively virtualizes the local storage capacity of each node into a flat namespace providing scale-out, federated file storage transparently as a single mount point across the network.  During the deployment you can either use the web console or the CLI tools to monitor what s created.  When logging to the Web UI as  operator , selecting the project called  app-storage  and navigating to  Applications     Pods  it will look similar to this:   When done, going back to the  Overview  page it should look like this:   The pods named  glusterfs-...  are running GlusterFS in containers which have super-privileged access to the block device(s) and networking device (shares same IP address, reserves certain ports) of the container host:   At least 3, but potentially more, GlusterFS pods form a cluster (a  Trusted Storage Pool  in GlusterFS terminology) across which storage will transparently be replicated in a synchronous fashion.  After some time you will see a 4th pod come up.  heketi  is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our lab environment heketi may be scheduled on one of the OpenShift App nodes. In production  heketi  should be configured to run on OpenShift Infrastructure nodes.   !!! Tip: \n    For the extra curious - or if you still need to beat some time - here is a more detailed list of actions that are performed:  - an OpenShift namespace is selected / created for the CNS pods\n- the permission to run CNS pods in `privileged` mode is added to the *ServiceAccount* used by CNS\n- a JSON structure is created that lays out a map of OpenShift nodes that will run CNS (called a `topology`), including information about their network and available disk devices\n- passwords are generated for user and administrative accounts of the CNS API server (`heketi`)\n- a set of templates are used to create an intermediary instance of *heketi*\n- the designated CNS nodes are labeled with a specific key-value pair\n- a `DaemonSet` configuration is created that causes a CNS pod to launch on every node matching that key-value pair\n- the intermediary instance of *heketi* uses the JSON-formatted topology to initialize the CNS pods (creating LVM and directory structures on the supplied block devices)\n- the intermediary instance of *heketi* is used to initiate GlusterFS peering so the CNS pods form a GlusterFS *Trusted Storage Pool*\n- the intermediary instance of *heketi* is used to create a GlusterFS volume to host the *heketi*-internal database (based on `BoltDB`)\n- a copy of the database of the intermediary *heketi* instance is created on that volume\n- the intermediary instance is terminated and a new *heketi* instance is deployed mounting the GlusterFS volume and the database\n- a `Service` is created in OpenShift to expose the API of *heketi* to the network\n- a `Route` is created in OpenShift to make the *heketi* pod reachable from the outside   By now the installation should have completed successfully with output similar to the below:  PLAY RECAP ***************************************************************************************************************\ninfra-1.lab                : ok=72   changed=3    unreachable=0    failed=0\ninfra-2.lab                : ok=72   changed=3    unreachable=0    failed=0\ninfra-3.lab                : ok=72   changed=3    unreachable=0    failed=0\nlocalhost                  : ok=9    changed=0    unreachable=0    failed=0\nmaster.lab                 : ok=137  changed=34   unreachable=0    failed=0\nnode-1.lab                 : ok=86   changed=4    unreachable=0    failed=0\nnode-2.lab                 : ok=86   changed=4    unreachable=0    failed=0\nnode-3.lab                 : ok=86   changed=4    unreachable=0    failed=0\nnode-4.lab                 : ok=72   changed=3    unreachable=0    failed=0\n\nWednesday 20 September 2017  11:44:29 +0000 (0:00:00.157)       0:04:33.834 ***\n===============================================================================\nopenshift_storage_glusterfs : Wait for GlusterFS pods ------------------ 83.80s\nopenshift_storage_glusterfs : Wait for deploy-heketi pod --------------- 31.64s\nopenshift_version : Get available atomic-openshift version ------------- 19.64s\nopenshift_storage_glusterfs : Wait for heketi pod ---------------------- 10.90s\nopenshift_storage_glusterfs : Wait for copy job to finish -------------- 10.88s\nopenshift_storage_glusterfs : Delete deploy resources ------------------- 5.20s\nopenshift_storage_glusterfs : Load heketi topology ---------------------- 4.81s\nopenshift_facts : Ensure various deps are installed --------------------- 4.57s\nopenshift_storage_glusterfs : Create heketi DB volume ------------------- 3.55s\nopenshift_version : Get available atomic-openshift version -------------- 3.17s\nopenshift_storage_glusterfs : Deploy deploy-heketi pod ------------------ 3.06s\nopenshift_storage_glusterfs : Deploy heketi pod ------------------------- 3.04s\nopenshift_storage_glusterfs : Label GlusterFS nodes --------------------- 2.14s\nopenshift_storage_glusterfs : Deploy GlusterFS pods --------------------- 1.75s\nopenshift_docker_facts : Set docker facts ------------------------------- 1.61s\nopenshift_storage_glusterfs : Add service accounts to privileged SCC ---- 1.44s\nopenshift_storage_glusterfs : Verify target namespace exists ------------ 1.43s\nopenshift_docker_facts : Set docker facts ------------------------------- 1.39s\nopenshift_facts : Gather Cluster facts and set is_containerized if needed --- 1.25s\nopenshift_storage_glusterfs : Create heketi service account ------------- 1.08s  Notice there are 0 failed tasks on any host.", 
            "title": "What happens in the background"
        }, 
        {
            "location": "/module-2-deploy-cns/#test-container-native-storage", 
            "text": "At this stage you have deployed CNS. Let\u2019s verify all components are in place.   If not already there on the CLI on the Master node change back to the  app-storage  namespace:  oc project app-storage   First, verify that a new OpenShift  StorageClass  has been created:  oc get storageclass  The  StorageClass  is used later in OpenShift request storage from CNS:  NAME                TYPE\nglusterfs-storage   kubernetes.io/glusterfs   Next, list all running pods:  oc get pods -o wide  You should see all pods up and running. Highlighted below are pods that run GlusterFS containerized sharing the IP of the OpenShift node they are running on.  NAME                      READY     STATUS    RESTARTS   AGE       IP           NODE glusterfs-storage-6p5zh   1/1       Running   0          57m       10.0.4.203   node-3.lab glusterfs-storage-9mx29   1/1       Running   0          57m       10.0.3.202   node-2.lab glusterfs-storage-ww7s2   1/1       Running   0          57m       10.0.2.201   node-1.lab heketi-storage-1-h27cg    1/1       Running   0          55m       10.131.2.4   node-4.lab   Note  The exact pod names will be different in your environment, since they are auto-generated. Also the  heketi  pod might run on another node with another IP.   To expose heketi\u2019s API a  Service  named  heketi  has been generated in OpenShift.   Check the  Service  with:  oc get service/heketi-storage  The output should look similar to the below:  NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nheketi-storage   172.30.225.240    none         8080/TCP   1h  To also use heketi outside of OpenShift in addition to the  Service  a  Route  has been deployed.   Display the route with:  oc get route/heketi-storage  The output should look similar to the below:  NAME             HOST/PORT                                                   PATH      SERVICES         PORT      TERMINATION   WILDCARD\nheketi-storage   heketi-storage-app-storage.cloudapps.35.158.172.55.nip.io             heketi-storage    all                    None  Based on this  heketi  will be available on the heketi API URL, in this example: \nhttp:// heketi-storage-app-storage.cloudapps.35.158.172.55.nip.io  !!! Note: \n    In your environment the URL will be slightly different. It will contain the public IPv4 address of your deployment, dynamically resolved by the nip.io service.   You may verify this trivial health check using  curl  (and an in-line  oc  command to dynamically retrieve the  route  for easy copy paste):  curl http://$(oc get route/heketi-storage -o jsonpath= {.spec.host} )/hello  This should say:  Hello from Heketi  This verifies heketi is running. To ensure it s functional and has been set up with authentication we are going to query it with the heketi CLI client.  First, the client needs to know the heketi API URL above and the password for the built-in  admin  user.    View the generated  admin  password for  heketi  from the pod configuration using  YOUR specific pod name , e.g.  oc describe pod/heketi-storage-1-h27cg | grep HEKETI_ADMIN_KEY  Example output:  HEKETI_ADMIN_KEY:           sV7MHQ7S08N7ONJz1nnt/l/wBSK3L3w0xaEqDzG3YM4=  This information, the heketi user name, the API URL, and the password is needed whenever you want to use the  heketi-cli  client. So it s a good idea to store this in environment variables.   Enter the following lines in your shell to conveniently retrieve and store the heketi API URL, the password of of the  admin  user and the user name set to  admin :  HEKETI_POD=$(oc get pods -l glusterfs=heketi-storage-pod -o jsonpath= {.items[0].metadata.name} )\nexport HEKETI_CLI_SERVER=http://$(oc get route/heketi-storage -o jsonpath= {.spec.host} )\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=$(oc get pod/$HEKETI_POD -o jsonpath= {.spec.containers[0].env[?(@.name== HEKETI_ADMIN_KEY )].value} )   You are now able to use the heketi CLI tool:  heketi-cli cluster list  This should list at least one cluster by it s UUID:  Clusters:\nfb67f97166c58f161b85201e1fd9b8ed  !!! Note: \n    The UUID is auto-generated and will be different for you.   Use the  UUID unique to your environment  and obtain more information about it:  heketi-cli cluster info fb67f97166c58f161b85201e1fd9b8e  There should be 3 nodes and 1 volume, again displayed with their UUIDs.  Cluster id: fb67f97166c58f161b85201e1fd9b8ed\nNodes:\n22cbcd136fa40ffe766a13f305cc1e3b\nbfc006b571e85a083118054233bfb16d\nc5979019ac13b9fe02f4e4e2dc6d62cb\nVolumes:\n2415fba2b9364a65711da2a8311a663a   To display a comprehensive overview of everything heketi knows about query it s topology:  heketi-cli topology info  You will get lengthy output that shows what nodes and disk devices CNS has used to deploy a containerised GlusterFS cluster.  Cluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\nVolumes:\n\nName: heketidbstorage\nSize: 2\nId: 2415fba2b9364a65711da2a8311a663a\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nMount: 10.0.2.201:heketidbstorage\nMount Options: backup-volfile-servers=10.0.3.202,10.0.4.203\nDurability Type: replicate\nReplica: 3\nSnapshot: Disabled\n\nBricks:\n  Id: 55851d8ab270112c07ab7a38d55c8045\n  Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n  Size (GiB): 2\n  Node: bfc006b571e85a083118054233bfb16d\n  Device: 41b8a921f8e6d31cb04c7dd35b6b4cf2\n\n  Id: 67161e0e607c38677a0ef3f617b8dc1e\n  Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n  Size (GiB): 2\n  Node: 22cbcd136fa40ffe766a13f305cc1e3b\n  Device: 8ea71174529a35f41fc0d1b288da6299\n\n  Id: a8bf049dcea2d5245b64a792d4b85e6b\n  Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick\n  Size (GiB): 2\n  Node: c5979019ac13b9fe02f4e4e2dc6d62cb\n  Device: 2a49883a5cb39c3b845477ff85a729ba\n\n\nNodes:\n\nNode Id: 22cbcd136fa40ffe766a13f305cc1e3b\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 2\nManagement Hostname: node-2.lab\nStorage Hostname: 10.0.3.102\nDevices:\nId:8ea71174529a35f41fc0d1b288da6299   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:67161e0e607c38677a0ef3f617b8dc1e   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick\n\nNode Id: bfc006b571e85a083118054233bfb16d\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 3\nManagement Hostname: node-3.lab\nStorage Hostname: 10.0.4.103\nDevices:\nId:41b8a921f8e6d31cb04c7dd35b6b4cf2   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:55851d8ab270112c07ab7a38d55c8045   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick\n\nNode Id: c5979019ac13b9fe02f4e4e2dc6d62cb\nState: online\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\nZone: 1\nManagement Hostname: node-1.lab\nStorage Hostname: 10.0.2.101\nDevices:\nId:2a49883a5cb39c3b845477ff85a729ba   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47\n  Bricks:\n    Id:a8bf049dcea2d5245b64a792d4b85e6b   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick", 
            "title": "Test Container-native Storage"
        }, 
        {
            "location": "/module-2-deploy-cns/#summary", 
            "text": "With this we have deployed a simple 3 node CNS cluster on top of OpenShift running as regular pods on app nodes. We have also verified that the API service is running and has correctly recognized the storage cluster.", 
            "title": "Summary"
        }, 
        {
            "location": "/module-3-cns-for-apps/", 
            "text": "Overview\n\n\nIn this module you will use CNS as a developer would do in OpenShift. For that purpose you will dynamically provision storage both in standalone fashion and in context of an application deployment.\n\nThis module requires that you have completed \nModule 2\n.\n\n\n\n\nOpenShift Storage 101\n#\n\n\nOpenShift uses Kubernetes\n PersistentStorage facility to dynamically allocate storage of any kind for applications. This is a fairly simple framework in which only 3 components are relevant: the storage provider, the storage volume and the request for a storage volume.\n\n\n\n\nOpenShift knows non-ephemeral storage as \npersistent\n volumes. This is storage that is decoupled from pod lifecycles. Users can request such storage by submitting a \nPersistentVolumeClaim\n to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).\n\n\nA storage provider in the system is represented by a \nStorageClass\n and is referenced in the claim. Upon receiving the claim OpenShift talks to the API of the actual storage system to provision the storage.  \n\n\nThe provisioned storage is represented in OpenShift as a \nPersistentVolume\n which can directly be used by pods to mount it.\n\n\nWith these basics defined we can try CNS in our system. First examine the \nStorageClass\n the installer has automatically created for us.\n\n\n Remain logged in as \noperator\n for now:\n\n\noc login -u operator\n\n\n\n\n\n Examine the \nStorageClass\n objects available:\n\n\noc get storageclass\n\n\n\n\n\nopenshift-ansible\n defined a \nStorageClass\n for CNS:\n\n\nNAME                TYPE\nglusterfs-storage   kubernetes.io/glusterfs\n\n\n\n\n\n Let\ns look at the details:\n\n\noc describe storageclass/glusterfs-storage\n\n\n\n\n\nThe output indicates the backing storage type: GlusterFS\n\n\nName:       glusterfs-storage\nIsDefaultClass: No\nAnnotations:    \nnone\n\nProvisioner:    kubernetes.io/glusterfs\nParameters: resturl=http://heketi-storage-app-storage.cloudapps.52.28.134.154.nip.io,restuser=admin,secretName=heketi-storage-admin-secret,secretNamespace=app-storage\n\n\n\n\n\n!!! Note:\n\n    The exact value for \nresturl\n will again be different for you because it\ns based on the \nroute\n/IP address on your system.\n\n\nThe \nProvisioner\n is a module in OpenShift/Kubernetes that can talk to the CNS API service: \nheketi\n. The parameters supplied in the \nStorageClass\n tell the \nProvisioner\n the URL of the API as well as the \nadmin\n users (defined in \nrestuser\n) password in the form of an OpenShift \nsecret\n (\nbase64\nd hash of the password).\n\nThe \nProvisioner\n is not an entity directly accessible to users.\n\n\n\n\nRequesting Storage\n#\n\n\nTo get storage provisioned via this \nStorageClass\n as a user you have to \nclaim\n storage. The object \nPersistentVolumeClaim\n (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity.\n\nAlso the access mode is set here, where \nReadWriteMany\n allows one or more container in parallel to mount and access this storage. This capability is dependent on the storage backend. In our case, with GlusterFS, we have one of the few systems that can reliable implement shared storage.\n\n\n Create a claim by specifying a file called \ncns-pvc.yml\n with the following contents:\n\n\ncns-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-container-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteMany\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n10Gi\n\n  \nstorageClassName\n:\n \nglusterfs-storage\n\n\n\n\n\n\nWith above PVC we are requesting 10 GiB of shared storage. Instead of \nReadWriteMany\n you could also have specified \nReadWriteOnly\n (for read-only) and \nReadWriteOnce\n (for non-shared storage, where only one pod can mount at a time).\n\n\n Submit the PVC to the system like so:\n\n\noc create -f cns-pvc.yml\n\n\n\n\n\n After a couple of seconds, look at the requests state with the following command:\n\n\noc get pvc\n\n\n\n\n\nYou should see the PVC listed and in \nBound\n state.\n\n\nNAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGE\nmy-container-storage   Bound     pvc-848cbc48-9fe3-11e7-83c3-022238c6a515   10Gi       RWX           glusterfs-storage   6s\n\n\n\n\n\n\n\nCaution\n\n\nIf the PVC is stuck in \nPENDING\n state you will need to investigate. Run \noc describe pvc/my-container-storage\n to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly specified in the \nPVC\n (wrong name, not specified) or (less likely here) the backing storage system has a problem (in our case: error on heketi side, incorrect URL in \nStorageClass\n, etc.)\n\n\n\n\n\n\nTip\n\n\nAlternatively, you can also do this step with the UI. Log on as \noperator\n and select any Project. Then go to the \nStorage\n tab. Select \nCreate\n storage and make selections accordingly to the PVC described before.\n\n\n\n\n\n\nWhen the claim was fulfilled successfully it is in the \nBound\n state. That means the system has successfully (via the \nStorageClass\n) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a \nPersistentVolume\n (PV) which is \nbound\n to the PVC.\n\n\n Look at the PVC for these details:\n\n\noc describe pvc/my-container-storage\n\n\n\n\n\nThe details of the PVC show all the desired properties of the requested storage and against which \nStorageClass\n it has been submitted. Since it\ns already bound thanks to dynamic provisioning it also displays the name of the \nPersistentVolume\n which was generated to fulfil the claim.\n\nThe name of the \nPV\n always follows the pattern \npvc-...\n.\n\n\nName:           my-container-storage\n\nNamespace:      app-storage\nStorageClass:   glusterfs-storage\nStatus:         Bound\n\nVolume:         pvc-848cbc48-9fe3-11e7-83c3-022238c6a515\n\nLabels:         \nnone\n\nAnnotations:    pv.kubernetes.io/bind-completed=yes\n                pv.kubernetes.io/bound-by-controller=yes\n                volume.beta.kubernetes.io/storage-provisioner=kubernetes.io/glusterfs\nCapacity:       10Gi\nAccess Modes:   RWX\nEvents:\n  FirstSeen LastSeen    Count   From                SubObjectPath   Type        Reason          Message\n  --------- --------    -----   ----                -------------   --------    ------          -------\n  10m       10m     1   persistentvolume-controller         Normal      ProvisioningSucceeded   Successfully provisioned volume pvc-848cbc48-9fe3-11e7-83c3-022238c6a515 using kubernetes.io/glusterfs\n\n\n\n\n\n\n\nNote\n\n\nThe \nPV\n name will be different in your environment since it\u2019s automatically generated.\n\n\n\n\nIn order to look at a the details of a \nPV\n in a default setup like this you need more privileges.\n\n\n Look at the corresponding \nPV\n by it\u2019s name. Use the following command which stores the exact name in an environment variable extracted by an \noc\n command for copy\npaste-friendliness:\n\n\nPV_NAME=$(oc get pvc/my-container-storage -o jsonpath=\n{.spec.volumeName}\n)\noc describe pv/${PV_NAME}\n\n\n\n\n\nThe output shows several interesting things, like the access mode (RWX = ReadWriteMany), the reclaim policy (what happens when the PV object gets deleted), the capacity and the type of storage backing this PV (in our case GlusterFS as part of CNS):\n\n\nName:           pvc-848cbc48-9fe3-11e7-83c3-022238c6a515\nLabels:         \nnone\n\nAnnotations:    pv.beta.kubernetes.io/gid=2001\n                pv.kubernetes.io/bound-by-controller=yes\n                pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs\n                volume.beta.kubernetes.io/mount-options=auto_unmount\n\nStorageClass:   glusterfs-storage\n\nStatus:         Bound\n\nClaim:          app-storage/my-container-storage\n\nReclaim Policy: Delete\n\nAccess Modes:   RWX\n\nCapacity:       10Gi\n\nMessage:\nSource:\n\n  Type:         Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n\n  EndpointsName:glusterfs-dynamic-my-container-storage\n  Path:         vol_7e1733b13e1b46c028a71590f8cfe8b5\n  ReadOnly:     false\nEvents:         \nnone\n\n\n\n\n\n\nNote how all the properties exactly match up with what the \nPVC\n requested.\n\n\n\n\nWhy is it called \nBound\n?\n\n\nOriginally \nPVs\n weren\nt automatically created. Hence in earlier documentation you may also find references about administrators actually \npre-provisioning\n \nPVs\n. Later \nPVCs\n would \npick up\n/match a suitable \nPV\n by looking at it\u2019s capacity and access mode. When successful they are \nbound\n to this \nPV\n.\n\nThis was needed for storage like NFS that does not have an API and therefore does not support \ndynamic provisioning\n. That\ns called \nstatic provisioning\n.\n\nThis kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.\n\n\n\n\nAlthough the storage is provisioned on the GlusterFS side it\ns not yet used by any application/pod/host. So let\u2019s release this storage capacity again.\n\nStorage is freed up by deleting the \nPVC\n. The \nPVC\n controls the lifecycle of the storage, not the \nPV\n.\n\n\n\n\nImportant\n\n\nNever delete \nPVs\n that are dynamically provided. They are only handles for pods mounting the storage. With dynamic provisioning storage lifecycle is entirely controlled via \nPVCs\n.\n\n\n\n\n Delete the storage by deleting the \nPVC\n like this:\n\n\noc delete pvc/my-container-storage\n\n\n\n\n\n\n\nMake CNS the default storage\n#\n\n\nFor the following example it is required to make the \nStorageClass\n that got created for CNS the system-wide default. This simplifies the following steps.\n\n\n Use the \noc patch\n command to change the definition of the \nStorageClass\n on the fly:\n\n\noc patch storageclass glusterfs-storage \\\n-p \n{\nmetadata\n: {\nannotations\n: {\nstorageclass.kubernetes.io/is-default-class\n: \ntrue\n}}}\n\n\n\n\n\n\n Look at the \nStorageClass\n again to see the change reflected:\n\n\noc describe storageclass/glusterfs-storage\n\n\n\n\n\nVerify it\ns indeed the default (see highlighted line):\n\n\nName:           glusterfs-storage\n\nIsDefaultClass: Yes\n\nAnnotations:    \nnone\n\nProvisioner:    kubernetes.io/glusterfs\nParameters:     resturl=http://heketi-storage-app-storage.cloudapps.52.28.134.154.nip.io,restuser=admin,secretName=heketi-storage-admin-secret,secretNamespace=app-storage\n\n\n\n\n\n\n\nImportant\n\n\nIt is crucial that you \ndo not skip this step\n as it is fundamental for the next example to work.\n\n\n\n\n\n\nUsing non-shared storage for databases\n#\n\n\nNormally a user doesn\u2019t request storage with a \nPVC\n directly. Rather the \nPVC\n is part of a larger template that describes the entire application stack. Such examples ship with OpenShift out of the box.\n\n\n\n\nAlternative\n\n\nThe steps described in this section to launch the Rails/Postgres example app can again also be done with the UI. For this purpose follow these steps similar to the one in Module 1:\n\n\n\n\n\n\n\n\nLog on to the OpenShift UI as the \ndeveloper\n user\n\n\n\n\n\n\nCreate a new project called \nmy-test-project\n, label and description is optional\n\n\n\n\n\n\nIn the Overview, next to the project\u2019s name select \nAdd to project\n\n\n\n\n\n\nIn the \nBrowse Catalog\n view select \nRuby\n from the list of programming languages\n\n\n\n\n\n\nSelect the example app entitled \nRails + PostgreSQL (Persistent)\n\n\n\n\n\n\n(optional) Change the \nVolume Capacity\n parameter to 5GiB\n\n\n\n\n\n\nSelect \nCreate\n to start deploying the app\n\n\n\n\n\n\nSelect \nContinue to Overview\n in the confirmation screen\n\n\n\n\n\n\nWait for the application deployment to finish and continue below at\n\n\n\n\n\n\n\n\n\n\n\n\nTo create an application from the OpenShift Example templates on the CLI follow these steps.\n\n\n Log in as \ndeveloper\n and the password \nr3dh4t\n\n\noc login -u developer\n\n\n\n\n\n Create a new project with a name of your choice:\n\n\noc new-project my-test-project\n\n\n\n\n\nTo use the example applications that ship with OpenShift we can use the \nnew-app\n command of the \noc\n client. It will allow us to specify one of the  application stack templates in the system. There are a lot of example templates that ship in the pre-defined namespace called \nopenshift\n which is the default place where \noc new-app\n will look.\n\n\nLet\ns pick a database application that definitely needs persistent storage. It\ns going to be part of a simple example blog application based on Rails and PostgreSQL.\n\n\n Instantiate this application with the following command\n\n\noc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi\n\n\n\n\n\nAmong various OpenShift resources also our \nPVC\n will be created:\n\n\n[...output omitted...]\nsecret \nrails-pgsql-persistent\n created\nservice \nrails-pgsql-persistent\n created\nroute \nrails-pgsql-persistent\n created\nimagestream \nrails-pgsql-persistent\n created\nbuildconfig \nrails-pgsql-persistent\n created\n\ndeploymentconfig \nrails-pgsql-persistent\n created\n\npersistentvolumeclaim \npostgresql\n created\nservice \npostgresql\n created\ndeploymentconfig \npostgresql\n created\n\n\n\n\n\nThe deployment process for the application stack continues in the background.\n\n\nWe have given the \nnew-app\n command an additional switch: \n-p VOLUME_CAPACITY=5Gi\n. This causes a parameter in the template called \nVOLUME_CAPACITY\n to be set to 5GiB. Parameters make templates more generic. In our case the template contains a \nPersistentVolumeClaim\n (like highlighted above) which will take it\ns size from this parameter.\n\n\n\n\nWhat other parameters does this template have?\n\n\nPlenty. If you are interested about all the variables/parameters this particular template supports, you can run \noc process openshift//rails-pgsql-persistent --parameters\n.\n\n\n\n\n\n\nWhat else does the template file contain?\n\n\nThe template describes all OpenShift resources necessary to stand up the rails pod and the postgres pod and make them accessible via services and routes. If you are curious: \noc get template/rails-pgsql-persistent -n openshift -o yaml\n\nIn essence it creates Ruby on Rails instance in a pod which functionality mimics a very basic blogging application. The blog articles are saved in a PostgreSQL database that runs in a separate pod.\n\nAbove mentioned \nPVC\n can be found there as well (around line 194) which supplies the postgres pod with persistent storage below the mount point \n/var/lib/pgsql/data\n (around line 275).\n\n\n\n\nYou can now either use the OpenShift UI (while being logged as \ndeveloper\n in the project \nmy-test-project\n) or the CLI to follow the deployment process.\n\n\nIn the UI you will observe both pods deploying like this:\n\n\n\n\n\n\n On the CLI watch the containers deploy like this:\n\n\noc get pods -w\n\n\n\n\n\nThe complete output should look like this:\n\n\nNAME                             READY     STATUS              RESTARTS   AGE\npostgresql-1-deploy              0/1       ContainerCreating   0          11s\nrails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s\nNAME                  READY     STATUS    RESTARTS   AGE\npostgresql-1-deploy   1/1       Running   0          14s\npostgresql-1-81gnm   0/1       Pending   0         0s\npostgresql-1-81gnm   0/1       Pending   0         0s\nrails-pgsql-persistent-1-build   1/1       Running   0         19s\npostgresql-1-81gnm   0/1       Pending   0         15s\npostgresql-1-81gnm   0/1       ContainerCreating   0         16s\npostgresql-1-81gnm   0/1       Running   0         47s\npostgresql-1-81gnm   1/1       Running   0         4m\npostgresql-1-deploy   0/1       Completed   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-build   0/1       Completed   0         11m\nrails-pgsql-persistent-1-deploy   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m\nrails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Completed   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\n\n\n\n\n\nExit out of the watch mode with: \nCtrl\n + \nc\n\n\n!!! Note:\n\n    It may take up to 5-7 minutes for the deployment to complete.\n\n\nIf you did it via the UI the deployment is finished when both, rails app and postgres database are up and running:\n\n[![OpenShift Rails Example Deployment](img/openshift_rails_app_create_3.png)](img/openshift_rails_app_create_3.png)\n\n\n\n\n\nYou should also see a PVC being issued and in the \nBound\n state.\n\n\n Look at the PVC created:\n\n\noc get pvc/postgresql\n\n\n\n\n\nOutput:\n\n\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   15Gi       RWO           4m\n\n\n\n\n\nNow go ahead and try out the application. The overview page in the OpenShift UI will tell you the \nroute\n which has been deployed as well (the http://\n link in the upper right hand corner). Use it and append \n/articles\n to the URL to get to the actual app.\n\n\n Otherwise get it on the CLI like this:\n\n\noc get route\n\n\n\n\n\nOutput:\n\n\nNAME                     HOST/PORT                                                               PATH      SERVICES                 PORT      TERMINATION   WILDCARD\nrails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.34.252.58.209.nip.io             rails-pgsql-persistent   \nall\n                   None\n\n\n\n\n\n!!! Note:\n\n    Again, the URL will be slightly different for you.\n\n\nFollowing this output, point your browser to the URL (prepend it with \nhttp://\n and append \n/articles\n) to reach the actual application, in this case:\n\n\nhttp://\nrails-pgsql-persistent-my-test-project.cloudapps.\nYOUR-IP-HERE>\n.nip.io\n/\narticles\n\n\nYou should be able to successfully create articles and comments. The username/password to create articles and comments is by default \nopenshift\n/\nsecret\n.\n\nWhen they are saved they are actually saved in the PostgreSQL database which stores it\u2019s table spaces on a GlusterFS volume provided by CNS.\n\n\n You can verify that the postgres pod indeed mounted the PVC under the pather where PostgreSQL normally stores it\ns data with this command:\n\n\noc volumes dc --all\n\n\n\n\n\nYou will see that the \nDeploymentConfig\n of the postgres pod did indeed include a \nPVC\n:\n\n\ndeploymentconfigs/postgresql\n  pvc/postgresql (allocated 5GiB) as postgresql-data\n    mounted at /var/lib/pgsql/data\ndeploymentconfigs/rails-pgsql-persistent\n\n\n\n\n\nNow let\u2019s take a look at how this was actually achieved.\n\n\n A normal user cannot see the details of a \nPersistentVolume\n. Log back in as \noperator\n:\n\n\noc login -u operator -n my-test-project\n\n\n\n\n\n Look at the PVC to determine the PV:\n\n\noc get pvc\n\n\n\n\n\nOutput:\n\n\nNAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   5Gi       RWO           10m\n\n\n\n\n\n\n\nNote\n\n\nYour volume (PV) name will be different as it\u2019s dynamically generated.\n\n\n\n\nThe PVC name is found in above output the \nVOLUME\n column.\n\n\n Look at the details of this PV with the following copy\npaste-friendly short-hand:\n\n\nPV_NAME=$(oc get pvc/postgresql -o jsonpath=\n{.spec.volumeName}\n)\noc describe pv/${PV_NAME}\n\n\n\n\n\nOutput shows (in highlight) the name of the volume, the backend type (GlusterFS) and the volume name GlusterFS uses internally.\n\n\nName:             pvc-c638ba71-a070-11e7-890c-02ed99595f95\n\nLabels:           \nnone\n\nAnnotations:      pv.beta.kubernetes.io/gid=2000\n                  pv.kubernetes.io/bound-by-controller=yes\n                  pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs\n                  volume.beta.kubernetes.io/mount-options=auto_unmount\nStorageClass:     glusterfs-storage\nStatus:           Bound\nClaim:            my-test-project/postgresql\nReclaim Policy:   Delete\nAccess Modes:     RWO\nCapacity:         5Gi\nMessage:\nSource:\n\n  Type:           Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n\n  EndpointsName:  glusterfs-dynamic-postgresql\n\n  Path:           vol_4b22dda4c9681f4325ba5e24cb4f64c6\n\n  ReadOnly:       false\nEvents:           \nnone\n\n\n\n\n\n\nNote the GlusterFS volume name, in this case \nvol_4b22dda4c9681f4325ba5e24cb4f64c6\n.\n\n\n Save the generated volume name of GlusterFS in a shell variable for later use:\n\n\nGLUSTER_VOL_NAME=$(oc get pv/${PV_NAME} -o jsonpath=\n{.spec.glusterfs.path}\n)\n\n\n\n\n\n Now let\u2019s switch to the namespace we used for CNS deployment:\n\n\noc project app-storage\n\n\n\n\n\n Look at the GlusterFS pods (filtered by label) running:\n\n\noc get pods -o wide -l glusterfs=storage-pod\n\n\n\n\n\nPick the first of the GlusterFS pods in the list:\n\n\nNAME                      READY     STATUS    RESTARTS   AGE       IP           NODE\n\nglusterfs-storage-16pb0   1/1       Running   0          23m       10.0.2.201   node-1.lab\n\nglusterfs-storage-37tqx   1/1       Running   0          23m       10.0.4.203   node-3.lab\nglusterfs-storage-68lxn   1/1       Running   0          23m       10.0.3.202   node-2.lab\n\n\n\n\n\nPick the first the pod in the list, in this example \nglusterfs-storage-16pb0\n and note it\ns host IP address.\n\n\n Use the following command to conveniently save it\ns name and host IP address in a shell variable for later use (copy \n paste those lines in your shell):\n\n\nFIRST_GLUSTER_POD=$(oc get pods -l glusterfs=storage-pod -o jsonpath=\n{.items[0].metadata.name}\n)\nHOST_IP=$(oc get pod/$FIRST_GLUSTER_POD -o jsonpath=\n{.status.hostIP}\n)\necho $FIRST_GLUSTER_POD\necho $HOST_IP\n\n\n\n\n\nNext we are going to use the remote session capability of the \noc\n client to execute a command in that pods namespace. We are going to leverage the GlusterFS CLI utilities being present in that pod.\n\n\n Ask GlusterFS from inside the CNS pod about all the GlusterFS volumes defined:\n\n\noc rsh $FIRST_GLUSTER_POD gluster vol list\n\n\n\n\n\nYou will see two volumes:\n\n\nheketidbstorage\nvol_4b22dda4c9681f4325ba5e24cb4f64c6\n\n\n\n\n\n\n\n\n\nheketidbstorage\n is an internal-only volume dedicated to heketi\u2019s internal database.\n\n\n\n\n\n\nthe second is the volume backing the PV of the PostgreSQL database deployed earlier, in this example \nvol_4b22dda4c9681f4325ba5e24cb4f64c6\n - your\ns will be differently named.\n\n\n\n\n\n\n Ask GlusterFS about the topology of this volume:\n\n\noc rsh $FIRST_GLUSTER_POD gluster vol info $GLUSTER_VOL_NAME\n\n\n\n\n\nThe output of the \ngluster\n command will show you how the volume has been created. You will also see that the pod you are currently logged on to serves one the bricks.\n\n\nVolume Name: vol_4b22dda4c9681f4325ba5e24cb4f64c6\nType: Replicate\nVolume ID: 37d53d51-34bc-4853-b564-3b0ea9bdd935\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\n\nBrick1: 10.0.2.201:/var/lib/heketi/mounts/vg_50f5d808e04ccab8d6fd0231c268db35/brick_4b59cd1f4a8ff8d8a3eddf7317829e73/brick\n\nBrick2: 10.0.4.203:/var/lib/heketi/mounts/vg_7cb3be478376539d0c4b54cf69688c8e/brick_688627cc5dca8d01a81fa504487116c0/brick\n\nBrick3: 10.0.3.202:/var/lib/heketi/mounts/vg_fb1a45c7853f415a3a09a164f0d717fb/brick_931730cb987383a605c1d1ff5d796fa9/brick\n\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on\n\n\n\n\n\nThe above output tells us GlusterFS created this volume as a 3-way replica set across 3 bricks. Bricks are local directories on GlusterFS nodes. They make up replication targets.\n\nIn our case the GlusterFS nodes are our CNS pods and since they share the physical hosts network they are displayed with these IP addresses (see highlighted lines) . This volume type \nReplicate\n is currently the only supported volume type in production. It synchronously replicates all data across those 3 bricks.\n\n\nLet\ns take a look at what\ns inside a brick.\n\n\n Paste this little piece of bash magic into your shell to conveniently store the brick directory from the first CNS pod you saw earlier in an environment variable:\n\n\nBRICK_DIR=$(echo -n $(oc rsh $FIRST_GLUSTER_POD gluster vol info $GLUSTER_VOL_NAME | grep $HOST_IP) | cut -d \n:\n -f 3 | tr -d $\n\\r\n )\necho $BRICK_DIR\n\n\n\n\n\n Now let\ns look at a brick directory from inside a CNS pod:\n\n\noc rsh $FIRST_GLUSTER_POD ls -ahl $BRICK_DIR\n\n\n\n\n\nWhat you see is the content of the brick directory from within the GlusterFS pod, which makes up 1 out of 3 copies of our postgres volume:\n\n\ntotal 16K\ndrwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .\ndrwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..\ndrw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs\ndrwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan\ndrwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata\n\n\n\n\n\n Going one level deeper, we see a data structure familiar to PostgreSQL users:\n\n\noc rsh $FIRST_GLUSTER_POD ls -ahl $BRICK_DIR/userdata\n\n\n\n\n\nThis is one of 3 copies of the postgres data directory hosted by CNS:\n\n\ntotal 68K\ndrwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .\ndrwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..\n-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION\ndrwx------.  6 1000080000 root   54 Jun  6 14:46 base\ndrwx------.  2 1000080000 root 8.0K Jun  6 14:47 global\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem\n-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf\n-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf\ndrwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log\ndrwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical\ndrwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact\ndrwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots\ndrwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat\ndrwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase\ndrwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog\n-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf\n-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf\n-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts\n-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid\n\n\n\n\n\nYou are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. Evidence that the database uses CNS.\n\n\nClients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol as it were an ordinary local mounts.\n\nWhen a pod starts that mounts storage from a \nPV\n backed by CNS the GlusterFS mount plugin in OpenShift will mount the GlusterFS volume on the right OpenShift node and then \nbind-mount\n this directory to the right pod\ns file namespace.\n\nThis happens transparently to the application and looks like a normal local filesystem inside the pod as you just saw. Let\ns have a look from the container host perspective:\n\n\n Get the name and the host IP of the postgres pod with this shell shortcut into environment variables for easy copy\npaste later:\n\n\nPOSTGRES_POD=$(oc get pods -l name=postgresql -n my-test-project -o jsonpath=\n{.items[0].metadata.name}\n)\nPOSTGRES_CONTAINER_HOST=$(oc get pod/$POSTGRES_POD -n my-test-project -o jsonpath=\n{.status.hostIP}\n)\necho $POSTGRES_POD\necho $POSTGRES_CONTAINER_HOST\n\n\n\n\n\nSince you are acting from the master node \nmaster.lab\n you can use SSH without password to execute a remote command on the OpenShift node hosting the postgres pod.\n\n\n Look for the GlusterFS mount points on the host, searching the GlusterFS volume that was provisioned for the database\n\n\nssh $POSTGRES_CONTAINER_HOST mount | grep $GLUSTER_VOL_NAME\n\n\n\n\n\n!!! Tip:\n\n    Answer the SSH clients question \nAre you sure you want to continue connecting (yes/no)?\n with \nyes\n.\n\n\nThe host should have mounted this GlusterFS volume, for example:\n\n\n10.0.2.201:vol_4b22dda4c9681f4325ba5e24cb4f64c6 on /var/lib/origin/openshift.local.volumes/pods/c7029a5a-a070-11e7-890c-02ed99595f95/volumes/kubernetes.io~glusterfs/pvc-c638ba71-a070-11e7-890c-02ed99595f95 type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)\n\n\n\n\n\nThis sums up the relationship between \nPVCs\n, \nPVs\n, GlusterFS volumes and container mounts in CNS.\n\n\nThe mounting and unmounting of GlusterFS volumes is faciliated automatically by the GlusterFS mount plugin that ships with OpenShift.\n\n\n\n\nProviding shared storage to multiple application instances\n#\n\n\nIn the previous example we provisioned an RWO PV - the volume is only usable with one pod at a time. RWO is what most of the OpenShift storage backends support and it just happened to be default in the example template.\n\nSo far only very few options, like the basic NFS support existed, to provide a \nPersistentVolume\n to more than one container at once. The reason is that most supported storage backends are actually \nblock-based\n. That is a block device is made available to one of the container hosts and is then formatted with an XFS filesystem, which is inherently not cluster-aware (cannot be safely written to from multiple Operating Systems / Containers).\n\nGlusterFS on the other hand is a true scale-out cluster filesystem with distributed locking. Hence we can use the access mode \nReadWriteMany\n on OpenShift.\n\n\nWith CNS this capability is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP-based file uploader that has multiple front-end instances sharing a common storage repository.\n\n\n Log back in as \ndeveloper\n to our project \nmy-test-project\n\n\noc login -u developer -n my-test-project\n\n\n\n\n\n Next deploy the example application:\n\n\noc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader\n\n\n\n\n\n\n\nNote\n\n\nThis is yet another way to build and launch an application from source code in OpenShift. The content before the ~ is the name of a Source-to-Image builder (a container that knows how to build applications of a certain type from source, in this case PHP) and the URL following is a GitHub repository hosting the source code.\n\n\n\n\nOutput:\n\n\n--\n Found image a1ebebb (6 weeks old) in image stream \nopenshift/php\n under tag \n7.0\n for \nopenshift/php:7.0\n\n\n    Apache 2.4 with PHP 7.0\n    -----------------------\n    Platform for building and running PHP 7.0 applications\n\n    Tags: builder, php, php70, rh-php70\n\n    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created\n      * The resulting image will be pushed to image stream \nfile-uploader:latest\n\n      * Use \nstart-build\n to trigger a new build\n    * This image will be deployed in deployment config \nfile-uploader\n\n    * Port 8080/tcp will be load balanced by service \nfile-uploader\n\n      * Other containers can access this service through the hostname \nfile-uploader\n\n\n--\n Creating resources ...\n    imagestream \nfile-uploader\n created\n    buildconfig \nfile-uploader\n created\n    deploymentconfig \nfile-uploader\n created\n    service \nfile-uploader\n created\n--\n Success\n    Build scheduled, use \noc logs -f bc/file-uploader\n to track its progress.\n    Run \noc status\n to view your app.\n\n\n\n\n\n Observe the application to be deployed with the suggested command:\n\n\noc logs -f bc/file-uploader\n\n\n\n\n\nThe follow-mode of the above command ends automatically when the build is successful and you return to your shell.\n\n\n[ ...output omitted...]\n\nCloning \nhttps://github.com/christianh814/openshift-php-upload-demo\n ...\n        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)\n        Author: Christian Hernandez \nchristianh814@users.noreply.github.com\n\n        Date:   Thu Mar 23 09:59:38 2017 -0700\n---\n Installing application source...\nPushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...\nPushed 0/5 layers, 2% complete\nPushed 1/5 layers, 20% complete\nPushed 2/5 layers, 40% complete\nPush successful\n\n\n\n\n\n When the build is completed ensure the pods are running:\n\n\noc get pods\n\n\n\n\n\nAmong your existing pods you should see new pods running.\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          2m\nfile-uploader-1-g7b0h            1/1       Running     0          1m\n...\n\n\n\n\n\nAs part of the deployment a \nService\n has been created for our app automatically. It load balances traffic to our PHP pods internally but not externally. For that a \nRoute\n needs to expose it to the network outside of OpenShift.\n\n\n Let\u2019s fix this:\n\n\noc expose svc/file-uploader\n\n\n\n\n\n Check the route that has been created:\n\n\noc get route/file-uploader\n\n\n\n\n\nThe route forwards all traffic on port 80 of it\ns automatically generated subdomain of the OpenShift router to port 8080 of the container running the app.\n\n\nNAME            HOST/PORT                                                      PATH      SERVICES        PORT       TERMINATION   WILDCARD\nfile-uploader   file-uploader-my-test-project.cloudapps.34.252.58.209.nip.io             file-uploader   8080-tcp                 None\n\n\n\n\n\nPoint your browser to the URL advertised by the route, that is \nhttp://file-uploader-my-test-project.cloudapps.\nYOUR-IP-HERE>\n.nip.io\n\n\nAlternatively, in the OpenShift UI, while logged on as \ndevleoper\n to the project called \nmy-test-project\n, click the \nDown Arrow\n in the \nOverview\n section next to the deployment called \nfile-uploader\n. The URL to your app will be in the section called \nROUTES\n.\n\n\n\n\nThe application again is very simply: it lists all file previously uploaded files and offers the ability to upload new ones, as well as download the existing uploads. Right now there is nothing.\n\n\nTry it out in your browser: select an arbitrary from your local system and upload it to the app.\n\n\n\n\nAfter uploading a file validate it has been stored successfully by following the link \nList Uploaded Files\n in the browser.\n\n\nLet\ns see how this is stored locally in the container.\n\n\n List the running pods of our application:\n\n\noc get pods -l app=file-uploader\n\n\n\n\n\nYou will see two entries:\n\n\nfile-uploader-1-build            0/1       Completed   0          7m\nfile-uploader-1-g7b0h            1/1       Running     0          6m\n\n\n\n\n\nThe name of the single pod currently running the app is this example is \nfile-uploader-1-g7b0h\n.\n\nThe container called \nfile-uploader-1-build\n is the builder container that deployed the application and it has already terminated.\n\n\n\n\nNote\n\n\nThe exact name of the pod will be different in your environment.\n\n\n\n\n Use the following shell command to store the exact name of the \nfile-uploader\n application pod in your environment in a shell variable called \nUPLOADER_POD\n:\n\n\nUPLOADER_POD=$(oc get pods -l app=file-uploader -o jsonpath=\n{.items[0].metadata.name}\n)\necho $UPLOADER_POD\n\n\n\n\n\n Use the remote shell capability of the \noc\n client to list the content of \nuploaded/\n directory inside the pod after you uploaded a file in the PHP app:\n\n\noc rsh $UPLOADER_POD ls -ahl /opt/app-root/src/uploaded\n\n\n\n\n\nIn the below example output we\nve uploaded a file named \ncns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\n in the app via the browser, and we see it store from within the pod:\n\n\ntotal 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\n\n\n\n\n\nThe app should also list the file in the overview:\n\n\n\n\nHowever, in it\ns default configuration his pod currently does not use any persistent storage. It uses it\ns local filesystem - that is stores the file in inside the container image\ns root filesystem.\n\n\n\n\nImportant\n\n\nNever store important data inside a pods root filesystem or \nemptyDir\n. It\u2019s ephemeral by definition and will be lost as soon as the pod terminates.\n\nWorse, the container\ns root filesystem is even slower than \nemptyDir\n as it needs to traverse the \noverlay2\n stack, that Red Hat Enterprise Linux uses by default as of version 7.4 for running container images.\n\nAlso, inherently pods using this kind of storage cannot be scaled out trivially.\n\n\n\n\nLet\u2019s see when this become a problem.\n\n\n Let\u2019s scale the deployment to 3 instances of the app:\n\n\noc scale dc/file-uploader --replicas=3\n\n\n\n\n\n Watch the additional pods getting spawned:\n\n\noc get pods -l app=file-uploader\n\n\n\n\n\nYou will see 2 additional pods being spawned:\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-3cgh1            1/1       Running     0          20s\nfile-uploader-1-3hckj            1/1       Running     0          20s\nfile-uploader-1-g7b0h            1/1       Running     0          3m\n\n\n\n\n\n\n\nNote\n\n\nThe pod names will be different in your environment since they are automatically generated. It takes a couple of seconds until they are ready.\n\n\n\n\nAlternatively, in the UI, wait for the \nfile-uploader\n application reach 3 healthy pods (the blue circle is completely filled):\n\n\n\n\nOn the command line this will look like this:\n\n\noc get pods -l app=file-uploader\n\n\n\n\n\nNAME                    READY     STATUS    RESTARTS   AGE\nfile-uploader-1-98fwm   1/1       Running   0          2m\nfile-uploader-1-g7b0h   1/1       Running   0          8m\nfile-uploader-1-rwt2p   1/1       Running   0          2m\n\n\n\n\n\nThese 3 pods now make up our application. OpenShift will load balance incoming traffic between them.\n\nHowever, when you log on to one of the new instances you will see they have no data.\n\n\n Store the names of all in some environment variables for easy copy\npaste:\n\n\nUPLOADER_POD_1=$(oc get pods -l app=file-uploader -o jsonpath=\n{.items[0].metadata.name}\n)\nUPLOADER_POD_2=$(oc get pods -l app=file-uploader -o jsonpath=\n{.items[1].metadata.name}\n)\nUPLOADER_POD_3=$(oc get pods -l app=file-uploader -o jsonpath=\n{.items[2].metadata.name}\n)\n\n\n\n\n\n Lets check all upload directories of all pods:\n\n\noc rsh $UPLOADER_POD_1 ls -ahl /opt/app-root/src/uploaded\noc rsh $UPLOADER_POD_2 ls -ahl /opt/app-root/src/uploaded\noc rsh $UPLOADER_POD_3 ls -ahl /opt/app-root/src/uploaded\n\n\n\n\n\nOh oh, only one of the pods has the previously uploaded file. Looks like our application data is not consistent anymore:\n\n\noc rsh $UPLOADER_POD_1 ls -ahl /opt/app-root/src/uploaded\ntotal 0\ndrwxrwxr-x. 2 default root  22 Sep 24 11:31 .\ndrwxrwxr-x. 1 default root 124 Sep 24 11:31 ..\n-rw-rw-r--. 1 default root   0 Sep 24 11:31 .gitkeep\n\noc rsh $UPLOADER_POD_2 ls -ahl /opt/app-root/src/uploaded\ntotal 108K\ndrwxrwxr-x. 1 default    root   52 Sep 24 11:35 .\ndrwxrwxr-x. 1 default    root   22 Sep 24 11:31 ..\n-rw-rw-r--. 1 default    root    0 Sep 24 11:31 .gitkeep\n-rw-r--r--. 1 1000080000 root  16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\n\noc rsh $UPLOADER_POD_3 ls -ahl /opt/app-root/src/uploaded\ntotal 0\ndrwxrwxr-x. 2 default root  22 Sep 24 11:31 .\ndrwxrwxr-x. 1 default root 124 Sep 24 11:31 ..\n-rw-rw-r--. 1 default root   0 Sep 24 11:31 .gitkeep\n\n\n\n\n\nIt\ns empty because the previously uploaded files were stored locally in the first container and are not available to the others.\n\n\nSimilarly, other users of the app will sometimes see your uploaded files and sometimes not. With the deployment scaled to 3 instances OpenShifts router will simply round-robin across them. You can simulate this with another instance of your browser in \nIncognito mode\n pointing to your app.\n\n\nThe app is of course not usable like this. We can fix this by providing shared storage to this app.\n\n\n First create a \nPVC\n with the appropriate setting in a file called \ncns-rwx-pvc.yml\n with below contents:\n\n\ncns-rwx-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-shared-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteMany\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n5Gi\n\n  \nstorageClassName\n:\n \nglusterfs-storage\n\n\n\n\n\n\nNotice the access mode explicitly requested to be \nReadWriteMany\n (also referred to as \nRWX\n). Storage provisioned like this can be mounted by multiple containers on multiple hosts at the same time.\n\n\n Submit the request to the system:\n\n\noc create -f cns-rwx-pvc.yml\n\n\n\n\n\n Let\u2019s look at the result:\n\n\noc get pvc\n\n\n\n\n\nACCESSMODES\n is set to \nRWX\n:\n\n\nNAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s\n...\n\n\n\n\n\nWe can now update the \nDeploymentConfig\n of our application to use this \nPVC\n to provide the application with persistent, shared storage for uploads.\n\n\n Update the configuration of the application by adding a volume claim like this:\n\n\noc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded\n\n\n\n\n\nOur app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the \nPVC\n under \n/opt/app-root/src/upload\n (the path is predictable so we can hard-code it here).\n\n\n You can watch it like this:\n\n\noc logs dc/file-uploader -f\n\n\n\n\n\nThe new \nDeploymentConfig\n will supersede the old one.\n\n\n--\n Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don\nt exceed 4 pods)\n    Scaling file-uploader-2 up to 1\n    Scaling file-uploader-1 down to 2\n    Scaling file-uploader-2 up to 2\n    Scaling file-uploader-1 down to 1\n    Scaling file-uploader-2 up to 3\n    Scaling file-uploader-1 down to 0\n--\n Success\n\n\n\n\n\nExit out of the follow mode with: \nCtrl\n + \nc\n\n\n\n\nWarning\n\n\nChanging the storage settings of a pod can be destructive. Any existing data will \nnot be preserved\n. You are responsible to care for data migration.\n\nOne strategy here could have been to use \noc rsync\n saving the data to a local directory on the machine running the \noc\n client.\n\n\n\n\nYou can also observe the rolling upgrade of the file uploader application in the OpenShift UI:\n\n\n\n\nThe new \nDeploymentConfig\n named \nfile-uploader-2\n will have 3 pods all sharing the same storage.\n\n\n Get the names of the new pods:\n\n\noc get pods -l app=file-uploader\n\n\n\n\n\nOutput:\n\n\nNAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          18m\nfile-uploader-2-jd22b            1/1       Running     0          1m\nfile-uploader-2-kw9lq            1/1       Running     0          2m\nfile-uploader-2-xbz24            1/1       Running     0          1m\n\n\n\n\n\nTry it out in your application: upload new files and watch them being visible from within all application pods. In new browser \nIncognito\n sessions, simulating other users, the application behaves normally as it circles through the pods between browser requests.\n\n\nThat\u2019s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.\n\n\nWith CNS this is available wherever OpenShift is deployed with no external dependency.", 
            "title": "Module 3 - Persistent Storage for Apps"
        }, 
        {
            "location": "/module-3-cns-for-apps/#openshift-storage-101", 
            "text": "OpenShift uses Kubernetes  PersistentStorage facility to dynamically allocate storage of any kind for applications. This is a fairly simple framework in which only 3 components are relevant: the storage provider, the storage volume and the request for a storage volume.   OpenShift knows non-ephemeral storage as  persistent  volumes. This is storage that is decoupled from pod lifecycles. Users can request such storage by submitting a  PersistentVolumeClaim  to the system, which carries aspects like desired capacity or access mode (shared, single, read-only).  A storage provider in the system is represented by a  StorageClass  and is referenced in the claim. Upon receiving the claim OpenShift talks to the API of the actual storage system to provision the storage.    The provisioned storage is represented in OpenShift as a  PersistentVolume  which can directly be used by pods to mount it.  With these basics defined we can try CNS in our system. First examine the  StorageClass  the installer has automatically created for us.   Remain logged in as  operator  for now:  oc login -u operator   Examine the  StorageClass  objects available:  oc get storageclass  openshift-ansible  defined a  StorageClass  for CNS:  NAME                TYPE\nglusterfs-storage   kubernetes.io/glusterfs   Let s look at the details:  oc describe storageclass/glusterfs-storage  The output indicates the backing storage type: GlusterFS  Name:       glusterfs-storage\nIsDefaultClass: No\nAnnotations:     none \nProvisioner:    kubernetes.io/glusterfs\nParameters: resturl=http://heketi-storage-app-storage.cloudapps.52.28.134.154.nip.io,restuser=admin,secretName=heketi-storage-admin-secret,secretNamespace=app-storage  !!! Note: \n    The exact value for  resturl  will again be different for you because it s based on the  route /IP address on your system.  The  Provisioner  is a module in OpenShift/Kubernetes that can talk to the CNS API service:  heketi . The parameters supplied in the  StorageClass  tell the  Provisioner  the URL of the API as well as the  admin  users (defined in  restuser ) password in the form of an OpenShift  secret  ( base64 d hash of the password). \nThe  Provisioner  is not an entity directly accessible to users.", 
            "title": "OpenShift Storage 101"
        }, 
        {
            "location": "/module-3-cns-for-apps/#requesting-storage", 
            "text": "To get storage provisioned via this  StorageClass  as a user you have to  claim  storage. The object  PersistentVolumeClaim  (PVC) basically acts a request to the system to provision storage with certain properties, like a specific capacity. \nAlso the access mode is set here, where  ReadWriteMany  allows one or more container in parallel to mount and access this storage. This capability is dependent on the storage backend. In our case, with GlusterFS, we have one of the few systems that can reliable implement shared storage.   Create a claim by specifying a file called  cns-pvc.yml  with the following contents:  cns-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-container-storage  spec : \n   accessModes : \n   -   ReadWriteMany \n   resources : \n     requests : \n       storage :   10Gi \n   storageClassName :   glusterfs-storage   With above PVC we are requesting 10 GiB of shared storage. Instead of  ReadWriteMany  you could also have specified  ReadWriteOnly  (for read-only) and  ReadWriteOnce  (for non-shared storage, where only one pod can mount at a time).   Submit the PVC to the system like so:  oc create -f cns-pvc.yml   After a couple of seconds, look at the requests state with the following command:  oc get pvc  You should see the PVC listed and in  Bound  state.  NAME                   STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS        AGE\nmy-container-storage   Bound     pvc-848cbc48-9fe3-11e7-83c3-022238c6a515   10Gi       RWX           glusterfs-storage   6s   Caution  If the PVC is stuck in  PENDING  state you will need to investigate. Run  oc describe pvc/my-container-storage  to see a more detailed explanation. Typically there are two root causes - the StorageClass is not properly specified in the  PVC  (wrong name, not specified) or (less likely here) the backing storage system has a problem (in our case: error on heketi side, incorrect URL in  StorageClass , etc.)    Tip  Alternatively, you can also do this step with the UI. Log on as  operator  and select any Project. Then go to the  Storage  tab. Select  Create  storage and make selections accordingly to the PVC described before.    When the claim was fulfilled successfully it is in the  Bound  state. That means the system has successfully (via the  StorageClass ) reached out to the storage backend (in our case GlusterFS). The backend in turn provisioned the storage and provided a handle back OpenShift. In OpenShift the provisioned storage is then represented by a  PersistentVolume  (PV) which is  bound  to the PVC.   Look at the PVC for these details:  oc describe pvc/my-container-storage  The details of the PVC show all the desired properties of the requested storage and against which  StorageClass  it has been submitted. Since it s already bound thanks to dynamic provisioning it also displays the name of the  PersistentVolume  which was generated to fulfil the claim. \nThe name of the  PV  always follows the pattern  pvc-... .  Name:           my-container-storage Namespace:      app-storage\nStorageClass:   glusterfs-storage\nStatus:         Bound Volume:         pvc-848cbc48-9fe3-11e7-83c3-022238c6a515 Labels:          none \nAnnotations:    pv.kubernetes.io/bind-completed=yes\n                pv.kubernetes.io/bound-by-controller=yes\n                volume.beta.kubernetes.io/storage-provisioner=kubernetes.io/glusterfs\nCapacity:       10Gi\nAccess Modes:   RWX\nEvents:\n  FirstSeen LastSeen    Count   From                SubObjectPath   Type        Reason          Message\n  --------- --------    -----   ----                -------------   --------    ------          -------\n  10m       10m     1   persistentvolume-controller         Normal      ProvisioningSucceeded   Successfully provisioned volume pvc-848cbc48-9fe3-11e7-83c3-022238c6a515 using kubernetes.io/glusterfs   Note  The  PV  name will be different in your environment since it\u2019s automatically generated.   In order to look at a the details of a  PV  in a default setup like this you need more privileges.   Look at the corresponding  PV  by it\u2019s name. Use the following command which stores the exact name in an environment variable extracted by an  oc  command for copy paste-friendliness:  PV_NAME=$(oc get pvc/my-container-storage -o jsonpath= {.spec.volumeName} )\noc describe pv/${PV_NAME}  The output shows several interesting things, like the access mode (RWX = ReadWriteMany), the reclaim policy (what happens when the PV object gets deleted), the capacity and the type of storage backing this PV (in our case GlusterFS as part of CNS):  Name:           pvc-848cbc48-9fe3-11e7-83c3-022238c6a515\nLabels:          none \nAnnotations:    pv.beta.kubernetes.io/gid=2001\n                pv.kubernetes.io/bound-by-controller=yes\n                pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs\n                volume.beta.kubernetes.io/mount-options=auto_unmount StorageClass:   glusterfs-storage Status:         Bound Claim:          app-storage/my-container-storage Reclaim Policy: Delete Access Modes:   RWX Capacity:       10Gi Message:\nSource:   Type:         Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)   EndpointsName:glusterfs-dynamic-my-container-storage\n  Path:         vol_7e1733b13e1b46c028a71590f8cfe8b5\n  ReadOnly:     false\nEvents:          none   Note how all the properties exactly match up with what the  PVC  requested.   Why is it called  Bound ?  Originally  PVs  weren t automatically created. Hence in earlier documentation you may also find references about administrators actually  pre-provisioning   PVs . Later  PVCs  would  pick up /match a suitable  PV  by looking at it\u2019s capacity and access mode. When successful they are  bound  to this  PV . \nThis was needed for storage like NFS that does not have an API and therefore does not support  dynamic provisioning . That s called  static provisioning . \nThis kind of storage should not be used anymore as it requires manual intervention, risky capacity planning and incurs inefficient storage utilization.   Although the storage is provisioned on the GlusterFS side it s not yet used by any application/pod/host. So let\u2019s release this storage capacity again. \nStorage is freed up by deleting the  PVC . The  PVC  controls the lifecycle of the storage, not the  PV .   Important  Never delete  PVs  that are dynamically provided. They are only handles for pods mounting the storage. With dynamic provisioning storage lifecycle is entirely controlled via  PVCs .    Delete the storage by deleting the  PVC  like this:  oc delete pvc/my-container-storage", 
            "title": "Requesting Storage"
        }, 
        {
            "location": "/module-3-cns-for-apps/#make-cns-the-default-storage", 
            "text": "For the following example it is required to make the  StorageClass  that got created for CNS the system-wide default. This simplifies the following steps.   Use the  oc patch  command to change the definition of the  StorageClass  on the fly:  oc patch storageclass glusterfs-storage \\\n-p  { metadata : { annotations : { storageclass.kubernetes.io/is-default-class :  true }}}    Look at the  StorageClass  again to see the change reflected:  oc describe storageclass/glusterfs-storage  Verify it s indeed the default (see highlighted line):  Name:           glusterfs-storage IsDefaultClass: Yes Annotations:     none \nProvisioner:    kubernetes.io/glusterfs\nParameters:     resturl=http://heketi-storage-app-storage.cloudapps.52.28.134.154.nip.io,restuser=admin,secretName=heketi-storage-admin-secret,secretNamespace=app-storage   Important  It is crucial that you  do not skip this step  as it is fundamental for the next example to work.", 
            "title": "Make CNS the default storage"
        }, 
        {
            "location": "/module-3-cns-for-apps/#using-non-shared-storage-for-databases", 
            "text": "Normally a user doesn\u2019t request storage with a  PVC  directly. Rather the  PVC  is part of a larger template that describes the entire application stack. Such examples ship with OpenShift out of the box.   Alternative  The steps described in this section to launch the Rails/Postgres example app can again also be done with the UI. For this purpose follow these steps similar to the one in Module 1:     Log on to the OpenShift UI as the  developer  user    Create a new project called  my-test-project , label and description is optional    In the Overview, next to the project\u2019s name select  Add to project    In the  Browse Catalog  view select  Ruby  from the list of programming languages    Select the example app entitled  Rails + PostgreSQL (Persistent)    (optional) Change the  Volume Capacity  parameter to 5GiB    Select  Create  to start deploying the app    Select  Continue to Overview  in the confirmation screen    Wait for the application deployment to finish and continue below at       To create an application from the OpenShift Example templates on the CLI follow these steps.   Log in as  developer  and the password  r3dh4t  oc login -u developer   Create a new project with a name of your choice:  oc new-project my-test-project  To use the example applications that ship with OpenShift we can use the  new-app  command of the  oc  client. It will allow us to specify one of the  application stack templates in the system. There are a lot of example templates that ship in the pre-defined namespace called  openshift  which is the default place where  oc new-app  will look.  Let s pick a database application that definitely needs persistent storage. It s going to be part of a simple example blog application based on Rails and PostgreSQL.   Instantiate this application with the following command  oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi  Among various OpenShift resources also our  PVC  will be created:  [...output omitted...]\nsecret  rails-pgsql-persistent  created\nservice  rails-pgsql-persistent  created\nroute  rails-pgsql-persistent  created\nimagestream  rails-pgsql-persistent  created\nbuildconfig  rails-pgsql-persistent  created deploymentconfig  rails-pgsql-persistent  created persistentvolumeclaim  postgresql  created\nservice  postgresql  created\ndeploymentconfig  postgresql  created  The deployment process for the application stack continues in the background.  We have given the  new-app  command an additional switch:  -p VOLUME_CAPACITY=5Gi . This causes a parameter in the template called  VOLUME_CAPACITY  to be set to 5GiB. Parameters make templates more generic. In our case the template contains a  PersistentVolumeClaim  (like highlighted above) which will take it s size from this parameter.   What other parameters does this template have?  Plenty. If you are interested about all the variables/parameters this particular template supports, you can run  oc process openshift//rails-pgsql-persistent --parameters .    What else does the template file contain?  The template describes all OpenShift resources necessary to stand up the rails pod and the postgres pod and make them accessible via services and routes. If you are curious:  oc get template/rails-pgsql-persistent -n openshift -o yaml \nIn essence it creates Ruby on Rails instance in a pod which functionality mimics a very basic blogging application. The blog articles are saved in a PostgreSQL database that runs in a separate pod. \nAbove mentioned  PVC  can be found there as well (around line 194) which supplies the postgres pod with persistent storage below the mount point  /var/lib/pgsql/data  (around line 275).   You can now either use the OpenShift UI (while being logged as  developer  in the project  my-test-project ) or the CLI to follow the deployment process.  In the UI you will observe both pods deploying like this:     On the CLI watch the containers deploy like this:  oc get pods -w  The complete output should look like this:  NAME                             READY     STATUS              RESTARTS   AGE\npostgresql-1-deploy              0/1       ContainerCreating   0          11s\nrails-pgsql-persistent-1-build   0/1       ContainerCreating   0          11s\nNAME                  READY     STATUS    RESTARTS   AGE\npostgresql-1-deploy   1/1       Running   0          14s\npostgresql-1-81gnm   0/1       Pending   0         0s\npostgresql-1-81gnm   0/1       Pending   0         0s\nrails-pgsql-persistent-1-build   1/1       Running   0         19s\npostgresql-1-81gnm   0/1       Pending   0         15s\npostgresql-1-81gnm   0/1       ContainerCreating   0         16s\npostgresql-1-81gnm   0/1       Running   0         47s\npostgresql-1-81gnm   1/1       Running   0         4m\npostgresql-1-deploy   0/1       Completed   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\npostgresql-1-deploy   0/1       Terminating   0         4m\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       Pending   0         0s\nrails-pgsql-persistent-1-deploy   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-build   0/1       Completed   0         11m\nrails-pgsql-persistent-1-deploy   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       Pending   0         0s\nrails-pgsql-persistent-1-hook-pre   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-hook-pre   1/1       Running   0         6s\nrails-pgsql-persistent-1-hook-pre   0/1       Completed   0         15s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Pending   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       ContainerCreating   0         0s\nrails-pgsql-persistent-1-dkj7w   0/1       Running   0         1m\nrails-pgsql-persistent-1-dkj7w   1/1       Running   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Completed   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-deploy   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m\nrails-pgsql-persistent-1-hook-pre   0/1       Terminating   0         1m  Exit out of the watch mode with:  Ctrl  +  c  !!! Note: \n    It may take up to 5-7 minutes for the deployment to complete.  If you did it via the UI the deployment is finished when both, rails app and postgres database are up and running:\n\n[![OpenShift Rails Example Deployment](img/openshift_rails_app_create_3.png)](img/openshift_rails_app_create_3.png)  You should also see a PVC being issued and in the  Bound  state.   Look at the PVC created:  oc get pvc/postgresql  Output:  NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   15Gi       RWO           4m  Now go ahead and try out the application. The overview page in the OpenShift UI will tell you the  route  which has been deployed as well (the http://  link in the upper right hand corner). Use it and append  /articles  to the URL to get to the actual app.   Otherwise get it on the CLI like this:  oc get route  Output:  NAME                     HOST/PORT                                                               PATH      SERVICES                 PORT      TERMINATION   WILDCARD\nrails-pgsql-persistent   rails-pgsql-persistent-my-test-project.cloudapps.34.252.58.209.nip.io             rails-pgsql-persistent    all                    None  !!! Note: \n    Again, the URL will be slightly different for you.  Following this output, point your browser to the URL (prepend it with  http://  and append  /articles ) to reach the actual application, in this case:  http:// rails-pgsql-persistent-my-test-project.cloudapps. YOUR-IP-HERE> .nip.io / articles  You should be able to successfully create articles and comments. The username/password to create articles and comments is by default  openshift / secret . \nWhen they are saved they are actually saved in the PostgreSQL database which stores it\u2019s table spaces on a GlusterFS volume provided by CNS.   You can verify that the postgres pod indeed mounted the PVC under the pather where PostgreSQL normally stores it s data with this command:  oc volumes dc --all  You will see that the  DeploymentConfig  of the postgres pod did indeed include a  PVC :  deploymentconfigs/postgresql\n  pvc/postgresql (allocated 5GiB) as postgresql-data\n    mounted at /var/lib/pgsql/data\ndeploymentconfigs/rails-pgsql-persistent  Now let\u2019s take a look at how this was actually achieved.   A normal user cannot see the details of a  PersistentVolume . Log back in as  operator :  oc login -u operator -n my-test-project   Look at the PVC to determine the PV:  oc get pvc  Output:  NAME         STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\npostgresql   Bound     pvc-6c348fbb-4e9d-11e7-970e-0a9938370404   5Gi       RWO           10m   Note  Your volume (PV) name will be different as it\u2019s dynamically generated.   The PVC name is found in above output the  VOLUME  column.   Look at the details of this PV with the following copy paste-friendly short-hand:  PV_NAME=$(oc get pvc/postgresql -o jsonpath= {.spec.volumeName} )\noc describe pv/${PV_NAME}  Output shows (in highlight) the name of the volume, the backend type (GlusterFS) and the volume name GlusterFS uses internally.  Name:             pvc-c638ba71-a070-11e7-890c-02ed99595f95 Labels:            none \nAnnotations:      pv.beta.kubernetes.io/gid=2000\n                  pv.kubernetes.io/bound-by-controller=yes\n                  pv.kubernetes.io/provisioned-by=kubernetes.io/glusterfs\n                  volume.beta.kubernetes.io/mount-options=auto_unmount\nStorageClass:     glusterfs-storage\nStatus:           Bound\nClaim:            my-test-project/postgresql\nReclaim Policy:   Delete\nAccess Modes:     RWO\nCapacity:         5Gi\nMessage:\nSource:   Type:           Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)   EndpointsName:  glusterfs-dynamic-postgresql   Path:           vol_4b22dda4c9681f4325ba5e24cb4f64c6   ReadOnly:       false\nEvents:            none   Note the GlusterFS volume name, in this case  vol_4b22dda4c9681f4325ba5e24cb4f64c6 .   Save the generated volume name of GlusterFS in a shell variable for later use:  GLUSTER_VOL_NAME=$(oc get pv/${PV_NAME} -o jsonpath= {.spec.glusterfs.path} )   Now let\u2019s switch to the namespace we used for CNS deployment:  oc project app-storage   Look at the GlusterFS pods (filtered by label) running:  oc get pods -o wide -l glusterfs=storage-pod  Pick the first of the GlusterFS pods in the list:  NAME                      READY     STATUS    RESTARTS   AGE       IP           NODE glusterfs-storage-16pb0   1/1       Running   0          23m       10.0.2.201   node-1.lab glusterfs-storage-37tqx   1/1       Running   0          23m       10.0.4.203   node-3.lab\nglusterfs-storage-68lxn   1/1       Running   0          23m       10.0.3.202   node-2.lab  Pick the first the pod in the list, in this example  glusterfs-storage-16pb0  and note it s host IP address.   Use the following command to conveniently save it s name and host IP address in a shell variable for later use (copy   paste those lines in your shell):  FIRST_GLUSTER_POD=$(oc get pods -l glusterfs=storage-pod -o jsonpath= {.items[0].metadata.name} )\nHOST_IP=$(oc get pod/$FIRST_GLUSTER_POD -o jsonpath= {.status.hostIP} )\necho $FIRST_GLUSTER_POD\necho $HOST_IP  Next we are going to use the remote session capability of the  oc  client to execute a command in that pods namespace. We are going to leverage the GlusterFS CLI utilities being present in that pod.   Ask GlusterFS from inside the CNS pod about all the GlusterFS volumes defined:  oc rsh $FIRST_GLUSTER_POD gluster vol list  You will see two volumes:  heketidbstorage\nvol_4b22dda4c9681f4325ba5e24cb4f64c6    heketidbstorage  is an internal-only volume dedicated to heketi\u2019s internal database.    the second is the volume backing the PV of the PostgreSQL database deployed earlier, in this example  vol_4b22dda4c9681f4325ba5e24cb4f64c6  - your s will be differently named.     Ask GlusterFS about the topology of this volume:  oc rsh $FIRST_GLUSTER_POD gluster vol info $GLUSTER_VOL_NAME  The output of the  gluster  command will show you how the volume has been created. You will also see that the pod you are currently logged on to serves one the bricks.  Volume Name: vol_4b22dda4c9681f4325ba5e24cb4f64c6\nType: Replicate\nVolume ID: 37d53d51-34bc-4853-b564-3b0ea9bdd935\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks: Brick1: 10.0.2.201:/var/lib/heketi/mounts/vg_50f5d808e04ccab8d6fd0231c268db35/brick_4b59cd1f4a8ff8d8a3eddf7317829e73/brick Brick2: 10.0.4.203:/var/lib/heketi/mounts/vg_7cb3be478376539d0c4b54cf69688c8e/brick_688627cc5dca8d01a81fa504487116c0/brick Brick3: 10.0.3.202:/var/lib/heketi/mounts/vg_fb1a45c7853f415a3a09a164f0d717fb/brick_931730cb987383a605c1d1ff5d796fa9/brick Options Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on  The above output tells us GlusterFS created this volume as a 3-way replica set across 3 bricks. Bricks are local directories on GlusterFS nodes. They make up replication targets. \nIn our case the GlusterFS nodes are our CNS pods and since they share the physical hosts network they are displayed with these IP addresses (see highlighted lines) . This volume type  Replicate  is currently the only supported volume type in production. It synchronously replicates all data across those 3 bricks.  Let s take a look at what s inside a brick.   Paste this little piece of bash magic into your shell to conveniently store the brick directory from the first CNS pod you saw earlier in an environment variable:  BRICK_DIR=$(echo -n $(oc rsh $FIRST_GLUSTER_POD gluster vol info $GLUSTER_VOL_NAME | grep $HOST_IP) | cut -d  :  -f 3 | tr -d $ \\r  )\necho $BRICK_DIR   Now let s look at a brick directory from inside a CNS pod:  oc rsh $FIRST_GLUSTER_POD ls -ahl $BRICK_DIR  What you see is the content of the brick directory from within the GlusterFS pod, which makes up 1 out of 3 copies of our postgres volume:  total 16K\ndrwxrwsr-x.   5 root       2001   57 Jun  6 14:44 .\ndrwxr-xr-x.   3 root       root   19 Jun  6 14:44 ..\ndrw---S---. 263 root       2001 8.0K Jun  6 14:46 .glusterfs\ndrwxr-sr-x.   3 root       2001   25 Jun  6 14:44 .trashcan\ndrwx------.  20 1000080000 2001 8.0K Jun  6 14:46 userdata   Going one level deeper, we see a data structure familiar to PostgreSQL users:  oc rsh $FIRST_GLUSTER_POD ls -ahl $BRICK_DIR/userdata  This is one of 3 copies of the postgres data directory hosted by CNS:  total 68K\ndrwx------. 20 1000080000 2001 8.0K Jun  6 14:46 .\ndrwxrwsr-x.  5 root       2001   57 Jun  6 14:44 ..\n-rw-------.  2 1000080000 root    4 Jun  6 14:44 PG_VERSION\ndrwx------.  6 1000080000 root   54 Jun  6 14:46 base\ndrwx------.  2 1000080000 root 8.0K Jun  6 14:47 global\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_clog\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_commit_ts\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_dynshmem\n-rw-------.  2 1000080000 root 4.6K Jun  6 14:46 pg_hba.conf\n-rw-------.  2 1000080000 root 1.6K Jun  6 14:44 pg_ident.conf\ndrwx------.  2 1000080000 root   32 Jun  6 14:46 pg_log\ndrwx------.  4 1000080000 root   39 Jun  6 14:44 pg_logical\ndrwx------.  4 1000080000 root   36 Jun  6 14:44 pg_multixact\ndrwx------.  2 1000080000 root   18 Jun  6 14:46 pg_notify\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_replslot\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_serial\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_snapshots\ndrwx------.  2 1000080000 root    6 Jun  6 14:46 pg_stat\ndrwx------.  2 1000080000 root   84 Jun  6 15:16 pg_stat_tmp\ndrwx------.  2 1000080000 root   18 Jun  6 14:44 pg_subtrans\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_tblspc\ndrwx------.  2 1000080000 root    6 Jun  6 14:44 pg_twophase\ndrwx------.  3 1000080000 root   60 Jun  6 14:44 pg_xlog\n-rw-------.  2 1000080000 root   88 Jun  6 14:44 postgresql.auto.conf\n-rw-------.  2 1000080000 root  21K Jun  6 14:46 postgresql.conf\n-rw-------.  2 1000080000 root   46 Jun  6 14:46 postmaster.opts\n-rw-------.  2 1000080000 root   89 Jun  6 14:46 postmaster.pid  You are looking at the PostgreSQL internal data file structure from the perspective of the GlusterFS server side. Evidence that the database uses CNS.  Clients, like the OpenShift nodes and their application pods talk to this storage with the GlusterFS protocol as it were an ordinary local mounts. \nWhen a pod starts that mounts storage from a  PV  backed by CNS the GlusterFS mount plugin in OpenShift will mount the GlusterFS volume on the right OpenShift node and then  bind-mount  this directory to the right pod s file namespace. \nThis happens transparently to the application and looks like a normal local filesystem inside the pod as you just saw. Let s have a look from the container host perspective:   Get the name and the host IP of the postgres pod with this shell shortcut into environment variables for easy copy paste later:  POSTGRES_POD=$(oc get pods -l name=postgresql -n my-test-project -o jsonpath= {.items[0].metadata.name} )\nPOSTGRES_CONTAINER_HOST=$(oc get pod/$POSTGRES_POD -n my-test-project -o jsonpath= {.status.hostIP} )\necho $POSTGRES_POD\necho $POSTGRES_CONTAINER_HOST  Since you are acting from the master node  master.lab  you can use SSH without password to execute a remote command on the OpenShift node hosting the postgres pod.   Look for the GlusterFS mount points on the host, searching the GlusterFS volume that was provisioned for the database  ssh $POSTGRES_CONTAINER_HOST mount | grep $GLUSTER_VOL_NAME  !!! Tip: \n    Answer the SSH clients question  Are you sure you want to continue connecting (yes/no)?  with  yes .  The host should have mounted this GlusterFS volume, for example:  10.0.2.201:vol_4b22dda4c9681f4325ba5e24cb4f64c6 on /var/lib/origin/openshift.local.volumes/pods/c7029a5a-a070-11e7-890c-02ed99595f95/volumes/kubernetes.io~glusterfs/pvc-c638ba71-a070-11e7-890c-02ed99595f95 type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072)  This sums up the relationship between  PVCs ,  PVs , GlusterFS volumes and container mounts in CNS.  The mounting and unmounting of GlusterFS volumes is faciliated automatically by the GlusterFS mount plugin that ships with OpenShift.", 
            "title": "Using non-shared storage for databases"
        }, 
        {
            "location": "/module-3-cns-for-apps/#providing-shared-storage-to-multiple-application-instances", 
            "text": "In the previous example we provisioned an RWO PV - the volume is only usable with one pod at a time. RWO is what most of the OpenShift storage backends support and it just happened to be default in the example template. \nSo far only very few options, like the basic NFS support existed, to provide a  PersistentVolume  to more than one container at once. The reason is that most supported storage backends are actually  block-based . That is a block device is made available to one of the container hosts and is then formatted with an XFS filesystem, which is inherently not cluster-aware (cannot be safely written to from multiple Operating Systems / Containers). \nGlusterFS on the other hand is a true scale-out cluster filesystem with distributed locking. Hence we can use the access mode  ReadWriteMany  on OpenShift.  With CNS this capability is now available to all OpenShift deployments, no matter where they are deployed. To demonstrate this capability with an application we will deploy a PHP-based file uploader that has multiple front-end instances sharing a common storage repository.   Log back in as  developer  to our project  my-test-project  oc login -u developer -n my-test-project   Next deploy the example application:  oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader   Note  This is yet another way to build and launch an application from source code in OpenShift. The content before the ~ is the name of a Source-to-Image builder (a container that knows how to build applications of a certain type from source, in this case PHP) and the URL following is a GitHub repository hosting the source code.   Output:  --  Found image a1ebebb (6 weeks old) in image stream  openshift/php  under tag  7.0  for  openshift/php:7.0 \n\n    Apache 2.4 with PHP 7.0\n    -----------------------\n    Platform for building and running PHP 7.0 applications\n\n    Tags: builder, php, php70, rh-php70\n\n    * A source build using source code from https://github.com/christianh814/openshift-php-upload-demo will be created\n      * The resulting image will be pushed to image stream  file-uploader:latest \n      * Use  start-build  to trigger a new build\n    * This image will be deployed in deployment config  file-uploader \n    * Port 8080/tcp will be load balanced by service  file-uploader \n      * Other containers can access this service through the hostname  file-uploader \n\n--  Creating resources ...\n    imagestream  file-uploader  created\n    buildconfig  file-uploader  created\n    deploymentconfig  file-uploader  created\n    service  file-uploader  created\n--  Success\n    Build scheduled, use  oc logs -f bc/file-uploader  to track its progress.\n    Run  oc status  to view your app.   Observe the application to be deployed with the suggested command:  oc logs -f bc/file-uploader  The follow-mode of the above command ends automatically when the build is successful and you return to your shell.  [ ...output omitted...]\n\nCloning  https://github.com/christianh814/openshift-php-upload-demo  ...\n        Commit: 7508da63d78b4abc8d03eac480ae930beec5d29d (Update index.html)\n        Author: Christian Hernandez  christianh814@users.noreply.github.com \n        Date:   Thu Mar 23 09:59:38 2017 -0700\n---  Installing application source...\nPushing image 172.30.120.134:5000/my-test-project/file-uploader:latest ...\nPushed 0/5 layers, 2% complete\nPushed 1/5 layers, 20% complete\nPushed 2/5 layers, 40% complete\nPush successful   When the build is completed ensure the pods are running:  oc get pods  Among your existing pods you should see new pods running.  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          2m\nfile-uploader-1-g7b0h            1/1       Running     0          1m\n...  As part of the deployment a  Service  has been created for our app automatically. It load balances traffic to our PHP pods internally but not externally. For that a  Route  needs to expose it to the network outside of OpenShift.   Let\u2019s fix this:  oc expose svc/file-uploader   Check the route that has been created:  oc get route/file-uploader  The route forwards all traffic on port 80 of it s automatically generated subdomain of the OpenShift router to port 8080 of the container running the app.  NAME            HOST/PORT                                                      PATH      SERVICES        PORT       TERMINATION   WILDCARD\nfile-uploader   file-uploader-my-test-project.cloudapps.34.252.58.209.nip.io             file-uploader   8080-tcp                 None  Point your browser to the URL advertised by the route, that is  http://file-uploader-my-test-project.cloudapps. YOUR-IP-HERE> .nip.io  Alternatively, in the OpenShift UI, while logged on as  devleoper  to the project called  my-test-project , click the  Down Arrow  in the  Overview  section next to the deployment called  file-uploader . The URL to your app will be in the section called  ROUTES .   The application again is very simply: it lists all file previously uploaded files and offers the ability to upload new ones, as well as download the existing uploads. Right now there is nothing.  Try it out in your browser: select an arbitrary from your local system and upload it to the app.   After uploading a file validate it has been stored successfully by following the link  List Uploaded Files  in the browser.  Let s see how this is stored locally in the container.   List the running pods of our application:  oc get pods -l app=file-uploader  You will see two entries:  file-uploader-1-build            0/1       Completed   0          7m\nfile-uploader-1-g7b0h            1/1       Running     0          6m  The name of the single pod currently running the app is this example is  file-uploader-1-g7b0h . \nThe container called  file-uploader-1-build  is the builder container that deployed the application and it has already terminated.   Note  The exact name of the pod will be different in your environment.    Use the following shell command to store the exact name of the  file-uploader  application pod in your environment in a shell variable called  UPLOADER_POD :  UPLOADER_POD=$(oc get pods -l app=file-uploader -o jsonpath= {.items[0].metadata.name} )\necho $UPLOADER_POD   Use the remote shell capability of the  oc  client to list the content of  uploaded/  directory inside the pod after you uploaded a file in the PHP app:  oc rsh $UPLOADER_POD ls -ahl /opt/app-root/src/uploaded  In the below example output we ve uploaded a file named  cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz  in the app via the browser, and we see it store from within the pod:  total 16K\n-rw-r--r--. 1 1000080000 root 16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz  The app should also list the file in the overview:   However, in it s default configuration his pod currently does not use any persistent storage. It uses it s local filesystem - that is stores the file in inside the container image s root filesystem.   Important  Never store important data inside a pods root filesystem or  emptyDir . It\u2019s ephemeral by definition and will be lost as soon as the pod terminates. \nWorse, the container s root filesystem is even slower than  emptyDir  as it needs to traverse the  overlay2  stack, that Red Hat Enterprise Linux uses by default as of version 7.4 for running container images. \nAlso, inherently pods using this kind of storage cannot be scaled out trivially.   Let\u2019s see when this become a problem.   Let\u2019s scale the deployment to 3 instances of the app:  oc scale dc/file-uploader --replicas=3   Watch the additional pods getting spawned:  oc get pods -l app=file-uploader  You will see 2 additional pods being spawned:  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-3cgh1            1/1       Running     0          20s\nfile-uploader-1-3hckj            1/1       Running     0          20s\nfile-uploader-1-g7b0h            1/1       Running     0          3m   Note  The pod names will be different in your environment since they are automatically generated. It takes a couple of seconds until they are ready.   Alternatively, in the UI, wait for the  file-uploader  application reach 3 healthy pods (the blue circle is completely filled):   On the command line this will look like this:  oc get pods -l app=file-uploader  NAME                    READY     STATUS    RESTARTS   AGE\nfile-uploader-1-98fwm   1/1       Running   0          2m\nfile-uploader-1-g7b0h   1/1       Running   0          8m\nfile-uploader-1-rwt2p   1/1       Running   0          2m  These 3 pods now make up our application. OpenShift will load balance incoming traffic between them. \nHowever, when you log on to one of the new instances you will see they have no data.   Store the names of all in some environment variables for easy copy paste:  UPLOADER_POD_1=$(oc get pods -l app=file-uploader -o jsonpath= {.items[0].metadata.name} )\nUPLOADER_POD_2=$(oc get pods -l app=file-uploader -o jsonpath= {.items[1].metadata.name} )\nUPLOADER_POD_3=$(oc get pods -l app=file-uploader -o jsonpath= {.items[2].metadata.name} )   Lets check all upload directories of all pods:  oc rsh $UPLOADER_POD_1 ls -ahl /opt/app-root/src/uploaded\noc rsh $UPLOADER_POD_2 ls -ahl /opt/app-root/src/uploaded\noc rsh $UPLOADER_POD_3 ls -ahl /opt/app-root/src/uploaded  Oh oh, only one of the pods has the previously uploaded file. Looks like our application data is not consistent anymore:  oc rsh $UPLOADER_POD_1 ls -ahl /opt/app-root/src/uploaded\ntotal 0\ndrwxrwxr-x. 2 default root  22 Sep 24 11:31 .\ndrwxrwxr-x. 1 default root 124 Sep 24 11:31 ..\n-rw-rw-r--. 1 default root   0 Sep 24 11:31 .gitkeep\n\noc rsh $UPLOADER_POD_2 ls -ahl /opt/app-root/src/uploaded\ntotal 108K\ndrwxrwxr-x. 1 default    root   52 Sep 24 11:35 .\ndrwxrwxr-x. 1 default    root   22 Sep 24 11:31 ..\n-rw-rw-r--. 1 default    root    0 Sep 24 11:31 .gitkeep\n-rw-r--r--. 1 1000080000 root  16K May 26 09:32 cns-deploy-4.0.0-15.el7rhgs.x86_64.rpm.gz\n\noc rsh $UPLOADER_POD_3 ls -ahl /opt/app-root/src/uploaded\ntotal 0\ndrwxrwxr-x. 2 default root  22 Sep 24 11:31 .\ndrwxrwxr-x. 1 default root 124 Sep 24 11:31 ..\n-rw-rw-r--. 1 default root   0 Sep 24 11:31 .gitkeep  It s empty because the previously uploaded files were stored locally in the first container and are not available to the others.  Similarly, other users of the app will sometimes see your uploaded files and sometimes not. With the deployment scaled to 3 instances OpenShifts router will simply round-robin across them. You can simulate this with another instance of your browser in  Incognito mode  pointing to your app.  The app is of course not usable like this. We can fix this by providing shared storage to this app.   First create a  PVC  with the appropriate setting in a file called  cns-rwx-pvc.yml  with below contents:  cns-rwx-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-shared-storage  spec : \n   accessModes : \n   -   ReadWriteMany \n   resources : \n     requests : \n       storage :   5Gi \n   storageClassName :   glusterfs-storage   Notice the access mode explicitly requested to be  ReadWriteMany  (also referred to as  RWX ). Storage provisioned like this can be mounted by multiple containers on multiple hosts at the same time.   Submit the request to the system:  oc create -f cns-rwx-pvc.yml   Let\u2019s look at the result:  oc get pvc  ACCESSMODES  is set to  RWX :  NAME                STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\nmy-shared-storage   Bound     pvc-62aa4dfe-4ad2-11e7-b56f-2cc2602a6dc8   10Gi       RWX           22s\n...  We can now update the  DeploymentConfig  of our application to use this  PVC  to provide the application with persistent, shared storage for uploads.   Update the configuration of the application by adding a volume claim like this:  oc volume dc/file-uploader --add --name=shared-storage --type=persistentVolumeClaim --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded  Our app will now re-deploy (in a rolling fashion) with the new settings - all pods will mount the volume identified by the  PVC  under  /opt/app-root/src/upload  (the path is predictable so we can hard-code it here).   You can watch it like this:  oc logs dc/file-uploader -f  The new  DeploymentConfig  will supersede the old one.  --  Scaling up file-uploader-2 from 0 to 3, scaling down file-uploader-1 from 3 to 0 (keep 3 pods available, don t exceed 4 pods)\n    Scaling file-uploader-2 up to 1\n    Scaling file-uploader-1 down to 2\n    Scaling file-uploader-2 up to 2\n    Scaling file-uploader-1 down to 1\n    Scaling file-uploader-2 up to 3\n    Scaling file-uploader-1 down to 0\n--  Success  Exit out of the follow mode with:  Ctrl  +  c   Warning  Changing the storage settings of a pod can be destructive. Any existing data will  not be preserved . You are responsible to care for data migration. \nOne strategy here could have been to use  oc rsync  saving the data to a local directory on the machine running the  oc  client.   You can also observe the rolling upgrade of the file uploader application in the OpenShift UI:   The new  DeploymentConfig  named  file-uploader-2  will have 3 pods all sharing the same storage.   Get the names of the new pods:  oc get pods -l app=file-uploader  Output:  NAME                             READY     STATUS      RESTARTS   AGE\nfile-uploader-1-build            0/1       Completed   0          18m\nfile-uploader-2-jd22b            1/1       Running     0          1m\nfile-uploader-2-kw9lq            1/1       Running     0          2m\nfile-uploader-2-xbz24            1/1       Running     0          1m  Try it out in your application: upload new files and watch them being visible from within all application pods. In new browser  Incognito  sessions, simulating other users, the application behaves normally as it circles through the pods between browser requests.  That\u2019s it. You have successfully provided shared storage to pods throughout the entire system, therefore avoiding the need for data to be replicated at the application level to each pod.  With CNS this is available wherever OpenShift is deployed with no external dependency.", 
            "title": "Providing shared storage to multiple application instances"
        }, 
        {
            "location": "/module-4-cluster-ops/", 
            "text": "Overview\n\n\nIn this module you be introduced to some standard operational procedures. You will learn how to run multiple GlusterFS \nTrusted Storage Pools\n on OpenShift and how to expand and maintain deployments.\n\n\nHerein, we will use the term pool (GlusterFS terminology) and cluster (\nheketi\n terminology) interchangeably.\n\n\nThis module requires that you have completed \nModule 2\n.\n\n\n\n\nRunning multiple storage pools\n#\n\n\nIn the previous modules a single GlusterFS cluster was used to supply \nPersistentVolumes\n to applications. CNS allows for multiple clusters to run in a single OpenShift deployment, controlled by a central \nheketi\n API:\n\n\nThere are several use cases for this:\n\n\n\n\n\n\nProvide data isolation between clusters of different tenants\n\n\n\n\n\n\nProvide multiple performance tiers of CNS, i.e. HDD-based vs. SSD-based\n\n\n\n\n\n\nRun OpenShift across large geo-graphical distances with a CNS cluster per region whereas otherwise latency prohibits synchronous data replication in  a stretched setup\n\n\n\n\n\n\n!!! Note:\n\n    The procedures to add an additional CNS cluster to an existing setup is not yet supported by \nopenshift-ansible\n.\n\n\nBecause we cannot use \nopenshift-ansible\n as of today we need to run a couple of steps manually that would otherwise be automated.\n\n\nTo deploy a second CNS cluster, aka GlusterFS pool, follow these steps:\n\n\n Log in as \noperator\n to namespace \napp-storage\n\n\noc login -u operator -n app-storage\n\n\n\n\n\nYour deployment has 6 OpenShift Application Nodes in total, \nnode-1\n, \nnode-2\n and \nnode-3\n currently setup running CNS. We will now set up a \nsecond CNS cluster\n using \nnode-4\n, \nnode-5\n and \nnode-6\n.\n\n\nFirst we need to make sure the firewall on those systems is updated. Without \nopenshift-ansible\n automating CNS deployment the ports necessary for running GlusterFS are not yet opened.\n\n\n First, create a file called \nconfigure-firewall.yml\n and copy\npaste the following contents:\n\n\nconfigure-firewall.yml:\n\n\n---\n\n\n\n-\n \nhosts\n:\n\n    \n-\n \nnode-4.lab\n\n    \n-\n \nnode-5.lab\n\n    \n-\n \nnode-6.lab\n\n\n  \ntasks\n:\n\n\n    \n-\n \nname\n:\n \ninsert iptables rules required for GlusterFS\n\n      \nblockinfile\n:\n\n        \ndest\n:\n \n/etc/sysconfig/iptables\n\n        \nblock\n:\n \n|\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT\n\n          \n-A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT\n\n        \ninsertbefore\n:\n \n^COMMIT\n\n\n    \n-\n \nname\n:\n \nreload iptables\n\n      \nsystemd\n:\n\n        \nname\n:\n \niptables\n\n        \nstate\n:\n \nreloaded\n\n\n\n...\n\n\n\n\n\n\n Run this small Ansible playbook to apply and reload the firewall configuration on all 3 nodes conveniently:\n\n\nansible-playbook configure-firewall.yml\n\n\n\n\n\nThe playbook should complete successfully:\n\n\nPLAY [node-4.lab,node-5.lab,node-6.lab] ******************************************************************************************\n\nTASK [Gathering Facts] ***********************************************************************************************************\nSunday 24 September 2017  14:02:50 +0000 (0:00:00.056)       0:00:00.056 ******\nok: [node-5.lab]\nok: [node-6.lab]\nok: [node-4.lab]\n\nTASK [insert iptables rules required for GlusterFS] ******************************************************************************\nSunday 24 September 2017  14:02:51 +0000 (0:00:00.859)       0:00:00.916 ******\nchanged: [node-4.lab]\nchanged: [node-5.lab]\nchanged: [node-6.lab]\n\nTASK [reload iptables] ***********************************************************************************************************\nSunday 24 September 2017  14:02:51 +0000 (0:00:00.268)       0:00:01.184 ******\nchanged: [node-6.lab]\nchanged: [node-5.lab]\nchanged: [node-4.lab]\n\nPLAY RECAP ***********************************************************************************************************************\nnode-4.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-5.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-6.lab                 : ok=3    changed=2    unreachable=0    failed=0\n\nSunday 24 September 2017  14:02:52 +0000 (0:00:00.334)       0:00:01.519 ******\n===============================================================================\nGathering Facts --------------------------------------------------------- 0.86s\nreload iptables --------------------------------------------------------- 0.34s\ninsert iptables rules required for GlusterFS ---------------------------- 0.27s\n\n\n\n\n\n Next, we need to apply additional labels to the remaining 3 OpenShift Nodes:\n\n\noc label node/node-4.lab glusterfs=storage-host\noc label node/node-5.lab glusterfs=storage-host\noc label node/node-6.lab glusterfs=storage-host\n\n\n\n\n\nThe label will be used to control GlusterFS pod placement and availability. They are part of a \nDaemonSet\n definition that is looking for hosts with this particular label.\n\n\n Wait for all pods to show \n1/1\n in the \nREADY\n column:\n\n\n oc get pods -o wide -n app-storage\n\n\n\n\n\nYou can also watch the additional GlusterFS pods deploy in the OpenShift UI, while being logged in as \noperator\n in project \napp-storage\n, select \nApplications\n from the left menu and then \nPods\n:\n\n\n\n\n!!! Note:\n\n    It may take up to 3 minutes for the GlusterFS pods to transition into \nREADY\n state.\n\n\n When done, on the CLI display all GlusterFS pods alongside with the name of the container host they are running on:\n\n\noc get pods -o wide -n app-storage -l glusterfs=storage-pod\n\n\n\n\n\nYou will see that now also app nodes \nnode-4\n, \nnode-5\n and \nnode-6\n run GlusterFS pods, although they are unitialized and not yet ready to use by CNS yet.\n\n\nFor manual bulk import of new nodes like this, a JSON topology file is used which includes the existing cluster as well as the new, second cluster with a separate set of nodes.\n\n\n Create a new file named \n2-clusters-topology.json\n with the content below (use copy\npaste):\n\n\n2-clusters-topology.json:\n\n\n{\n\n    \nclusters\n:\n \n[\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-1.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.201\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-2.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.202\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-3.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.203\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n\n        \n},\n\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-4.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.204\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-5.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.205\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-6.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.206\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\nThe file contains the same content as the dynamically generated JSON structure \nopenshift-ansible\n used, but with a second cluster specification (beginning at the highlighted line).\n\n\nWhen loading this topology to \nheketi\n, it will recognize the existing cluster (leaving it unchanged) and start creating the new one, with the same bootstrapping process used to initialize the first cluster.\n\nThat is: the \nglusterd\n process running in the pods will form a new 3-node cluster and the supplied block storage device \n/dev/xvdc\n will be formatted.\n\n\n Prepare the heketi CLI tool like previously in \nModule 2\n.\n\n\nHEKETI_POD=$(oc get pods -l glusterfs=heketi-storage-pod -n app-storage -o jsonpath=\n{.items[0].metadata.name}\n)\nexport HEKETI_CLI_SERVER=http://$(oc get route/heketi-storage -o jsonpath=\n{.spec.host}\n)\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=$(oc get pod/$HEKETI_POD -o jsonpath=\n{.spec.containers[0].env[?(@.name==\nHEKETI_ADMIN_KEY\n)].value}\n)\n\n\n\n\n\n Verify there is currently only a single cluster known to heketi\n\n\nheketi-cli cluster list\n\n\n\n\n\nExample output:\n\n\nClusters:\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\nYour ID will be different since it\ns auto-generated.\n\n\n Save your specific ID of the first cluster with this shell command (and the versatile \njq\n json parser) into an environment variable:\n\n\nFIRST_CNS_CLUSTER=$(heketi-cli cluster list --json | jq -r \n.clusters[0]\n)\n\n\n\n\n\n\n\nImportant\n\n\nDo not skip above step. The value in the environment variable \nFIRST_CNS_CLUSTER\n is required later in this module.\n\n\n\n\n Load the new topology with the heketi client\n\n\nheketi-cli topology load --json=2-clusters-topology.json\n\n\n\n\n\nYou should see output similar to the following:\n\n\nFound node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nCreating cluster ... ID: 46b205a4298c625c4bca2206b7a82dd3\nCreating node node-4.lab ... ID: 604d2eb15a5ca510ff3fc5ecf912d3c0\nAdding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 538b860406870288af23af0fbc2cd27f\nAdding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 7736bd0cb6a84540860303a6479cacb2\nAdding device /dev/xvdc ... OK\n\n\n\n\n\nAs indicated from above output a new cluster got created.\n\n\n List all clusters:\n\n\nheketi-cli cluster list\n\n\n\n\n\nYou should see a second cluster in the list:\n\n\nClusters:\n46b205a4298c625c4bca2206b7a82dd3\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\nThe second cluster, in this example with the ID \n46b205a4298c625c4bca2206b7a82dd3\n, is an entirely independent GlusterFS deployment. The exact value will be different in your environment.\n\n\nheketi\n is now able to differentiate between the clusters with storage provisioning requests when their UUID is specified.\n\n\n Save the UUID of the second CNS cluster in an environment variable as follows for easy copy\npaste later:\n\n\nSECOND_CNS_CLUSTER=$(heketi-cli cluster list --json | jq -r \n.clusters[] | select(contains(\\\n$FIRST_CNS_CLUSTER\\\n) | not)\n)\n\n\n\n\n\nNow we have two independent GlusterFS clusters managed by the same heketi instance:\n\n\n\n\n\n\n\n\n\n\nNodes\n\n\nCluster UUID\n\n\n\n\n\n\n\n\n\n\nFirst Cluster\n\n\nnode-1, node-2, node-3\n\n\nfb67f97166c58f161b85201e1fd9b8ed\n\n\n\n\n\n\nSecond Cluster\n\n\nnode-4, node-5, node-6\n\n\n46b205a4298c625c4bca2206b7a82dd3\n\n\n\n\n\n\n\n\n Query the updated topology:\n\n\nheketi-cli topology info\n\n\n\n\n\nAbbreviated output:\n\n\nCluster Id: 46b205a4298c625c4bca2206b7a82dd3\n\n    Volumes:\n\n    Nodes:\n\n      Node Id: 538b860406870288af23af0fbc2cd27f\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 2\n      Management Hostname: node-5.lab\n      Storage Hostname: 10.0.3.105\n      Devices:\n        Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 1\n      Management Hostname: node-4.lab\n      Storage Hostname: 10.0.2.104\n      Devices:\n        Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 7736bd0cb6a84540860303a6479cacb2\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 3\n      Management Hostname: node-6.lab\n      Storage Hostname: 10.0.4.106\n      Devices:\n        Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\n[...output omitted for brevity...]\n\n\n\n\n\nheketi formed an new, independent 3-node GlusterFS cluster on those nodes.\n\n\n Check running GlusterFS pods\n\n\noc get pods -o wide -l glusterfs=storage-pod\n\n\n\n\n\nFrom the output you can spot the pod names running on the new cluster\ns nodes:\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\n\nglusterfs-1nvtj   1/1       Running   0          23m       10.0.4.206   node-6.lab\n\nglusterfs-5gvw8   1/1       Running   0          24m       10.0.2.204   node-4.lab\n\nglusterfs-5rc2g   1/1       Running   0          4h        10.0.2.201   node-1.lab\n\nglusterfs-b4wg1   1/1       Running   0          24m       10.0.3.205   node-5.lab\n\nglusterfs-jbvdk   1/1       Running   0          4h        10.0.3.202   node-2.lab\nglusterfs-rchtr   1/1       Running   0          4h        10.0.4.203   node-3.lab\n\n\n\n\n\n!!! Note:\n\n    Again note that the pod names are dynamically generated and will be different. Look the FQDN of your hosts to determine one of new cluster\ns pods.\n\n\n Let\ns run the \ngluster peer status\n command in the GlusterFS pod running on the node \nnode-6.lab\n:\n\n\nPOD_NUMBER_SIX=$(oc get pods -o jsonpath=\n{.items[?(@.status.hostIP==\n10.0.4.206\n)].metadata.name}\n)\noc rsh $POD_NUMBER_SIX gluster peer status\n\n\n\n\n\nAs expected this node only has 2 peers, evidence that it\ns running in it\ns own GlusterFS pool separate from the first cluster in deployed in Module 2.\n\n\nNumber of Peers: 2\n\nHostname: node-5.lab\nUuid: 0db9b5d0-7fa8-4d2f-8b9e-6664faf34606\nState: Peer in Cluster (Connected)\nOther names:\n10.0.3.205\n\nHostname: node-4.lab\nUuid: 695b661d-2a55-4f94-b22e-40a9db79c01a\nState: Peer in Cluster (Connected)\n\n\n\n\n\nBefore you can use the second cluster two tasks have to be accomplished so we can use both distinctively:\n\n\n\n\n\n\nThe \nStorageClass\n for the first cluster has to be updated to point the first cluster\ns UUID,\n\n\n\n\n\n\nA second \nStorageClass\n for the second cluster has to be created, pointing to the same heketi API\n\n\n\n\n\n\n\n\nWhy do we need to update the first \nStorageClass\n?\n\n\nWhen no cluster UUID is specified, heketi serves volume creation requests from any cluster currently registered to it. That would be two now.\n\nIn order to request a volume from specific cluster you have to supply the cluster\ns UUID to heketi. This is done via a parameter in the \nStorageClass\n. The first \nStorageClass\n has no UUID specified so far because \nopenshift-ansible\n did not create it.\n\n\n\n\nUnfortunately you cannot \noc patch\n a \nStorageClass\n parameters in OpenShift. So we have to delete it and re-create it. Don\nt worry - existing \nPVCs\n will remain untouched.\n\n\nTo simplify our work, instead of typing JSON/YAML, we will just export the current \nStorageClass\n definition JSON and manipulate it using \njq\n and some clever JSON queries to put the additional \nclusterid\n parameter in the right place.\n\n\n To do that, run the following command via copy\npaste:\n\n\noc get storageclass/glusterfs-storage -o json \\\n| jq \n.parameters=(.parameters + {\\\nclusterid\\\n: \\\n$FIRST_CNS_CLUSTER\\\n})\n \n glusterfs-storage-fast.json\n\n\n\n\n\nThis will result in a file named \nglusterfs-storage-fast.json\n looking like the following:\n\n\nglusterfs-storage-fast.json:\n\n\n{\n\n  \napiVersion\n:\n \nstorage.k8s.io/v1\n,\n\n  \nkind\n:\n \nStorageClass\n,\n\n  \nmetadata\n:\n \n{\n\n    \ncreationTimestamp\n:\n \n2017-09-24T12:45:24Z\n,\n\n    \nname\n:\n \nglusterfs-storage\n,\n\n    \nresourceVersion\n:\n \n2697\n,\n\n    \nselfLink\n:\n \n/apis/storage.k8s.io/v1/storageclasses/glusterfs-storage\n,\n\n    \nuid\n:\n \n3c107010-a126-11e7-b0a5-025fcde0880f\n\n  \n},\n\n  \nparameters\n:\n \n{\n\n    \nresturl\n:\n \nhttp://heketi-storage-app-storage.cloudapps.34.252.58.209.nip.io\n,\n\n    \nrestuser\n:\n \nadmin\n,\n\n    \nsecretName\n:\n \nheketi-storage-admin-secret\n,\n\n    \nsecretNamespace\n:\n \napp-storage\n,\n\n\n    \nclusterid\n:\n \nfb67f97166c58f161b85201e1fd9b8ed\n\n\n  \n},\n\n  \nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\n}\n\n\n\n\n\n\nNote the additional \nclusterid\n parameter highlighted. It\ns the first cluster\ns UUID as known by heketi. The exact values will be different in your environment. The rest of the definition remains the same.\n\n\n Delete the existing \nStorageClass\n definition in OpenShift\n\n\noc delete storageclass/glusterfs-storage\n\n\n\n\n\n Add the \nStorageClass\n again:\n\n\noc create -f glusterfs-storage-fast.json\n\n\n\n\n\nStep 1 complete. The existing \nStorageClass\n is \nupdated\n. \nPVC\n using the \nStorageClass\n \nglusterfs-storage\n will now specifically get served by the first CNS cluster, and only the first cluster.\n\n\nTo relieve you from manually editing JSON files, we will again use some \njq\n magic to generate the correct JSON structure for our second \nStorageClass\n, this time using the second CNS cluster\ns ID and a different name \nglusterfs-storage-slow\n\n\n Run the following command:\n\n\noc get storageclass/glusterfs-storage -o json \\\n| jq \n.parameters=(.parameters + {\\\nclusterid\\\n: \\\n$SECOND_CNS_CLUSTER\\\n})\n \\\n| jq \n.metadata.name = \nglusterfs-storage-slow\n \n glusterfs-storage-slow.json\n\n\n\n\n\nThis creates a file called \nglusterfs-storage-slow.json\n, looking similar to the below:\n\n\nglusterfs-storage-slow.json:\n\n\n{\n\n  \napiVersion\n:\n \nstorage.k8s.io/v1\n,\n\n  \nkind\n:\n \nStorageClass\n,\n\n  \nmetadata\n:\n \n{\n\n    \ncreationTimestamp\n:\n \n2017-09-24T15:12:34Z\n,\n\n\n    \nname\n:\n \nglusterfs-storage-slow\n,\n\n\n    \nresourceVersion\n:\n \n12722\n,\n\n    \nselfLink\n:\n \n/apis/storage.k8s.io/v1/storageclasses/glusterfs-storage\n,\n\n    \nuid\n:\n \ncb16946d-a13a-11e7-b0a5-025fcde0880f\n\n  \n},\n\n  \nparameters\n:\n \n{\n\n\n    \nclusterid\n:\n \n46b205a4298c625c4bca2206b7a82dd3\n,\n\n\n    \nresturl\n:\n \nhttp://heketi-storage-app-storage.cloudapps.34.252.58.209.nip.io\n,\n\n    \nrestuser\n:\n \nadmin\n,\n\n    \nsecretName\n:\n \nheketi-storage-admin-secret\n,\n\n    \nsecretNamespace\n:\n \napp-storage\n\n  \n},\n\n  \nprovisioner\n:\n \nkubernetes.io/glusterfs\n\n\n}\n\n\n\n\n\n\nAgain note the \nclusterid\n in the \nparameters\n section referencing the second cluster\ns UUID will as well as the update.\n\n\n Add the new \nStorageClass\n:\n\n\noc create -f glusterfs-storage-slow.json\n\n\n\n\n\nThis creates the \nStorageClass\n named \nglusterfs-storage-slow\n and because we copied the settings from the first one it\ns now also set as system-wide default (yes, OpenShift allows you to do that).\n\n\n Use the \noc patch\n command to fix this:\n\n\noc patch storageclass glusterfs-storage-slow \\\n-p \n{\nmetadata\n: {\nannotations\n: {\nstorageclass.kubernetes.io/is-default-class\n: \nfalse\n}}}\n\n\n\n\n\n\n Display all \nStorageClass\n objects to verify:\n\n\noc get storageclass\n\n\n\n\n\nThat\ns it. You now have 2 \nStorageClass\n definitions, one for each CNS cluster, managed by the same \nheketi\n instance.\n\n\nNAME                          TYPE\nglusterfs-storage (default)   kubernetes.io/glusterfs\nglusterfs-storage-slow        kubernetes.io/glusterfs\n\n\n\n\n\nLet\ns verify both \nStorageClasses\n are working as expected:\n\n\n Create the following two files containing PVCs issued against either of both GlusterFS pools via their respective \nStorageClass\n:\n\n\ncns-pvc-fast.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-fast-container-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteMany\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n5Gi\n\n  \nstorageClassName\n:\n \nglusterfs-storage\n\n\n\n\n\n\ncns-pvc-slow.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-slow-container-storage\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteMany\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n7Gi\n\n  \nstorageClassName\n:\n \nglusterfs-storage-slow\n\n\n\n\n\n\n Create both PVCs:\n\n\noc create -f cns-pvc-fast.yml\noc create -f cns-pvc-slow.yml\n\n\n\n\n\nCheck their provisioning state after a few seconds:\n\n\noc get pvc\n\n\n\n\n\nThey should both be in bound state after a couple of seconds:\n\n\nNAME                        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS             AGE\nmy-fast-container-storage   Bound     pvc-bfbf3f72-a13d-11e7-b0a5-025fcde0880f   5Gi        RWX           glusterfs-storage        6s\nmy-slow-container-storage   Bound     pvc-c045c082-a13d-11e7-b0a5-025fcde0880f   7Gi        RWX           glusterfs-storage-slow   6s\n\n\n\n\n\n If you check again the GlusterFS pod on on \nnode-6.lab\n running as part of the second cluster\n\n\noc rsh $POD_NUMBER_SIX gluster vol list\n\n\n\n\n\nyou will see a new volume has been created\n\n\nvol_755b4434cf9062104123e0d9919dd800\n\n\n\n\n\nThe other volume has been created on the first cluster.\n\n\n If you were to check GlusterFS on \nnode-1.lab\n\n\nPOD_NUMBER_ONE=$(oc get pods -o jsonpath=\n{.items[?(@.status.hostIP==\n10.0.2.201\n)].metadata.name}\n)\noc rsh $POD_NUMBER_ONE gluster vol list\n\n\n\n\n\nyou will also see a new volume has been created, alongside the volumes from the previous exercises and the \nheketidbstorage\n volume:\n\n\nheketidbstorage\n[...output omitted... ]\nvol_8eb957320215fe8801748b239d524808\n\n\n\n\n\nIf you now compare the \nPV\n objects that have been created:\n\n\n \n the first PV using \nStorageClass\n glusterfs-storage:\n\n\nFAST_PV=$(oc get pvc/my-fast-container-storage -o jsonpath=\n{.spec.volumeName}\n)\noc get pv/$FAST_PV -o jsonpath=\n{.spec.glusterfs.path}\n\n\n\n\n\n\n \n and the second PV using \nStorageClass\n glusterfs-storage-slow:\n\n\nSLOW_PV=$(oc get pvc/my-slow-container-storage -o jsonpath=\n{.spec.volumeName}\n)\noc get pv/$SLOW_PV -o jsonpath=\n{.spec.glusterfs.path}\n\n\n\n\n\n\n you will notice that they match the volumes found in the CNS clusters from within their pods respectively.\n\n\nThis is how you use multiple, parallel GlusterFS pools/clusters on a single OpenShift cluster with a single heketi instance. Whereas the first pool is created with \nopenshift-ansible\n subsequent pools/cluster are created with the \nheketi-cli\n client.\n\n\nClean up the PVCs and the second \nStorageClass\n in preparation for the next section.\n\n\n Delete both PVCs (and therefore their volume)\n\n\noc delete pvc/my-fast-container-storage\noc delete pvc/my-slow-container-storage\n\n\n\n\n\n Delete the second \nStorageClass\n\n\noc delete storageclass/glusterfs-storage-slow\n\n\n\n\n\n\n\nDeleting a CNS cluster\n#\n\n\nSince we want to re-use \nnode-4\n, \nnode-5\n and \nnode-6\n for the next section we need to delete it the GlusterFS pools on top of them first.\n\n\nThis is a process that involves multiple steps of manipulating the heketi topology with the \nheketi-cli\n client.\n\n\n Make sure the client is still properly configured via environment variables:\n\n\necho $HEKETI_CLI_SERVER\necho $HEKETI_CLI_USER\necho $HEKETI_CLI_KEY\n\n\n\n\n\n We also require the environment variables storing the UUIDs of both CNS clusters:\n\n\necho $FIRST_CNS_CLUSTER\necho $SECOND_CNS_CLUSTER\n\n\n\n\n\n First display the entire system topology as it is known to heketi:\n\n\nheketi-cli topology info\n\n\n\n\n\nYou will get detailled infos about both clusters.\n\nThe portions of interest for the second clusters we are about to delete are highlighted:\n\n\nCluster Id: 46b205a4298c625c4bca2206b7a82dd3\n\n\n    Volumes:\n\n    Nodes:\n\n\n        Node Id: 538b860406870288af23af0fbc2cd27f\n\n        State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 2\n        Management Hostname: node-5.lab\n        Storage Hostname: 10.0.3.105\n        Devices:\n\n            Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n                Bricks:\n\n\n        Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0\n\n        State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 1\n        Management Hostname: node-4.lab\n        Storage Hostname: 10.0.2.104\n        Devices:\n\n            Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n                Bricks:\n\n\n        Node Id: 7736bd0cb6a84540860303a6479cacb2\n\n        State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 3\n        Management Hostname: node-6.lab\n        Storage Hostname: 10.0.4.106\n        Devices:\n\n            Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n            Bricks:\n\n\n\n\n\nThe hierachical dependencies in this topology works as follows: Clusters \n Nodes \n Devices.\n\nAssuming there are no volumes present these need to be deleted in reverse order.\n\n\nTo make navigating this process easier and avoid mangling with anonymous UUID values we will use some simple scripting.\n\n\n This is how you get all nodes IDs of the second cluster:\n\n\nheketi-cli cluster info $SECOND_CNS_CLUSTER --json | jq -r \n.nodes[]\n\n\n\n\n\n\nFor example:\n\n\n538b860406870288af23af0fbc2cd27f\n604d2eb15a5ca510ff3fc5ecf912d3c0\n7736bd0cb6a84540860303a6479cacb2\n\n\n\n\n\n Let\ns put this in a variable so we can iterate over it:\n\n\nNODES=$(heketi-cli cluster info $SECOND_CNS_CLUSTER --json | jq -r \n.nodes[]\n)\n\n\n\n\n\n This is how you get information about a node\n\n\nheketi-cli node info ${NODES[0]}\n\n\n\n\n\nNode Id: 538b860406870288af23af0fbc2cd27f\nState: online\nCluster Id: 38cba86da51146a0ef9747383bd44476\nZone: 2\nManagement Hostname: node-5.lab\nStorage Hostname: 10.0.3.205\nDevices:\nId:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n\n\n\n\n Let\ns iterate over this \nNODES\n array and extract all device IDs:\n\n\nfor node in ${NODES} ; do heketi-cli node info $node --json | jq -r \n.devices[].id\n ; done\n\n\n\n\n\n for example:\n\n\ne481d022cea9bfb11e8a86c0dd8d3499\n09a25a114c53d7669235b368efd2f8d1\ncccadb2b54dccd99f698d2ae137a22ff\n\n\n\n\n\n Let\ns put this in a a variable too, so we can easily iterate over that:\n\n\nDEVICES=$(for node in ${NODES} ; do heketi-cli node info $node --json | jq -r \n.devices[].id\n ; done)\n\n\n\n\n\n Let\ns loop over this \nDEVICES\n array and delete the device by it\ns ID in heketi:\n\n\nfor device in $DEVICES ; do heketi-cli device delete $device ; done\n\n\n\n\n\nExample output:\n\n\nDevice 538b860406870288af23af0fbc2cd27f deleted\nDevice 604d2eb15a5ca510ff3fc5ecf912d3c0 deleted\nDevice 7736bd0cb6a84540860303a6479cacb2 deleted\n\n\n\n\n\n Since the nodes have no devices anymore we can delete those as well (you can\nt delete a node with a device still attached):\n\n\nfor node in ${NODES} ; do heketi-cli node delete $node ; done\n\n\n\n\n\nExample output:\n\n\nNode 4ff85abd2674c89e79c1f7c7f8ee1be4 deleted\nNode ed9c045f10a5c1f9057d07880543a461 deleted\nNode fd6ddca52c788e2d764fada1f4da2ce4 deleted\n\n\n\n\n\n Finally, without any nodes in the second cluster, you can also delete it (it won\nt work if there are nodes left):\n\n\nheketi-cli cluster delete $SECOND_CNS_CLUSTER\n\n\n\n\n\n Confirm the cluster is gone:\n\n\nheketi-cli cluster list\n\n\n\n\n\n Verify the new topology known by heketi now only containing a single cluster.\n\n\nheketi-cli topology info\n\n\n\n\n\nThis deleted all heketi database entries about the cluster. However the GlusterFS pods are still running, since they are controlled directly by OpenShift and the \nDaemonSet\n.\n\n\nThey can be stopped by removing the labels OpenShift uses to determine GlusterFS pod placement for CNS.\n\n\n Remove the labels from the last 3 OpenShift nodes like so:\n\n\noc label node/node-4.lab glusterfs-\noc label node/node-5.lab glusterfs-\noc label node/node-6.lab glusterfs-\n\n\n\n\n\nContrary to the output of these commands the label \nglusterfs\n is actually removed (indicated by the minus sign).\n\n\n Verify that all GlusterFS pods running on \nnode-4\n, \nnode-5\n and \nnode-6\n are indeed terminated:\n\n\noc get pods -o wide -n app-storage -l glusterfs=storage-pod\n\n\n\n\n\n!!! Note:\n\n    It can take up to 2 minutes for the pods to terminate.\n\n\nYou should be back down to 3 GlusterFS pods, e.g.\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.201   node-1.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.202   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.203   node-3.lab\n\n\n\n\n\n\n\nExpanding a GlusterFS pool\n#\n\n\nInstead of creating additional GlusterFS pools in CNS on OpenShift it is also possible to expand existing pools. This is useful the increase capacity, performance and resiliency of the storage system.\n\n\nThis works similar to creating additional pools, with bulk-import via the topology file. Only this time with nodes added to the existing cluster structure in JSON.\n\n\nSince manipulating JSON can be error-prone create a new file called \nexpanded-cluster.json\n with contents as below:\n\n\nexpanded-cluster.json:\n\n\n{\n\n    \nclusters\n:\n \n[\n\n        \n{\n\n            \nnodes\n:\n \n[\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-1.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.201\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-2.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.202\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-3.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.203\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-4.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.2.204\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n1\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-5.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.3.205\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n2\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n},\n\n                \n{\n\n                    \nnode\n:\n \n{\n\n                        \nhostnames\n:\n \n{\n\n                            \nmanage\n:\n \n[\n\n                                \nnode-6.lab\n\n                            \n],\n\n                            \nstorage\n:\n \n[\n\n                                \n10.0.4.206\n\n                            \n]\n\n                        \n},\n\n                        \nzone\n:\n \n3\n\n                    \n},\n\n                    \ndevices\n:\n \n[\n\n                        \n/dev/xvdc\n\n                    \n]\n\n                \n}\n\n            \n]\n\n        \n}\n\n    \n]\n\n\n}\n\n\n\n\n\n\nThe difference between this file to the \n2-clusters-topology.json\n is that we now have 6 nodes in a single cluster instead of 2 clusters, with 3 nodes each.\n\n\n Again, apply the expected labels to the remaining 3 OpenShift Nodes:\n\n\noc label node/node-4.lab glusterfs=storage-host\noc label node/node-5.lab glusterfs=storage-host\noc label node/node-6.lab glusterfs=storage-host\n\n\n\n\n\n Wait for all pods to show \n1/1\n in the \nREADY\n column:\n\n\n oc get pods -o wide -n app-storage -l glusterfs=storage-pod\n\n\n\n\n\n!!! Note:\n\n    It may take up to 3 minutes for the GlusterFS pods to transition into \nREADY\n state.\n\n\nThis confirms all GlusterFS pods are ready to receive remote commands:\n\n\nNAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-0lr75   1/1       Running   0          4m        10.0.4.106   node-6.lab\nglusterfs-1dxz3   1/1       Running   0          4m        10.0.3.105   node-5.lab\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab\nglusterfs-8nrn0   1/1       Running   0          4m        10.0.2.104   node-4.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab\n\n\n\n\n\n Ensure the environment variables for operating \nheketi-cli\n are still in place:\n\n\necho $HEKETI_CLI_SERVER\necho $HEKETI_CLI_USER\necho $HEKETI_CLI_KEY\n\n\n\n\n\n Now load the new topology:\n\n\nheketi-cli topology load --json=expanded-cluster.json\n\n\n\n\n\nThe output indicated that the existing cluster was expanded, rather than creating a new one:\n\n\nFound node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nCreating node node-4.lab ... ID: 544158e53934a3d351b874b7d915e8d4\n    Adding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 645b6edd4044cb1dd828f728d1c3eb81\n    Adding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 3f39ebf3c8c82531a7ba447135742776\n    Adding device /dev/xvdc ... OK\n\n\n\n\n\n Verify the their new peers are now part of the first CNS cluster:\n\n\nPOD_NUMBER_ONE=$(oc get pods -o jsonpath=\n{.items[?(@.status.hostIP==\n10.0.2.201\n)].metadata.name}\n)\noc rsh $POD_NUMBER_ONE gluster peer status\n\n\n\n\n\nYou should now have a GlusterFS consisting of 6 nodes:\n\n\nNumber of Peers: 5\n\nHostname: 10.0.3.202\nUuid: c6a6d571-fd9b-4bd8-aade-e480ec2f8eed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.203\nUuid: 46044d06-a928-49c6-8427-a7ab37268fed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.2.204\nUuid: 62abb8b9-7a68-4658-ac84-8098a1460703\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.3.205\nUuid: 5b44b6ea-6fb5-4ea9-a6f7-328179dc6dda\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.206\nUuid: ed39ecf7-1f5c-4934-a89d-ee1dda9a8f98\nState: Peer in Cluster (Connected)\n\n\n\n\n\nWith this you have expanded the existing pool. New PVCs will start to use capacity from the additional nodes.\n\n\n\n\nImportant\n\n\nIn this lab, with this expansion, you now have a GlusterFS pool with mixed media types (both size and speed). It is recommended to have the same media type per pool.\n\nIf you like to offer multiple media types for CNS in OpenShift, use separate pools and separate \nStorageClass\n objects as described in the \nprevious section\n.\n\n\n\n\n\n\nAdding a device to a node\n#\n\n\nInstead of adding entirely new nodes you can also add new storage devices for CNS to use on existing nodes.\n\n\nIt is again possible to do this by loading an updated topology file. Alternatively to bulk-loading via JSON you are also able to do this directly with the \nheketi-cli\n utility. This also applies to the previous sections in this module.\n\n\nFor this purpose \nnode-3.lab\n has an additional, so far unused block device \n/dev/xvdd\n.\n\n\n To use the heketi-cli make sure the environment variables are still set:\n\n\necho $HEKETI_CLI_SERVER\necho $HEKETI_CLI_USER\necho $HEKETI_CLI_KEY\n\n\n\n\n\n Determine the UUUI heketi uses to identify \nnode-6.lab\n in it\ns database and save it in an environment variable:\n\n\nNODE_ID_SIX=$(heketi-cli topology info --json | jq -r \n.clusters[] | select(.id==\\\n$FIRST_CNS_CLUSTER\\\n) | .nodes[] | select(.hostnames.manage[0] == \\\nnode-6.lab\\\n) | .id\n)\n\n\n\n\n\n Query the node\ns available devices:\n\n\nheketi-cli node info $NODE_ID_SIX\n\n\n\n\n\nThe node has one device available:\n\n\nNode Id: 3f39ebf3c8c82531a7ba447135742776\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.206\nDevices:\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):509     Used (GiB):0       Free (GiB):509\n\n\n\n\n\n Add the device \n/dev/xvdd\n to the node using the UUID noted earlier.\n\n\nheketi-cli device add --node=$NODE_ID_SIX --name=/dev/xvdd\n\n\n\n\n\nThe device is registered in heketi\ns database.\n\n\nDevice added successfully\n\n\n\n\n\n Query the node\ns available devices again and you\nll see a second device.\n\n\nheketi-cli node info $NODE_ID_SIX\n\n\n\n\n\nThat node now has 2 devices. The new device will be used by subsequent \nPVC\n being served by this cluster.\n\n\nNode Id: 3f39ebf3c8c82531a7ba447135742776\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.206\nDevices:\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):509     Used (GiB):0       Free (GiB):509\nId:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n\n\n\n\n\n\n\nReplacing a failed device\n#\n\n\nOne of heketi\ns advantages is the automation of otherwise tedious manual tasks, like replacing a faulty brick in GlusterFS to repair degraded volumes.\n\nWe will simulate this use case now.\n\n\n Make sure you are \noperator\n in OpenShift and using the project \nmy-test-project\n\n\noc login -u operator -n my-test-project\n\n\n\n\n\n Create the file \ncns-large-pvc.yml\n with content below:\n\n\ncns-large-pvc.yml:\n\n\nkind\n:\n \nPersistentVolumeClaim\n\n\napiVersion\n:\n \nv1\n\n\nmetadata\n:\n\n  \nname\n:\n \nmy-large-container-store\n\n\nspec\n:\n\n  \naccessModes\n:\n\n  \n-\n \nReadWriteMany\n\n  \nresources\n:\n\n    \nrequests\n:\n\n      \nstorage\n:\n \n200Gi\n\n  \nstorageClassName\n:\n \nglusterfs-storage\n\n\n\n\n\n\n Create this request for a large volume:\n\n\noc create -f cns-large-pvc.yml\n\n\n\n\n\nThe requested capacity in this \nPVC\n is larger than any single brick on nodes \nnode-1.lab\n, \nnode-2.lab\n and \nnode-3.lab\n so it will be created from the bricks of the other 3 nodes which have larger bricks (500 GiB).\n\n\nWhere are now going to determine a \nPVCs\n physical backing device on CNS. This is done with the following relationships between the various entities of GlusterFS, heketi and OpenShift in mind:\n\n\nPVC -\n PV -\n heketi volume -\n GlusterFS volume -\n GlusterFS brick -\n Physical Device\n\n\n First, get the \nPV\n\n\noc describe pvc/my-large-container-store\n\n\n\n\n\nNote the \nPVs\n name:\n\n\n    Name:       my-large-container-store\n    Namespace:  my-test-project\n    StorageClass:   app-storage\n    Status:     Bound\n\n    Volume:     pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n\n    Labels:     \nnone\n\n    Capacity:   200Gi\n    Access Modes:   RWO\n    No events.\n\n\n\n\n\n Get the GlusterFS volume name of this PV, \nuse your PVs name here\n, e.g.\n\n\noc describe pv/pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n\n\n\n\n\nThe GlusterFS volume name as it used by GlusterFS:\n\n\n    Name:       pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n    Labels:     \nnone\n\n    StorageClass:   app-storage\n    Status:     Bound\n    Claim:      my-test-project/my-large-container-store\n    Reclaim Policy: Delete\n    Access Modes:   RWO\n    Capacity:   200Gi\n    Message:\n    Source:\n        Type:       Glusterfs (a Glusterfs mount on the host that shares a pod\ns lifetime)\n        EndpointsName:  glusterfs-dynamic-my-large-container-store\n\n        Path:       vol_3ff9946ddafaabe9745f184e4235d4e1\n\n        ReadOnly:       false\n    No events.\n\n\n\n\n\nLet\ns programmatically determine and safe the relevant information so you don\nt have to type all this stuff.\n\nWe need: the \nPV\n name, the respective GlusterFS volume\ns, the name of the GlusterFS pod on the node \nnode-6.lab\n and that node\ns id in \nheketi\n and IP address in environment variables:\n\n\nLARGE_PV=$(oc get pvc/my-large-container-store -o jsonpath=\n{.spec.volumeName}\n)\nLARGE_GLUSTER_VOLUME=$(oc get pv/$LARGE_PV -o jsonpath=\n{.spec.glusterfs.path}\n)\nPOD_NUMBER_SIX=$(oc get pods -n app-storage -o jsonpath=\n{.items[?(@.status.hostIP==\n10.0.4.206\n)].metadata.name}\n)\nNODE_ID_SIX=$(heketi-cli topology info --json | jq -r \n.clusters[] | select(.id==\\\n$FIRST_CNS_CLUSTER\\\n) | .nodes[] | select(.hostnames.manage[0] == \\\nnode-6.lab\\\n) | .id\n)\nNODE_IP_SIX=$(oc get pod/$POD_NUMBER_SIX -n app-storage -o jsonpath=\n{.status.hostIP}\n)\n\necho \nLARGE_PV             = $LARGE_PV\n\necho \nLARGE_GLUSTER_VOLUME = $LARGE_GLUSTER_VOLUME\n\necho \nPOD_NUMBER_SIX       = $POD_NUMBER_SIX\n\necho \nNODE_ID_SIX          = $NODE_ID_SIX\n\necho \nNODE_IP_SIX          = $NODE_IP_SIX\n\n\n\n\n\n\n Change to the CNS namespace\n\n\noc project app-storage\n\n\n\n\n\n Log on to one of the GlusterFS pods\n\n\noc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME\n\n\n\n\n\nThe output indicates this volume is indeed backed by, among others, \nnode-6.lab\n (see highlighted line)\n\n\nVolume Name: vol_3ff9946ddafaabe9745f184e4235d4e1\nType: Replicate\nVolume ID: 774ae26f-bd3f-4c06-990b-57012cc5974b\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 10.0.3.205:/var/lib/heketi/mounts/vg_e1b93823a2906c6758aeec13930a0919/brick_b3d5867d2f86ac93fce6967128643f85/brick\nBrick2: 10.0.2.204:/var/lib/heketi/mounts/vg_3c3489a5779c1c840a82a26e0117a415/brick_6323bd816f17c8347b3a68e432501e96/brick\n\nBrick3: 10.0.4.206:/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick\n\nOptions Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on\n\n\n\n\n\n Safe the brick directory served by \nnode-6.lab\n in an environment variable:\n\n\nBRICK_DIR=$(echo -n $(oc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME | grep $NODE_IP_SIX) | cut -d \n:\n -f 3 | tr -d $\n\\r\n )\necho $BRICK_DIR\n\n\n\n\n\n Using the full path of brick you can cross-check with heketi\ns topology on which device it is based on:\n\n\nheketi-cli topology info | grep -B2 $BRICK_DIR\n\n\n\n\n\nAmong other data \ngrep\n will show the physical backing device of this brick\ns mount path:\n\n\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):201     Free (GiB):298\n\n            Bricks:\n                Id:a6c92b6a07983e9b8386871f5b82497f   Size (GiB):200     Path: /var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick\n\n\n\n\n\nIn this case it\ns \n/dev/xvdd\n of \nnode-6.lab\n.\n\n\n!!! Note:\n\n    The device might be different for you. This is subject to heketi\ns dynamic scheduling.\n\n\nWe will now proceed to disable and delete this device. For that we have to find and use it\ns UUID in \nheketi\n.\n\n\nSafe the \nheketi\n device\ns ID from the brick on \nnode-6.lab\n using the following definition of an environment variable, again by leveraging \njq\n to parse the JSON output of \nheketi topology info\n:\n\n\nFAILED_DEVICE_ID=$(heketi-cli topology info --json | jq \n.clusters[] | select(.id==\\\n$FIRST_CNS_CLUSTER\\\n) | .nodes[] | select(.hostnames.manage[0] == \\\nnode-6.lab\\\n) | .devices \n | jq -r \n.[] | select (.bricks[0].path ==\\\n$BRICK_DIR\\\n) | .id\n)\n\n\n\n\n\n Check the device ID that you have selected:\n\n\necho $FAILED_DEVICE_ID\n\n\n\n\n\nLet\ns assume this device on \nnode-6.lab\n has failed and needs to be replaced.\n\n\nIn such a case you\nll take the device\ns ID and go through the following steps:\n\n\n First, disable the device in heketi\n\n\nheketi-cli device disable $FAILED_DEVICE_ID\n\n\n\n\n\nThis will take the device offline and exclude it from future volume creation requests.\n\n\n Now remove the device in heketi\n\n\nheketi-cli device remove $FAILED_DEVICE_ID\n\n\n\n\n\nYou will notice this command takes a while.\n\nThat\ns because it will trigger a brick-replacement in GlusterFS. The command will block and heketi in the background will transparently create new bricks for each brick on the device to be deleted. The replacement operation will be conducted with the new bricks replacing all bricks on the device to be deleted. During this time the data remains accessible.\n\n\nThe new bricks, if possible, will automatically be created in zones different from the remaining bricks to maintain equal balancing and cross-zone availability.\n\n\n Finally, you are now able to delete the device in heketi entirely\n\n\nheketi-cli device delete $FAILED_DEVICE_ID\n\n\n\n\n\n Check again the volumes topology directly from GlusterFS\n\n\noc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME\n\n\n\n\n\nYou will notice that the brick from \nnode-6.lab\n is now a different mount path, because it was backed by a new device.\n\n\n Use the following to programmatically determine the new device heketi used to replace the one you just deleted:\n\n\nNEW_BRICK_DIR=$(echo -n $(oc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME | grep $NODE_IP_SIX) | cut -d \n:\n -f 3 | tr -d $\n\\r\n )\nNEW_DEVICE=$(heketi-cli topology info --json | jq \n.clusters[] | select(.id==\\\n$FIRST_CNS_CLUSTER\\\n) | .nodes[] | select(.hostnames.manage[0] == \\\nnode-6.lab\\\n) | .devices \n | jq -r \n.[] | select (.bricks[0].path ==\\\n$NEW_BRICK_DIR\\\n) | .name\n)\n\necho $NEW_DEVICE\n\n\n\n\n\nIf you cross-check again the new bricks mount path with the heketi topology you will see it\ns indeed coming from a different device. The remaining device in \nnode-6.lab\n, in this case \n/dev/xvdc\n\n\n\n\nTip\n\n\nDevice removal while maintaining volume health is possible in heketi as well. Simply delete all devices of the node in question as discussed above. Then the device can be deleted from heketi with \nheketi-cli device delete \ndevice-uuid", 
            "title": "Module 4 - Cluster Operations"
        }, 
        {
            "location": "/module-4-cluster-ops/#running-multiple-storage-pools", 
            "text": "In the previous modules a single GlusterFS cluster was used to supply  PersistentVolumes  to applications. CNS allows for multiple clusters to run in a single OpenShift deployment, controlled by a central  heketi  API:  There are several use cases for this:    Provide data isolation between clusters of different tenants    Provide multiple performance tiers of CNS, i.e. HDD-based vs. SSD-based    Run OpenShift across large geo-graphical distances with a CNS cluster per region whereas otherwise latency prohibits synchronous data replication in  a stretched setup    !!! Note: \n    The procedures to add an additional CNS cluster to an existing setup is not yet supported by  openshift-ansible .  Because we cannot use  openshift-ansible  as of today we need to run a couple of steps manually that would otherwise be automated.  To deploy a second CNS cluster, aka GlusterFS pool, follow these steps:   Log in as  operator  to namespace  app-storage  oc login -u operator -n app-storage  Your deployment has 6 OpenShift Application Nodes in total,  node-1 ,  node-2  and  node-3  currently setup running CNS. We will now set up a  second CNS cluster  using  node-4 ,  node-5  and  node-6 .  First we need to make sure the firewall on those systems is updated. Without  openshift-ansible  automating CNS deployment the ports necessary for running GlusterFS are not yet opened.   First, create a file called  configure-firewall.yml  and copy paste the following contents:  configure-firewall.yml:  ---  -   hosts : \n     -   node-4.lab \n     -   node-5.lab \n     -   node-6.lab \n\n   tasks : \n\n     -   name :   insert iptables rules required for GlusterFS \n       blockinfile : \n         dest :   /etc/sysconfig/iptables \n         block :   | \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24007 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 24008 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2222 -j ACCEPT \n           -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m multiport --dports 49152:49664 -j ACCEPT \n         insertbefore :   ^COMMIT \n\n     -   name :   reload iptables \n       systemd : \n         name :   iptables \n         state :   reloaded  ...    Run this small Ansible playbook to apply and reload the firewall configuration on all 3 nodes conveniently:  ansible-playbook configure-firewall.yml  The playbook should complete successfully:  PLAY [node-4.lab,node-5.lab,node-6.lab] ******************************************************************************************\n\nTASK [Gathering Facts] ***********************************************************************************************************\nSunday 24 September 2017  14:02:50 +0000 (0:00:00.056)       0:00:00.056 ******\nok: [node-5.lab]\nok: [node-6.lab]\nok: [node-4.lab]\n\nTASK [insert iptables rules required for GlusterFS] ******************************************************************************\nSunday 24 September 2017  14:02:51 +0000 (0:00:00.859)       0:00:00.916 ******\nchanged: [node-4.lab]\nchanged: [node-5.lab]\nchanged: [node-6.lab]\n\nTASK [reload iptables] ***********************************************************************************************************\nSunday 24 September 2017  14:02:51 +0000 (0:00:00.268)       0:00:01.184 ******\nchanged: [node-6.lab]\nchanged: [node-5.lab]\nchanged: [node-4.lab]\n\nPLAY RECAP ***********************************************************************************************************************\nnode-4.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-5.lab                 : ok=3    changed=2    unreachable=0    failed=0\nnode-6.lab                 : ok=3    changed=2    unreachable=0    failed=0\n\nSunday 24 September 2017  14:02:52 +0000 (0:00:00.334)       0:00:01.519 ******\n===============================================================================\nGathering Facts --------------------------------------------------------- 0.86s\nreload iptables --------------------------------------------------------- 0.34s\ninsert iptables rules required for GlusterFS ---------------------------- 0.27s   Next, we need to apply additional labels to the remaining 3 OpenShift Nodes:  oc label node/node-4.lab glusterfs=storage-host\noc label node/node-5.lab glusterfs=storage-host\noc label node/node-6.lab glusterfs=storage-host  The label will be used to control GlusterFS pod placement and availability. They are part of a  DaemonSet  definition that is looking for hosts with this particular label.   Wait for all pods to show  1/1  in the  READY  column:   oc get pods -o wide -n app-storage  You can also watch the additional GlusterFS pods deploy in the OpenShift UI, while being logged in as  operator  in project  app-storage , select  Applications  from the left menu and then  Pods :   !!! Note: \n    It may take up to 3 minutes for the GlusterFS pods to transition into  READY  state.   When done, on the CLI display all GlusterFS pods alongside with the name of the container host they are running on:  oc get pods -o wide -n app-storage -l glusterfs=storage-pod  You will see that now also app nodes  node-4 ,  node-5  and  node-6  run GlusterFS pods, although they are unitialized and not yet ready to use by CNS yet.  For manual bulk import of new nodes like this, a JSON topology file is used which includes the existing cluster as well as the new, second cluster with a separate set of nodes.   Create a new file named  2-clusters-topology.json  with the content below (use copy paste):  2-clusters-topology.json:  { \n     clusters :   [ \n         { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-1.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.201 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-2.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.202 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-3.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.203 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ]           },           { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-4.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.204 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-5.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.205 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-6.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.206 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ] \n         } \n     ]  }   The file contains the same content as the dynamically generated JSON structure  openshift-ansible  used, but with a second cluster specification (beginning at the highlighted line).  When loading this topology to  heketi , it will recognize the existing cluster (leaving it unchanged) and start creating the new one, with the same bootstrapping process used to initialize the first cluster. \nThat is: the  glusterd  process running in the pods will form a new 3-node cluster and the supplied block storage device  /dev/xvdc  will be formatted.   Prepare the heketi CLI tool like previously in  Module 2 .  HEKETI_POD=$(oc get pods -l glusterfs=heketi-storage-pod -n app-storage -o jsonpath= {.items[0].metadata.name} )\nexport HEKETI_CLI_SERVER=http://$(oc get route/heketi-storage -o jsonpath= {.spec.host} )\nexport HEKETI_CLI_USER=admin\nexport HEKETI_CLI_KEY=$(oc get pod/$HEKETI_POD -o jsonpath= {.spec.containers[0].env[?(@.name== HEKETI_ADMIN_KEY )].value} )   Verify there is currently only a single cluster known to heketi  heketi-cli cluster list  Example output:  Clusters:\nfb67f97166c58f161b85201e1fd9b8ed  Your ID will be different since it s auto-generated.   Save your specific ID of the first cluster with this shell command (and the versatile  jq  json parser) into an environment variable:  FIRST_CNS_CLUSTER=$(heketi-cli cluster list --json | jq -r  .clusters[0] )   Important  Do not skip above step. The value in the environment variable  FIRST_CNS_CLUSTER  is required later in this module.    Load the new topology with the heketi client  heketi-cli topology load --json=2-clusters-topology.json  You should see output similar to the following:  Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\nFound device /dev/xvdc\nCreating cluster ... ID: 46b205a4298c625c4bca2206b7a82dd3\nCreating node node-4.lab ... ID: 604d2eb15a5ca510ff3fc5ecf912d3c0\nAdding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 538b860406870288af23af0fbc2cd27f\nAdding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 7736bd0cb6a84540860303a6479cacb2\nAdding device /dev/xvdc ... OK  As indicated from above output a new cluster got created.   List all clusters:  heketi-cli cluster list  You should see a second cluster in the list:  Clusters:\n46b205a4298c625c4bca2206b7a82dd3\nfb67f97166c58f161b85201e1fd9b8ed  The second cluster, in this example with the ID  46b205a4298c625c4bca2206b7a82dd3 , is an entirely independent GlusterFS deployment. The exact value will be different in your environment.  heketi  is now able to differentiate between the clusters with storage provisioning requests when their UUID is specified.   Save the UUID of the second CNS cluster in an environment variable as follows for easy copy paste later:  SECOND_CNS_CLUSTER=$(heketi-cli cluster list --json | jq -r  .clusters[] | select(contains(\\ $FIRST_CNS_CLUSTER\\ ) | not) )  Now we have two independent GlusterFS clusters managed by the same heketi instance:      Nodes  Cluster UUID      First Cluster  node-1, node-2, node-3  fb67f97166c58f161b85201e1fd9b8ed    Second Cluster  node-4, node-5, node-6  46b205a4298c625c4bca2206b7a82dd3      Query the updated topology:  heketi-cli topology info  Abbreviated output:  Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n\n    Volumes:\n\n    Nodes:\n\n      Node Id: 538b860406870288af23af0fbc2cd27f\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 2\n      Management Hostname: node-5.lab\n      Storage Hostname: 10.0.3.105\n      Devices:\n        Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 1\n      Management Hostname: node-4.lab\n      Storage Hostname: 10.0.2.104\n      Devices:\n        Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\n      Node Id: 7736bd0cb6a84540860303a6479cacb2\n      State: online\n      Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n      Zone: 3\n      Management Hostname: node-6.lab\n      Storage Hostname: 10.0.4.106\n      Devices:\n        Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499\n            Bricks:\n\nCluster Id: fb67f97166c58f161b85201e1fd9b8ed\n\n[...output omitted for brevity...]  heketi formed an new, independent 3-node GlusterFS cluster on those nodes.   Check running GlusterFS pods  oc get pods -o wide -l glusterfs=storage-pod  From the output you can spot the pod names running on the new cluster s nodes:  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE glusterfs-1nvtj   1/1       Running   0          23m       10.0.4.206   node-6.lab glusterfs-5gvw8   1/1       Running   0          24m       10.0.2.204   node-4.lab glusterfs-5rc2g   1/1       Running   0          4h        10.0.2.201   node-1.lab glusterfs-b4wg1   1/1       Running   0          24m       10.0.3.205   node-5.lab glusterfs-jbvdk   1/1       Running   0          4h        10.0.3.202   node-2.lab\nglusterfs-rchtr   1/1       Running   0          4h        10.0.4.203   node-3.lab  !!! Note: \n    Again note that the pod names are dynamically generated and will be different. Look the FQDN of your hosts to determine one of new cluster s pods.   Let s run the  gluster peer status  command in the GlusterFS pod running on the node  node-6.lab :  POD_NUMBER_SIX=$(oc get pods -o jsonpath= {.items[?(@.status.hostIP== 10.0.4.206 )].metadata.name} )\noc rsh $POD_NUMBER_SIX gluster peer status  As expected this node only has 2 peers, evidence that it s running in it s own GlusterFS pool separate from the first cluster in deployed in Module 2.  Number of Peers: 2\n\nHostname: node-5.lab\nUuid: 0db9b5d0-7fa8-4d2f-8b9e-6664faf34606\nState: Peer in Cluster (Connected)\nOther names:\n10.0.3.205\n\nHostname: node-4.lab\nUuid: 695b661d-2a55-4f94-b22e-40a9db79c01a\nState: Peer in Cluster (Connected)  Before you can use the second cluster two tasks have to be accomplished so we can use both distinctively:    The  StorageClass  for the first cluster has to be updated to point the first cluster s UUID,    A second  StorageClass  for the second cluster has to be created, pointing to the same heketi API     Why do we need to update the first  StorageClass ?  When no cluster UUID is specified, heketi serves volume creation requests from any cluster currently registered to it. That would be two now. \nIn order to request a volume from specific cluster you have to supply the cluster s UUID to heketi. This is done via a parameter in the  StorageClass . The first  StorageClass  has no UUID specified so far because  openshift-ansible  did not create it.   Unfortunately you cannot  oc patch  a  StorageClass  parameters in OpenShift. So we have to delete it and re-create it. Don t worry - existing  PVCs  will remain untouched.  To simplify our work, instead of typing JSON/YAML, we will just export the current  StorageClass  definition JSON and manipulate it using  jq  and some clever JSON queries to put the additional  clusterid  parameter in the right place.   To do that, run the following command via copy paste:  oc get storageclass/glusterfs-storage -o json \\\n| jq  .parameters=(.parameters + {\\ clusterid\\ : \\ $FIRST_CNS_CLUSTER\\ })    glusterfs-storage-fast.json  This will result in a file named  glusterfs-storage-fast.json  looking like the following:  glusterfs-storage-fast.json:  { \n   apiVersion :   storage.k8s.io/v1 , \n   kind :   StorageClass , \n   metadata :   { \n     creationTimestamp :   2017-09-24T12:45:24Z , \n     name :   glusterfs-storage , \n     resourceVersion :   2697 , \n     selfLink :   /apis/storage.k8s.io/v1/storageclasses/glusterfs-storage , \n     uid :   3c107010-a126-11e7-b0a5-025fcde0880f \n   }, \n   parameters :   { \n     resturl :   http://heketi-storage-app-storage.cloudapps.34.252.58.209.nip.io , \n     restuser :   admin , \n     secretName :   heketi-storage-admin-secret , \n     secretNamespace :   app-storage ,       clusterid :   fb67f97166c58f161b85201e1fd9b8ed     }, \n   provisioner :   kubernetes.io/glusterfs  }   Note the additional  clusterid  parameter highlighted. It s the first cluster s UUID as known by heketi. The exact values will be different in your environment. The rest of the definition remains the same.   Delete the existing  StorageClass  definition in OpenShift  oc delete storageclass/glusterfs-storage   Add the  StorageClass  again:  oc create -f glusterfs-storage-fast.json  Step 1 complete. The existing  StorageClass  is  updated .  PVC  using the  StorageClass   glusterfs-storage  will now specifically get served by the first CNS cluster, and only the first cluster.  To relieve you from manually editing JSON files, we will again use some  jq  magic to generate the correct JSON structure for our second  StorageClass , this time using the second CNS cluster s ID and a different name  glusterfs-storage-slow   Run the following command:  oc get storageclass/glusterfs-storage -o json \\\n| jq  .parameters=(.parameters + {\\ clusterid\\ : \\ $SECOND_CNS_CLUSTER\\ })  \\\n| jq  .metadata.name =  glusterfs-storage-slow    glusterfs-storage-slow.json  This creates a file called  glusterfs-storage-slow.json , looking similar to the below:  glusterfs-storage-slow.json:  { \n   apiVersion :   storage.k8s.io/v1 , \n   kind :   StorageClass , \n   metadata :   { \n     creationTimestamp :   2017-09-24T15:12:34Z ,       name :   glusterfs-storage-slow ,       resourceVersion :   12722 , \n     selfLink :   /apis/storage.k8s.io/v1/storageclasses/glusterfs-storage , \n     uid :   cb16946d-a13a-11e7-b0a5-025fcde0880f \n   }, \n   parameters :   {       clusterid :   46b205a4298c625c4bca2206b7a82dd3 ,       resturl :   http://heketi-storage-app-storage.cloudapps.34.252.58.209.nip.io , \n     restuser :   admin , \n     secretName :   heketi-storage-admin-secret , \n     secretNamespace :   app-storage \n   }, \n   provisioner :   kubernetes.io/glusterfs  }   Again note the  clusterid  in the  parameters  section referencing the second cluster s UUID will as well as the update.   Add the new  StorageClass :  oc create -f glusterfs-storage-slow.json  This creates the  StorageClass  named  glusterfs-storage-slow  and because we copied the settings from the first one it s now also set as system-wide default (yes, OpenShift allows you to do that).   Use the  oc patch  command to fix this:  oc patch storageclass glusterfs-storage-slow \\\n-p  { metadata : { annotations : { storageclass.kubernetes.io/is-default-class :  false }}}    Display all  StorageClass  objects to verify:  oc get storageclass  That s it. You now have 2  StorageClass  definitions, one for each CNS cluster, managed by the same  heketi  instance.  NAME                          TYPE\nglusterfs-storage (default)   kubernetes.io/glusterfs\nglusterfs-storage-slow        kubernetes.io/glusterfs  Let s verify both  StorageClasses  are working as expected:   Create the following two files containing PVCs issued against either of both GlusterFS pools via their respective  StorageClass :  cns-pvc-fast.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-fast-container-storage  spec : \n   accessModes : \n   -   ReadWriteMany \n   resources : \n     requests : \n       storage :   5Gi \n   storageClassName :   glusterfs-storage   cns-pvc-slow.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-slow-container-storage  spec : \n   accessModes : \n   -   ReadWriteMany \n   resources : \n     requests : \n       storage :   7Gi \n   storageClassName :   glusterfs-storage-slow    Create both PVCs:  oc create -f cns-pvc-fast.yml\noc create -f cns-pvc-slow.yml  Check their provisioning state after a few seconds:  oc get pvc  They should both be in bound state after a couple of seconds:  NAME                        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   STORAGECLASS             AGE\nmy-fast-container-storage   Bound     pvc-bfbf3f72-a13d-11e7-b0a5-025fcde0880f   5Gi        RWX           glusterfs-storage        6s\nmy-slow-container-storage   Bound     pvc-c045c082-a13d-11e7-b0a5-025fcde0880f   7Gi        RWX           glusterfs-storage-slow   6s   If you check again the GlusterFS pod on on  node-6.lab  running as part of the second cluster  oc rsh $POD_NUMBER_SIX gluster vol list  you will see a new volume has been created  vol_755b4434cf9062104123e0d9919dd800  The other volume has been created on the first cluster.   If you were to check GlusterFS on  node-1.lab  POD_NUMBER_ONE=$(oc get pods -o jsonpath= {.items[?(@.status.hostIP== 10.0.2.201 )].metadata.name} )\noc rsh $POD_NUMBER_ONE gluster vol list  you will also see a new volume has been created, alongside the volumes from the previous exercises and the  heketidbstorage  volume:  heketidbstorage\n[...output omitted... ]\nvol_8eb957320215fe8801748b239d524808  If you now compare the  PV  objects that have been created:     the first PV using  StorageClass  glusterfs-storage:  FAST_PV=$(oc get pvc/my-fast-container-storage -o jsonpath= {.spec.volumeName} )\noc get pv/$FAST_PV -o jsonpath= {.spec.glusterfs.path}      and the second PV using  StorageClass  glusterfs-storage-slow:  SLOW_PV=$(oc get pvc/my-slow-container-storage -o jsonpath= {.spec.volumeName} )\noc get pv/$SLOW_PV -o jsonpath= {.spec.glusterfs.path}    you will notice that they match the volumes found in the CNS clusters from within their pods respectively.  This is how you use multiple, parallel GlusterFS pools/clusters on a single OpenShift cluster with a single heketi instance. Whereas the first pool is created with  openshift-ansible  subsequent pools/cluster are created with the  heketi-cli  client.  Clean up the PVCs and the second  StorageClass  in preparation for the next section.   Delete both PVCs (and therefore their volume)  oc delete pvc/my-fast-container-storage\noc delete pvc/my-slow-container-storage   Delete the second  StorageClass  oc delete storageclass/glusterfs-storage-slow", 
            "title": "Running multiple storage pools"
        }, 
        {
            "location": "/module-4-cluster-ops/#deleting-a-cns-cluster", 
            "text": "Since we want to re-use  node-4 ,  node-5  and  node-6  for the next section we need to delete it the GlusterFS pools on top of them first.  This is a process that involves multiple steps of manipulating the heketi topology with the  heketi-cli  client.   Make sure the client is still properly configured via environment variables:  echo $HEKETI_CLI_SERVER\necho $HEKETI_CLI_USER\necho $HEKETI_CLI_KEY   We also require the environment variables storing the UUIDs of both CNS clusters:  echo $FIRST_CNS_CLUSTER\necho $SECOND_CNS_CLUSTER   First display the entire system topology as it is known to heketi:  heketi-cli topology info  You will get detailled infos about both clusters. \nThe portions of interest for the second clusters we are about to delete are highlighted:  Cluster Id: 46b205a4298c625c4bca2206b7a82dd3 \n    Volumes:\n\n    Nodes:         Node Id: 538b860406870288af23af0fbc2cd27f         State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 2\n        Management Hostname: node-5.lab\n        Storage Hostname: 10.0.3.105\n        Devices:             Id:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499                 Bricks:         Node Id: 604d2eb15a5ca510ff3fc5ecf912d3c0         State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 1\n        Management Hostname: node-4.lab\n        Storage Hostname: 10.0.2.104\n        Devices:             Id:09a25a114c53d7669235b368efd2f8d1   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499                 Bricks:         Node Id: 7736bd0cb6a84540860303a6479cacb2         State: online\n        Cluster Id: 46b205a4298c625c4bca2206b7a82dd3\n        Zone: 3\n        Management Hostname: node-6.lab\n        Storage Hostname: 10.0.4.106\n        Devices:             Id:cccadb2b54dccd99f698d2ae137a22ff   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499             Bricks:  The hierachical dependencies in this topology works as follows: Clusters   Nodes   Devices. \nAssuming there are no volumes present these need to be deleted in reverse order.  To make navigating this process easier and avoid mangling with anonymous UUID values we will use some simple scripting.   This is how you get all nodes IDs of the second cluster:  heketi-cli cluster info $SECOND_CNS_CLUSTER --json | jq -r  .nodes[]   For example:  538b860406870288af23af0fbc2cd27f\n604d2eb15a5ca510ff3fc5ecf912d3c0\n7736bd0cb6a84540860303a6479cacb2   Let s put this in a variable so we can iterate over it:  NODES=$(heketi-cli cluster info $SECOND_CNS_CLUSTER --json | jq -r  .nodes[] )   This is how you get information about a node  heketi-cli node info ${NODES[0]}  Node Id: 538b860406870288af23af0fbc2cd27f\nState: online\nCluster Id: 38cba86da51146a0ef9747383bd44476\nZone: 2\nManagement Hostname: node-5.lab\nStorage Hostname: 10.0.3.205\nDevices:\nId:e481d022cea9bfb11e8a86c0dd8d3499   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499   Let s iterate over this  NODES  array and extract all device IDs:  for node in ${NODES} ; do heketi-cli node info $node --json | jq -r  .devices[].id  ; done   for example:  e481d022cea9bfb11e8a86c0dd8d3499\n09a25a114c53d7669235b368efd2f8d1\ncccadb2b54dccd99f698d2ae137a22ff   Let s put this in a a variable too, so we can easily iterate over that:  DEVICES=$(for node in ${NODES} ; do heketi-cli node info $node --json | jq -r  .devices[].id  ; done)   Let s loop over this  DEVICES  array and delete the device by it s ID in heketi:  for device in $DEVICES ; do heketi-cli device delete $device ; done  Example output:  Device 538b860406870288af23af0fbc2cd27f deleted\nDevice 604d2eb15a5ca510ff3fc5ecf912d3c0 deleted\nDevice 7736bd0cb6a84540860303a6479cacb2 deleted   Since the nodes have no devices anymore we can delete those as well (you can t delete a node with a device still attached):  for node in ${NODES} ; do heketi-cli node delete $node ; done  Example output:  Node 4ff85abd2674c89e79c1f7c7f8ee1be4 deleted\nNode ed9c045f10a5c1f9057d07880543a461 deleted\nNode fd6ddca52c788e2d764fada1f4da2ce4 deleted   Finally, without any nodes in the second cluster, you can also delete it (it won t work if there are nodes left):  heketi-cli cluster delete $SECOND_CNS_CLUSTER   Confirm the cluster is gone:  heketi-cli cluster list   Verify the new topology known by heketi now only containing a single cluster.  heketi-cli topology info  This deleted all heketi database entries about the cluster. However the GlusterFS pods are still running, since they are controlled directly by OpenShift and the  DaemonSet .  They can be stopped by removing the labels OpenShift uses to determine GlusterFS pod placement for CNS.   Remove the labels from the last 3 OpenShift nodes like so:  oc label node/node-4.lab glusterfs-\noc label node/node-5.lab glusterfs-\noc label node/node-6.lab glusterfs-  Contrary to the output of these commands the label  glusterfs  is actually removed (indicated by the minus sign).   Verify that all GlusterFS pods running on  node-4 ,  node-5  and  node-6  are indeed terminated:  oc get pods -o wide -n app-storage -l glusterfs=storage-pod  !!! Note: \n    It can take up to 2 minutes for the pods to terminate.  You should be back down to 3 GlusterFS pods, e.g.  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.201   node-1.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.202   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.203   node-3.lab", 
            "title": "Deleting a CNS cluster"
        }, 
        {
            "location": "/module-4-cluster-ops/#expanding-a-glusterfs-pool", 
            "text": "Instead of creating additional GlusterFS pools in CNS on OpenShift it is also possible to expand existing pools. This is useful the increase capacity, performance and resiliency of the storage system.  This works similar to creating additional pools, with bulk-import via the topology file. Only this time with nodes added to the existing cluster structure in JSON.  Since manipulating JSON can be error-prone create a new file called  expanded-cluster.json  with contents as below:  expanded-cluster.json:  { \n     clusters :   [ \n         { \n             nodes :   [ \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-1.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.201 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-2.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.202 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-3.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.203 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-4.lab \n                             ], \n                             storage :   [ \n                                 10.0.2.204 \n                             ] \n                         }, \n                         zone :   1 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-5.lab \n                             ], \n                             storage :   [ \n                                 10.0.3.205 \n                             ] \n                         }, \n                         zone :   2 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 }, \n                 { \n                     node :   { \n                         hostnames :   { \n                             manage :   [ \n                                 node-6.lab \n                             ], \n                             storage :   [ \n                                 10.0.4.206 \n                             ] \n                         }, \n                         zone :   3 \n                     }, \n                     devices :   [ \n                         /dev/xvdc \n                     ] \n                 } \n             ] \n         } \n     ]  }   The difference between this file to the  2-clusters-topology.json  is that we now have 6 nodes in a single cluster instead of 2 clusters, with 3 nodes each.   Again, apply the expected labels to the remaining 3 OpenShift Nodes:  oc label node/node-4.lab glusterfs=storage-host\noc label node/node-5.lab glusterfs=storage-host\noc label node/node-6.lab glusterfs=storage-host   Wait for all pods to show  1/1  in the  READY  column:   oc get pods -o wide -n app-storage -l glusterfs=storage-pod  !!! Note: \n    It may take up to 3 minutes for the GlusterFS pods to transition into  READY  state.  This confirms all GlusterFS pods are ready to receive remote commands:  NAME              READY     STATUS    RESTARTS   AGE       IP           NODE\nglusterfs-0lr75   1/1       Running   0          4m        10.0.4.106   node-6.lab\nglusterfs-1dxz3   1/1       Running   0          4m        10.0.3.105   node-5.lab\nglusterfs-5rc2g   1/1       Running   0          5h        10.0.2.101   node-1.lab\nglusterfs-8nrn0   1/1       Running   0          4m        10.0.2.104   node-4.lab\nglusterfs-jbvdk   1/1       Running   0          5h        10.0.3.102   node-2.lab\nglusterfs-rchtr   1/1       Running   0          5h        10.0.4.103   node-3.lab   Ensure the environment variables for operating  heketi-cli  are still in place:  echo $HEKETI_CLI_SERVER\necho $HEKETI_CLI_USER\necho $HEKETI_CLI_KEY   Now load the new topology:  heketi-cli topology load --json=expanded-cluster.json  The output indicated that the existing cluster was expanded, rather than creating a new one:  Found node node-1.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-2.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nFound node node-3.lab on cluster fb67f97166c58f161b85201e1fd9b8ed\n    Found device /dev/xvdc\nCreating node node-4.lab ... ID: 544158e53934a3d351b874b7d915e8d4\n    Adding device /dev/xvdc ... OK\nCreating node node-5.lab ... ID: 645b6edd4044cb1dd828f728d1c3eb81\n    Adding device /dev/xvdc ... OK\nCreating node node-6.lab ... ID: 3f39ebf3c8c82531a7ba447135742776\n    Adding device /dev/xvdc ... OK   Verify the their new peers are now part of the first CNS cluster:  POD_NUMBER_ONE=$(oc get pods -o jsonpath= {.items[?(@.status.hostIP== 10.0.2.201 )].metadata.name} )\noc rsh $POD_NUMBER_ONE gluster peer status  You should now have a GlusterFS consisting of 6 nodes:  Number of Peers: 5\n\nHostname: 10.0.3.202\nUuid: c6a6d571-fd9b-4bd8-aade-e480ec2f8eed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.203\nUuid: 46044d06-a928-49c6-8427-a7ab37268fed\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.2.204\nUuid: 62abb8b9-7a68-4658-ac84-8098a1460703\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.3.205\nUuid: 5b44b6ea-6fb5-4ea9-a6f7-328179dc6dda\nState: Peer in Cluster (Connected)\n\nHostname: 10.0.4.206\nUuid: ed39ecf7-1f5c-4934-a89d-ee1dda9a8f98\nState: Peer in Cluster (Connected)  With this you have expanded the existing pool. New PVCs will start to use capacity from the additional nodes.   Important  In this lab, with this expansion, you now have a GlusterFS pool with mixed media types (both size and speed). It is recommended to have the same media type per pool. \nIf you like to offer multiple media types for CNS in OpenShift, use separate pools and separate  StorageClass  objects as described in the  previous section .", 
            "title": "Expanding a GlusterFS pool"
        }, 
        {
            "location": "/module-4-cluster-ops/#adding-a-device-to-a-node", 
            "text": "Instead of adding entirely new nodes you can also add new storage devices for CNS to use on existing nodes.  It is again possible to do this by loading an updated topology file. Alternatively to bulk-loading via JSON you are also able to do this directly with the  heketi-cli  utility. This also applies to the previous sections in this module.  For this purpose  node-3.lab  has an additional, so far unused block device  /dev/xvdd .   To use the heketi-cli make sure the environment variables are still set:  echo $HEKETI_CLI_SERVER\necho $HEKETI_CLI_USER\necho $HEKETI_CLI_KEY   Determine the UUUI heketi uses to identify  node-6.lab  in it s database and save it in an environment variable:  NODE_ID_SIX=$(heketi-cli topology info --json | jq -r  .clusters[] | select(.id==\\ $FIRST_CNS_CLUSTER\\ ) | .nodes[] | select(.hostnames.manage[0] == \\ node-6.lab\\ ) | .id )   Query the node s available devices:  heketi-cli node info $NODE_ID_SIX  The node has one device available:  Node Id: 3f39ebf3c8c82531a7ba447135742776\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.206\nDevices:\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):509     Used (GiB):0       Free (GiB):509   Add the device  /dev/xvdd  to the node using the UUID noted earlier.  heketi-cli device add --node=$NODE_ID_SIX --name=/dev/xvdd  The device is registered in heketi s database.  Device added successfully   Query the node s available devices again and you ll see a second device.  heketi-cli node info $NODE_ID_SIX  That node now has 2 devices. The new device will be used by subsequent  PVC  being served by this cluster.  Node Id: 3f39ebf3c8c82531a7ba447135742776\nState: online\nCluster Id: eb909a08c8e8fd0bf80499fbbb8a8545\nZone: 3\nManagement Hostname: node-6.lab\nStorage Hostname: 10.0.4.206\nDevices:\nId:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):509     Used (GiB):0       Free (GiB):509\nId:cc594d7f5ce59ab2a991c70572a0852f   Name:/dev/xvdc           State:online    Size (GiB):499     Used (GiB):0       Free (GiB):499", 
            "title": "Adding a device to a node"
        }, 
        {
            "location": "/module-4-cluster-ops/#replacing-a-failed-device", 
            "text": "One of heketi s advantages is the automation of otherwise tedious manual tasks, like replacing a faulty brick in GlusterFS to repair degraded volumes. \nWe will simulate this use case now.   Make sure you are  operator  in OpenShift and using the project  my-test-project  oc login -u operator -n my-test-project   Create the file  cns-large-pvc.yml  with content below:  cns-large-pvc.yml:  kind :   PersistentVolumeClaim  apiVersion :   v1  metadata : \n   name :   my-large-container-store  spec : \n   accessModes : \n   -   ReadWriteMany \n   resources : \n     requests : \n       storage :   200Gi \n   storageClassName :   glusterfs-storage    Create this request for a large volume:  oc create -f cns-large-pvc.yml  The requested capacity in this  PVC  is larger than any single brick on nodes  node-1.lab ,  node-2.lab  and  node-3.lab  so it will be created from the bricks of the other 3 nodes which have larger bricks (500 GiB).  Where are now going to determine a  PVCs  physical backing device on CNS. This is done with the following relationships between the various entities of GlusterFS, heketi and OpenShift in mind:  PVC -  PV -  heketi volume -  GlusterFS volume -  GlusterFS brick -  Physical Device   First, get the  PV  oc describe pvc/my-large-container-store  Note the  PVs  name:      Name:       my-large-container-store\n    Namespace:  my-test-project\n    StorageClass:   app-storage\n    Status:     Bound     Volume:     pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8     Labels:      none \n    Capacity:   200Gi\n    Access Modes:   RWO\n    No events.   Get the GlusterFS volume name of this PV,  use your PVs name here , e.g.  oc describe pv/pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8  The GlusterFS volume name as it used by GlusterFS:      Name:       pvc-078a1698-4f5b-11e7-ac96-1221f6b873f8\n    Labels:      none \n    StorageClass:   app-storage\n    Status:     Bound\n    Claim:      my-test-project/my-large-container-store\n    Reclaim Policy: Delete\n    Access Modes:   RWO\n    Capacity:   200Gi\n    Message:\n    Source:\n        Type:       Glusterfs (a Glusterfs mount on the host that shares a pod s lifetime)\n        EndpointsName:  glusterfs-dynamic-my-large-container-store         Path:       vol_3ff9946ddafaabe9745f184e4235d4e1         ReadOnly:       false\n    No events.  Let s programmatically determine and safe the relevant information so you don t have to type all this stuff. \nWe need: the  PV  name, the respective GlusterFS volume s, the name of the GlusterFS pod on the node  node-6.lab  and that node s id in  heketi  and IP address in environment variables:  LARGE_PV=$(oc get pvc/my-large-container-store -o jsonpath= {.spec.volumeName} )\nLARGE_GLUSTER_VOLUME=$(oc get pv/$LARGE_PV -o jsonpath= {.spec.glusterfs.path} )\nPOD_NUMBER_SIX=$(oc get pods -n app-storage -o jsonpath= {.items[?(@.status.hostIP== 10.0.4.206 )].metadata.name} )\nNODE_ID_SIX=$(heketi-cli topology info --json | jq -r  .clusters[] | select(.id==\\ $FIRST_CNS_CLUSTER\\ ) | .nodes[] | select(.hostnames.manage[0] == \\ node-6.lab\\ ) | .id )\nNODE_IP_SIX=$(oc get pod/$POD_NUMBER_SIX -n app-storage -o jsonpath= {.status.hostIP} )\n\necho  LARGE_PV             = $LARGE_PV \necho  LARGE_GLUSTER_VOLUME = $LARGE_GLUSTER_VOLUME \necho  POD_NUMBER_SIX       = $POD_NUMBER_SIX \necho  NODE_ID_SIX          = $NODE_ID_SIX \necho  NODE_IP_SIX          = $NODE_IP_SIX    Change to the CNS namespace  oc project app-storage   Log on to one of the GlusterFS pods  oc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME  The output indicates this volume is indeed backed by, among others,  node-6.lab  (see highlighted line)  Volume Name: vol_3ff9946ddafaabe9745f184e4235d4e1\nType: Replicate\nVolume ID: 774ae26f-bd3f-4c06-990b-57012cc5974b\nStatus: Started\nSnapshot Count: 0\nNumber of Bricks: 1 x 3 = 3\nTransport-type: tcp\nBricks:\nBrick1: 10.0.3.205:/var/lib/heketi/mounts/vg_e1b93823a2906c6758aeec13930a0919/brick_b3d5867d2f86ac93fce6967128643f85/brick\nBrick2: 10.0.2.204:/var/lib/heketi/mounts/vg_3c3489a5779c1c840a82a26e0117a415/brick_6323bd816f17c8347b3a68e432501e96/brick Brick3: 10.0.4.206:/var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick Options Reconfigured:\ntransport.address-family: inet\nperformance.readdir-ahead: on\nnfs.disable: on   Safe the brick directory served by  node-6.lab  in an environment variable:  BRICK_DIR=$(echo -n $(oc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME | grep $NODE_IP_SIX) | cut -d  :  -f 3 | tr -d $ \\r  )\necho $BRICK_DIR   Using the full path of brick you can cross-check with heketi s topology on which device it is based on:  heketi-cli topology info | grep -B2 $BRICK_DIR  Among other data  grep  will show the physical backing device of this brick s mount path:  Id:62cbae7a3f6faac38a551a614419cca3   Name:/dev/xvdd           State:online    Size (GiB):499     Used (GiB):201     Free (GiB):298             Bricks:\n                Id:a6c92b6a07983e9b8386871f5b82497f   Size (GiB):200     Path: /var/lib/heketi/mounts/vg_62cbae7a3f6faac38a551a614419cca3/brick_a6c92b6a07983e9b8386871f5b82497f/brick  In this case it s  /dev/xvdd  of  node-6.lab .  !!! Note: \n    The device might be different for you. This is subject to heketi s dynamic scheduling.  We will now proceed to disable and delete this device. For that we have to find and use it s UUID in  heketi .  Safe the  heketi  device s ID from the brick on  node-6.lab  using the following definition of an environment variable, again by leveraging  jq  to parse the JSON output of  heketi topology info :  FAILED_DEVICE_ID=$(heketi-cli topology info --json | jq  .clusters[] | select(.id==\\ $FIRST_CNS_CLUSTER\\ ) | .nodes[] | select(.hostnames.manage[0] == \\ node-6.lab\\ ) | .devices   | jq -r  .[] | select (.bricks[0].path ==\\ $BRICK_DIR\\ ) | .id )   Check the device ID that you have selected:  echo $FAILED_DEVICE_ID  Let s assume this device on  node-6.lab  has failed and needs to be replaced.  In such a case you ll take the device s ID and go through the following steps:   First, disable the device in heketi  heketi-cli device disable $FAILED_DEVICE_ID  This will take the device offline and exclude it from future volume creation requests.   Now remove the device in heketi  heketi-cli device remove $FAILED_DEVICE_ID  You will notice this command takes a while. \nThat s because it will trigger a brick-replacement in GlusterFS. The command will block and heketi in the background will transparently create new bricks for each brick on the device to be deleted. The replacement operation will be conducted with the new bricks replacing all bricks on the device to be deleted. During this time the data remains accessible.  The new bricks, if possible, will automatically be created in zones different from the remaining bricks to maintain equal balancing and cross-zone availability.   Finally, you are now able to delete the device in heketi entirely  heketi-cli device delete $FAILED_DEVICE_ID   Check again the volumes topology directly from GlusterFS  oc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME  You will notice that the brick from  node-6.lab  is now a different mount path, because it was backed by a new device.   Use the following to programmatically determine the new device heketi used to replace the one you just deleted:  NEW_BRICK_DIR=$(echo -n $(oc rsh $POD_NUMBER_SIX gluster vol info $LARGE_GLUSTER_VOLUME | grep $NODE_IP_SIX) | cut -d  :  -f 3 | tr -d $ \\r  )\nNEW_DEVICE=$(heketi-cli topology info --json | jq  .clusters[] | select(.id==\\ $FIRST_CNS_CLUSTER\\ ) | .nodes[] | select(.hostnames.manage[0] == \\ node-6.lab\\ ) | .devices   | jq -r  .[] | select (.bricks[0].path ==\\ $NEW_BRICK_DIR\\ ) | .name )\n\necho $NEW_DEVICE  If you cross-check again the new bricks mount path with the heketi topology you will see it s indeed coming from a different device. The remaining device in  node-6.lab , in this case  /dev/xvdc   Tip  Device removal while maintaining volume health is possible in heketi as well. Simply delete all devices of the node in question as discussed above. Then the device can be deleted from heketi with  heketi-cli device delete  device-uuid", 
            "title": "Replacing a failed device"
        }, 
        {
            "location": "/module-5-cns-for-infra/", 
            "text": "Overview\n\n\nIn this module you will learn how to use Container-Native Storage to serve storage for OpenShift internal infrastructure.\n\nThis module does not have any pre-requisites.\n\n\n\n\nOpenShift Registry on CNS\n#\n\n\nThe Registry in OpenShift is used to store all images that result of Source-to-Image deployments as well as custom container images.\n\nIt runs as one or more containers in specific Infrastructure Nodes or Master Nodes in OpenShift.\n\n\nAs explorerd in Module 1, by default the registry uses a hosts local storage (\nemptyDir\n) which makes it prone to outages. To avoid such outages the storage needs to be \npersistent\n and \nshared\n in order to survive Registry pod restarts and scale-out.\n\n\nWhat we want to achieve is a setup like depicted in the following diagram:\n\n\n\n\nThis can be achieved with CNS simply by making the registry pods refer to a PVC in access mode \nRWX\n based on CNS.\n\nBefore OpenShift Container Platform 3.6 this had to be done manually on an existing CNS cluster.\n\n\nWith \nopenshift-ansible\n you now have this setup task automated. The playbooks that implement this will deploy a \nseparate CNS cluster\n, preferably on the \nInfrastructure Nodes\n, create a \nPVC\n and update the Registry \nDeploymentConfig\n to mount the associated \nPV\n which is where container images will then be stored.\n\n\n\n\nImportant\n\n\nThis method will be disruptive. All data stored in the registry so far will become unavailable.\n\nMigration scenarios exist but are beyond the scope of this lab.\n\n\n\n\n Review the \nopenshift-ansible\n inventory file in \n/etc/ansible/ocp-with-glusterfs-registry\n that has been prepared in your environment:\n\n\n/etc/ansible/ocp-with-glusterfs-registry:\n\n\n[OSEv3:children]\n\n\nmasters\n\n\nnodes\n\n\nglusterfs_registry\n\n\n\n\n[OSEv3:vars]\n\n\ndeployment_type\n=\nopenshift-enterprise\n\n\ncontainerized\n=\ntrue\n\n\nopenshift_image_tag\n=\nv3.6.173.0.21\n\n\nopenshift_master_identity_providers\n=\n[{\nname\n: \nhtpasswd\n, \nlogin\n: \ntrue\n, \nchallenge\n: \ntrue\n, \nkind\n: \nHTPasswdPasswordIdentityProvider\n, \nfilename\n: \n/etc/origin/master/htpasswd\n}]\n\n\nopenshift_master_htpasswd_users\n=\n{\ndeveloper\n: \n$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/\n,\noperator\n: \n$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/\n}\n\n\nopenshift_master_default_subdomain\n=\ncloudapps.52.59.170.248.nip.io\n\n\nopenshift_router_selector\n=\nrole=master\n\n\nopenshift_registry_selector\n=\nrole=infra\n\n\nopenshift_hosted_registry_storage_kind\n=\nglusterfs\n\n\nopenshift_storage_glusterfs_registry_storageclass\n=\ntrue\n\n\nopenshift_metrics_install_metrics\n=\nfalse\n\n\nopenshift_metrics_hawkular_hostname\n=\nhawkular-metrics.{{ openshift_master_default_subdomain }}\n\n\nopenshift_metrics_cassandra_storage_type\n=\npv\n\n\nopenshift_metrics_cassandra_pvc_size\n=\n10Gi\n\n\nopenshift_logging_install_logging\n=\nfalse\n\n\nopenshift_logging_es_pvc_size\n=\n10Gi\n\n\nopenshift_logging_es_pvc_dynamic\n=\ntrue\n\n\nopenshift_storage_glusterfs_image\n=\nrhgs3/rhgs-server-rhel7\n\n\nopenshift_storage_glusterfs_version\n=\n3.2.0-7\n\n\nopenshift_storage_glusterfs_heketi_image\n=\nrhgs3/rhgs-volmanager-rhel7\n\n\nopenshift_storage_glusterfs_heketi_version\n=\n3.2.0-11\n\n\nopenshift_storage_glusterfs_registry_namespace\n=\ninfra-storage\n\n\nopenshift_docker_additional_registries\n=\nmirror.lab:5555\n\n\nopenshift_docker_insecure_registries\n=\nmirror.lab:5555\n\n\noreg_url\n=\nmirror.lab:5555/openshift3/ose-${component}:${version}\n\n\nopenshift_examples_modify_imagestreams\n=\nfalse\n\n\nopenshift_disable_check\n=\ndisk_availability,memory_availability\n\n\n\n[masters]\n\n\nmaster.lab openshift_public_hostname\n=\n52.59.170.248.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=52.59.170.248\n\n\n\n[masters:vars]\n\n\nopenshift_schedulable\n=\ntrue\n\n\nopenshift_node_labels\n=\n{\nrole\n: \nmaster\n}\n\n\n\n[nodes]\n\n\nmaster.lab openshift_public_hostname\n=\n52.59.170.248.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=52.59.170.248\n\n\ninfra-1.lab openshift_hostname\n=\ninfra-1.lab openshift_ip=10.0.2.101 openshift_node_labels=\n{\nrole\n: \ninfra\n}\n\n\ninfra-2.lab openshift_hostname\n=\ninfra-2.lab openshift_ip=10.0.3.102 openshift_node_labels=\n{\nrole\n: \ninfra\n}\n\n\ninfra-3.lab openshift_hostname\n=\ninfra-3.lab openshift_ip=10.0.4.103 openshift_node_labels=\n{\nrole\n: \ninfra\n}\n\n\nnode-1.lab openshift_hostname\n=\nnode-1.lab openshift_ip=10.0.2.201 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-2.lab openshift_hostname\n=\nnode-2.lab openshift_ip=10.0.3.202 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-3.lab openshift_hostname\n=\nnode-3.lab openshift_ip=10.0.4.203 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-4.lab openshift_hostname\n=\nnode-4.lab openshift_ip=10.0.2.204 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-5.lab openshift_hostname\n=\nnode-5.lab openshift_ip=10.0.3.205 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\nnode-6.lab openshift_hostname\n=\nnode-6.lab openshift_ip=10.0.4.206 openshift_node_labels=\n{\nrole\n: \napp\n}\n\n\n\n[glusterfs_registry]\n\n\ninfra-1.lab glusterfs_ip\n=\n10.0.2.101 glusterfs_zone=1 glusterfs_devices=\n[ \n/dev/xvdc\n ]\n\n\ninfra-2.lab glusterfs_ip\n=\n10.0.3.102 glusterfs_zone=2 glusterfs_devices=\n[ \n/dev/xvdc\n ]\n\n\ninfra-3.lab glusterfs_ip\n=\n10.0.4.103 glusterfs_zone=3 glusterfs_devices=\n[ \n/dev/xvdc\n ]\n\n\n\n\n\n\nThe highlighted lines indicate the vital options in the inventory file to instruct \nopenshift-ansible\n to deploy this setup.\n\nHosts in the \n[glusterfs_registry]\n group will run the CNS cluster specifically created for OpenShift Infrastructure. Just like in Module 2, each host gets specific information about the free block device to use for CNS and the failure zone it resides in (the infrastructure nodes are also hosted in 3 different Availability Zones)\n\nThe option \nopenshift_hosted_registry_storage_kind=glusterfs\n will cause the registry to re-deployed with a \nPVC\n served by this cluster.\n\n\n First ensure that from an Ansible-perspective the required nodes are reachable\n\n\nansible -i /etc/ansible/ocp-with-glusterfs-registry glusterfs_registry -m ping\n\n\n\n\n\nAll 3 OpenShift infrastructure nodes should respond:\n\n\ninfra-3.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\ninfra-1.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\ninfra-2.lab | SUCCESS =\n {\n    \nchanged\n: false,\n    \nping\n: \npong\n\n}\n\n\n\n\n\n Enable the Master to be schedulable:\n\n\noadm manage-node master.lab --schedulable=true\n\n\n\n\n\n\n\nWhy does the master need to be schedulable?\n\n\nThis is a very simple lab environment :)\n\nThere is no sophisticated external load-balancing across the infrastructure nodes in place.\n\nThat\ns why the OpenShift router will run on the master node. The router will get re-deployed when executing the following playbook.\n\nSo making the master accept pods again we ensure the re-deployed router finds it\ns place again.\n\n\n\n\n Run the CNS registry playbook that ships with \nopenshift-ansible\n:\n\n\nansible-playbook -i /etc/ansible/ocp-with-glusterfs-registry \\\n/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-glusterfs/registry.yml\n\n\n\n\n\n Disable scheduling on the Master again:\n\n\noadm manage-node master.lab --schedulable=false\n\n\n\n\n\nThis will take about 6-7 minutes to complete.\n\n\n\n\nImportant\n\n\nAs in Module 2, executing the playbook in \nbyo/openshift-glusterfs/registry.yml\n directly post-deployment is not supported in production as of yet. Special care has been taken in this lab so that it works here.\n\nThis will change in one of the next releases of \nopenshift-ansible\n.\n\n\n\n\nThe playbook should succeed without any failures. After it completes you have a Registry that uses CNS to store container images. It has automatically been scaled to 3 pods for high availability too.\n\n\n Log in as \noperator\n in the \ndefault\n namespace\n\n\noc login -u operator -n default\n\n\n\n\n\n Verify a new version of the registry\ns \nDeploymentConfig\n is running:\n\n\noc get deploymentconfig/docker-registry\n\n\n\n\n\nIt should say:\n\n\nNAME              REVISION   DESIRED   CURRENT   TRIGGERED BY\ndocker-registry   2          1         1         config\n\n\n\n\n\n You can review the details and see how the \nPVC\n backs the registry by executing this command\n\n\noc describe deploymentconfig/docker-registry\n\n\n\n\n\nName:       docker-registry\nNamespace:  default\nCreated:    42 minutes ago\nLabels:     docker-registry=default\nAnnotations:    \nnone\n\nLatest Version: 2\nSelector:   docker-registry=default\nReplicas:   3\nTriggers:   Config\nStrategy:   Rolling\nTemplate:\nPod Template:\n  Labels:       docker-registry=default\n  Service Account:  registry\n  Containers:\n   registry:\n    Image:  mirror.lab:5555/openshift3/ose-docker-registry:v3.6.173.0.21\n    Port:   5000/TCP\n    Requests:\n      cpu:  100m\n      memory:   256Mi\n    Liveness:   http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3\n    Readiness:  http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\n    Environment:\n      REGISTRY_HTTP_ADDR:                   :5000\n      REGISTRY_HTTP_NET:                    tcp\n      REGISTRY_HTTP_SECRET:                 4wwwdIkD5sKc/AcAQ76BuNaOWF13MvkYge6ltjJobn0=\n      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:    false\n      REGISTRY_HTTP_TLS_KEY:                    /etc/secrets/registry.key\n      REGISTRY_HTTP_TLS_CERTIFICATE:                /etc/secrets/registry.crt\n    Mounts:\n      /etc/secrets from registry-certificates (rw)\n      /registry from registry-storage (rw)\n  Volumes:\n   registry-storage:\n\n    Type:   PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)\n\n    ClaimName:  registry-claim\n\n    ReadOnly:   false\n   registry-certificates:\n    Type:   Secret (a volume populated by a Secret)\n    SecretName: registry-certificates\n    Optional:   false\n\nDeployment #2 (latest):\n    Name:       docker-registry-2\n    Created:    14 minutes ago\n    Status:     Complete\n    Replicas:   3 current / 3 desired\n    Selector:   deployment=docker-registry-2,deploymentconfig=docker-registry,docker-registry=default\n    Labels:     docker-registry=default,openshift.io/deployment-config.name=docker-registry\n    Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nDeployment #1:\n    Created:    42 minutes ago\n    Status:     Complete\n    Replicas:   0 current / 0 desired\n\nEvents:\n  FirstSeen LastSeen    Count   From                SubObjectPath   Type        Reason              Message\n  --------- --------    -----   ----                -------------   --------    ------              -------\n  42m       42m     1   deploymentconfig-controller         Normal      DeploymentCreated       Created new replication controller \ndocker-registry-1\n for version 1\n  14m       14m     1   deploymentconfig-controller         Normal      DeploymentCreated       Created new replication controller \ndocker-registry-2\n for version 2\n  14m       14m     1   deploymentconfig-controller         Normal      ReplicationControllerScaled Scaled replication controller \ndocker-registry-1\n from 842477907952 to 3\n\n\n\n\n\nWhile the exact output will be different for you it is easy to tell the registry pods are configured to mount a \nPersistentVolume\n associated with the \nPersistentVolumeClaim\n named \nregistry-claim\n.\n\n\n Verify this \nPVC\n exists:\n\n\noc get pvc/registry-claim\n\n\n\n\n\nThe \nPVC\n was automatically generated by \nopenshift-ansible\n:\n\n\nNAME             STATUS    VOLUME            CAPACITY   ACCESSMODES   STORAGECLASS   AGE\nregistry-claim   Bound     registry-volume   5Gi        RWX                          23m\n\n\n\n\n\nIn the OpenShift UI you will see the new Registry configuration when you log on as \noperator\n and check the \nOverview\n page in the \ndefault\n namespace:\n\n\n\n\nopenshift-ansible\n generated an independent set of GlusterFS pods, a separate instance of \nheketi\n and a separate \nStorageClass\n as well. These components were configured to use the \ninfra-storage\n namespace. Refer to Module 2 to get an understanding of those components if you just skipped to here.\n\n\n Verify there are at least 3 GlusterFS pods and one \nheketi\n pod:\n\n\noc get pods -n infra-storage\n\n\n\n\n\nThere is now dedicated a CNS stack for OpenShift Infrastructure:\n\n\nNAME                       READY     STATUS    RESTARTS   AGE\nglusterfs-registry-1jct8   1/1       Running   0          33m\nglusterfs-registry-jp92q   1/1       Running   0          33m\nglusterfs-registry-x2kkm   1/1       Running   0          33m\nheketi-registry-1-k744l    1/1       Running   0          30m\n\n\n\n\n\nWith this you have successfully remediated a single point of failure from your OpenShift installation. Since this setup is entirely automated by \nopenshift-ansible\n you can deploy out of the box an OpenShift environment capable of hosting stateful applications and operate a fault-tolerant registry. All this with no external dependencies or complicated integration of external storage :)\n\n\n\n\nOpenShift Logging/Metrics on CNS\n#\n\n\nOpenShift Logging (Kibana) and Metrics (Cassandra) are also components that require persistent storage. Typically so far external block storage providers had to be used in order to get these services to work reliably.\n\n\nIt might seem counter-intuitive at first to consider CNS to serve these systems, mainly because:\n\n\n\n\nKibana and Cassandra are shared-nothing scale-out services\n\n\nCNS provides shared filesystem storage whereas for Kibana/Cassandra a block storage device formatted with a local filesystem like XFS would be enough\n\n\n\n\nHere are a couple of reasons why it is still a good idea to run those on CNS:\n\n\n\n\nthe total amount of storage available infra nodes is typically limited in capacity (CNS can scale beyond that)\n\n\nthe storage type available in infra nodes is most likely not suitable for performant long-term operations of these services (CNS uses aggregate performance of multiple devices and hosts)\n\n\nwithout CNS some sort external storage system is required that requires additional manual configuration steps, in any case you should not use \nemptyDir\n\n\n\n\n\n\nImportant\n\n\nIn the next couple of steps we will deploy Logging/Metrics on CNS. This is will be supported with the upcoming release of CNS 3.6 (early October 2017) in production. However the instructions will slightly vary because the \nPersistentVolumes\n will not be backed by a normal GlusterFS volume but by \ngluster-block\n to achieve high performance.\n\nIt will not be supported to run these services on standard \ngluster-fuse\n based CNS.\n\n\n\n\nTo review the required configuration sections in the \nopenshift-ansible\n inventory file, open the standard inventory file \n/etc/ansible/hosts/\n that was used to deploy this OCP cluster initially:\n\n\n/etc/ansible/hosts:\n\n\n[OSEv3:children]\n\n\nmasters\n\n\nnodes\n\n\n\n[OSEv3:vars]\n\n\ndeployment_type\n=\nopenshift-enterprise\n\n\ncontainerized\n=\ntrue\n\n\nopenshift_image_tag\n=\nv3.6.173.0.21\n\n\nopenshift_master_identity_providers\n=\n[{\nname\n: \nhtpasswd\n, \nlogin\n: \ntrue\n, \nchallenge\n: \ntrue\n, \nkind\n: \nHTPasswdPasswordIdentityProvider\n, \nfilename\n: \n/etc/origin/master/htpasswd\n}]\n\n\nopenshift_master_htpasswd_users\n=\n{\ndeveloper\n: \n$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/\n,\noperator\n: \n$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/\n}\n\n\nopenshift_master_default_subdomain\n=\ncloudapps.52.59.170.248.nip.io\n\n\nopenshift_router_selector\n=\nrole=master\n\n\nopenshift_registry_selector\n=\nrole=infra\n\n\nopenshift_metrics_install_metrics\n=\nfalse\n\n\nopenshift_metrics_hawkular_hostname\n=\nhawkular-metrics.{{ openshift_master_default_subdomain }}\n\n\nopenshift_metrics_cassandra_storage_type\n=\npv\n\n\nopenshift_metrics_cassandra_pvc_size\n=\n10Gi\n\n\nopenshift_logging_install_logging\n=\nfalse\n\n\nopenshift_logging_es_pvc_size\n=\n10Gi\n\n\nopenshift_logging_es_pvc_dynamic\n=\ntrue\n\n\n\n\n[... output omitted... ]\n\n\n\n\n\n\nThe highlighted lines indicate the settings that are required in order to put the Cassandra database of the OpenShift Metrics service on a \nPersistentVolume\n (\nopenshift_metrics_cassandra_storage_type=pv\n) and how large this volume should be (\nopenshift_metrics_cassandra_pvc_size=10Gi\n).\n\nSimilarly the backend for the ElasticSearch component of OpenShift Logging is set to \nPersistentVolume\n (\nopenshift_logging_es_pvc_dynamic=true\n) and the size is specifed (\nopenshift_logging_es_pvc_size=10Gi\n).\n\nWith these settings in place \nopenshift-ansible\n will request \nPVC\n object for these services.\n\n\nUnfortunately \nopenshift-ansible\n today is lacking the ability to specify a certain \nStorageClass\n with those \nPVCs\n, so we have to make the CNS cluster that was created above temporarily the system-wide default.\n\n\n Login as \noperator\n to the \nopenshift-infra\n namespace:\n\n\noc login -u operator -n openshift-infra\n\n\n\n\n\n First, if you deployed the general-purpose CNS cluster in \nModule 2\n, you need to disable the other \nStorageClass\n \nglusterfs-storage\n from the other CNS stack as being the default:\n\n\noc patch storageclass glusterfs-storage \\\n-p \n{\nmetadata\n: {\nannotations\n: {\nstorageclass.kubernetes.io/is-default-class\n: \nfalse\n}}}\n\n\n\n\n\n\n Then, use the \noc patch\n command again to change the definition of the \nStorageClass\n on the fly:\n\n\noc patch storageclass glusterfs-registry \\\n-p \n{\nmetadata\n: {\nannotations\n: {\nstorageclass.kubernetes.io/is-default-class\n: \ntrue\n}}}\n\n\n\n\n\n\n Verify that now the \nStorageClass\n \nglusterfs-registry\n is the default:\n\n\noc get storageclass\n\n\n\n\n\nNAME                           TYPE\n\nglusterfs-registry (default)   kubernetes.io/glusterfs\n\nglusterfs-storage              kubernetes.io/glusterfs\n\n\n\n\n\nDeploying OpenShift Metrics with Persistent Storage from CNS\n#\n\n\nThe inventory file \n/etc/ansible/hosts\n as explained has all the required options set to run Logging/Metrics on dynamic provisioned storage supplied via a \nPVC\n. The only variable that we need to override is (\nopenshift_metrics_install_metrics\n) to actually invoke the required playbooks the installation.\n\n\n Execute the Metrics deployment playbook like this:\n\n\nansible-playbook -i /etc/ansible/hosts \\\n    -e openshift_metrics_install_metrics=True \\\n    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml\n\n\n\n\n\nThis takes about 1-2 minutes to complete. However the deployment is not quite finished yet.\n\n\n Use the \nwatch\n command to wait for the \nhawkular-metrics\n pod to be in \nREADY\n state.\n\n\nwatch oc get pods -l name=hawkular-metrics\n\n\n\n\n\nExit out of the watch mode with: \nCtrl\n + \nc\n\n\nIt will be ready when the database (Cassandra) finished initializing. Alternatively in the UI observe the deployment in the \nOverview\n pane, focussing on the \nhawkular-metrics\n deployment:\n\n\n\n\nAfter 2-3 minutes all 3 pods, that make up the OpenShift Metrics service, should be ready:\n\n\n Verify all pods in the namespace have a \n1/1\n in the \nREADY\n column:\n\n\noc get pods\n\n\n\n\n\nNAME                         READY     STATUS    RESTARTS   AGE\nhawkular-cassandra-1-sxctx   1/1       Running   0          5m\nhawkular-metrics-895xz       1/1       Running   0          5m\nheapster-pjxpp               1/1       Running   0          5m\n\n\n\n\n\nTo use the Metrics service you need to logon / reload the OpenShift UI in your browser. You will then see a warning message like this one:\n\n\n\n\nDon\nt worry - this is due to self-signed SSL certificates in this environment.\n\n\n Click the \nOpen Metrics URL\n link and accept the self-signed certificate in your new browser tab. You will see the status page of the OpenShift Hawkular Metrics component:\n\n\n\n\n Then go back to the overview page. Next to the pods monitoring graphs for CPU, memory and network consumption will appear:\n\n\n\n\nIf you change to the \nStorage\n menu in the OpenShift UI you will also see the \nPVC\n that \nopenshift-ansible\n has set up for the Cassandra pod.\n\n\n\n\nCongratulations. You have successfully deploy OpenShift Metrics using scalable, fault-tolerant and persistent storage. The data that you see visualized in the UI is stored on a \nPersistentVolume\n served by CNS.\n\n\n\n\nPreview\n\n\nThis was a preview of the general process. Note that this will be supported for production with the release of CNS 3.6.\n\n\n\n\nDeploying OpenShift Logging with Persistent Storage from CNS\n#\n\n\nIn a very similar fashion you can install OpenShift Logging Services, run by Kibana and ElasticSearch.\n\n\n As \noperator\n, login in to the \nlogging\n namespace:\n\n\noc login -u operator -n logging\n\n\n\n\n\n Execute the Logging deployment playbook like this:\n\n\nansible-playbook -i /etc/ansible/hosts \\\n  -e openshift_logging_install_logging=true \\\n  /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml\n\n\n\n\n\nAfter 1-2 minutes the playbook finishes and you have a number of new pods in the \nlogging\n namespace:\n\n\n List all the ElasticSearch pods that aggregate and store the logs:\n\n\noc get pods -l component=es\n\n\n\n\n\nThis pod runs a single ElasticSearch instance.\n\n\nNAME                                      READY     STATUS    RESTARTS   AGE\nlogging-es-data-master-kcbtgll3-1-vb34h   1/1       Running   0          11m\n\n\n\n\n\n List the Kibana pod:\n\n\noc get pods -l component=kibana\n\n\n\n\n\nThis pod runs the Kibana front-end to query and search through logs:\n\n\nNAME                     READY     STATUS    RESTARTS   AGE\nlogging-kibana-1-1c2dj   2/2       Running   0          11m\n\n\n\n\n\n List all Fluentd pods:\n\n\noc get pods -l component=fluentd\n\n\n\n\n\nThese pods run as part of a \nDaemonSet\n and are responsible for collecting and shipping the various logs from all nodes to the ElasticSearch instance.\n\n\nNAME                    READY     STATUS    RESTARTS   AGE\nlogging-fluentd-3k7nh   1/1       Running   0          5m\nlogging-fluentd-473cf   1/1       Running   0          4m\nlogging-fluentd-9kgsv   1/1       Running   0          5m\nlogging-fluentd-h8fhb   1/1       Running   0          4m\nlogging-fluentd-pb6h8   1/1       Running   0          4m\nlogging-fluentd-q6lv4   1/1       Running   0          4m\nlogging-fluentd-r455n   1/1       Running   0          4m\nlogging-fluentd-v34ll   1/1       Running   0          5m\nlogging-fluentd-vxnd3   1/1       Running   0          5m\nlogging-fluentd-wf3lr   1/1       Running   0          5m\n\n\n\n\n\nSwitch to the OpenShift UI and as \noperator\n select the \nlogging\n project. In the \nOverview\n section you\nll a \nRoute\n for the Kibana deployment created. \nClick\n the link on the \nRoute\n to open the Kibana UI in a new browser tab and verify the Kibana deployment is healthy.\n\n\n\n\nThe public URL for the Kibana UI will be visible in the \nROUTES\n section of the \nlogging-kibana\n deployment.\n\n\n\n\nWhen you are logging on for the first time Kibana will ask for credentials.\n\nUse your OpenShift \noperator\n account with the password \nr3dh4t\n.\n\n\nAfter logging in you will see that Kibana has started indexing the database, which is hosted on CNS by ElasticSearch.\n\n\n\n\nThis shows that ElasticSearch search running of CNS-provided \nPersistentVolume\n successfully. It will take some time to complete the first indexing.", 
            "title": "Module 5 - Persistent Storage for Infrastructure"
        }, 
        {
            "location": "/module-5-cns-for-infra/#openshift-registry-on-cns", 
            "text": "The Registry in OpenShift is used to store all images that result of Source-to-Image deployments as well as custom container images. \nIt runs as one or more containers in specific Infrastructure Nodes or Master Nodes in OpenShift.  As explorerd in Module 1, by default the registry uses a hosts local storage ( emptyDir ) which makes it prone to outages. To avoid such outages the storage needs to be  persistent  and  shared  in order to survive Registry pod restarts and scale-out.  What we want to achieve is a setup like depicted in the following diagram:   This can be achieved with CNS simply by making the registry pods refer to a PVC in access mode  RWX  based on CNS. \nBefore OpenShift Container Platform 3.6 this had to be done manually on an existing CNS cluster.  With  openshift-ansible  you now have this setup task automated. The playbooks that implement this will deploy a  separate CNS cluster , preferably on the  Infrastructure Nodes , create a  PVC  and update the Registry  DeploymentConfig  to mount the associated  PV  which is where container images will then be stored.   Important  This method will be disruptive. All data stored in the registry so far will become unavailable. \nMigration scenarios exist but are beyond the scope of this lab.    Review the  openshift-ansible  inventory file in  /etc/ansible/ocp-with-glusterfs-registry  that has been prepared in your environment:  /etc/ansible/ocp-with-glusterfs-registry:  [OSEv3:children]  masters  nodes  glusterfs_registry   [OSEv3:vars]  deployment_type = openshift-enterprise  containerized = true  openshift_image_tag = v3.6.173.0.21  openshift_master_identity_providers = [{ name :  htpasswd ,  login :  true ,  challenge :  true ,  kind :  HTPasswdPasswordIdentityProvider ,  filename :  /etc/origin/master/htpasswd }]  openshift_master_htpasswd_users = { developer :  $apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/ , operator :  $apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/ }  openshift_master_default_subdomain = cloudapps.52.59.170.248.nip.io  openshift_router_selector = role=master  openshift_registry_selector = role=infra  openshift_hosted_registry_storage_kind = glusterfs  openshift_storage_glusterfs_registry_storageclass = true  openshift_metrics_install_metrics = false  openshift_metrics_hawkular_hostname = hawkular-metrics.{{ openshift_master_default_subdomain }}  openshift_metrics_cassandra_storage_type = pv  openshift_metrics_cassandra_pvc_size = 10Gi  openshift_logging_install_logging = false  openshift_logging_es_pvc_size = 10Gi  openshift_logging_es_pvc_dynamic = true  openshift_storage_glusterfs_image = rhgs3/rhgs-server-rhel7  openshift_storage_glusterfs_version = 3.2.0-7  openshift_storage_glusterfs_heketi_image = rhgs3/rhgs-volmanager-rhel7  openshift_storage_glusterfs_heketi_version = 3.2.0-11  openshift_storage_glusterfs_registry_namespace = infra-storage  openshift_docker_additional_registries = mirror.lab:5555  openshift_docker_insecure_registries = mirror.lab:5555  oreg_url = mirror.lab:5555/openshift3/ose-${component}:${version}  openshift_examples_modify_imagestreams = false  openshift_disable_check = disk_availability,memory_availability  [masters]  master.lab openshift_public_hostname = 52.59.170.248.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=52.59.170.248  [masters:vars]  openshift_schedulable = true  openshift_node_labels = { role :  master }  [nodes]  master.lab openshift_public_hostname = 52.59.170.248.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=52.59.170.248  infra-1.lab openshift_hostname = infra-1.lab openshift_ip=10.0.2.101 openshift_node_labels= { role :  infra }  infra-2.lab openshift_hostname = infra-2.lab openshift_ip=10.0.3.102 openshift_node_labels= { role :  infra }  infra-3.lab openshift_hostname = infra-3.lab openshift_ip=10.0.4.103 openshift_node_labels= { role :  infra }  node-1.lab openshift_hostname = node-1.lab openshift_ip=10.0.2.201 openshift_node_labels= { role :  app }  node-2.lab openshift_hostname = node-2.lab openshift_ip=10.0.3.202 openshift_node_labels= { role :  app }  node-3.lab openshift_hostname = node-3.lab openshift_ip=10.0.4.203 openshift_node_labels= { role :  app }  node-4.lab openshift_hostname = node-4.lab openshift_ip=10.0.2.204 openshift_node_labels= { role :  app }  node-5.lab openshift_hostname = node-5.lab openshift_ip=10.0.3.205 openshift_node_labels= { role :  app }  node-6.lab openshift_hostname = node-6.lab openshift_ip=10.0.4.206 openshift_node_labels= { role :  app }  [glusterfs_registry]  infra-1.lab glusterfs_ip = 10.0.2.101 glusterfs_zone=1 glusterfs_devices= [  /dev/xvdc  ]  infra-2.lab glusterfs_ip = 10.0.3.102 glusterfs_zone=2 glusterfs_devices= [  /dev/xvdc  ]  infra-3.lab glusterfs_ip = 10.0.4.103 glusterfs_zone=3 glusterfs_devices= [  /dev/xvdc  ]   The highlighted lines indicate the vital options in the inventory file to instruct  openshift-ansible  to deploy this setup. \nHosts in the  [glusterfs_registry]  group will run the CNS cluster specifically created for OpenShift Infrastructure. Just like in Module 2, each host gets specific information about the free block device to use for CNS and the failure zone it resides in (the infrastructure nodes are also hosted in 3 different Availability Zones) \nThe option  openshift_hosted_registry_storage_kind=glusterfs  will cause the registry to re-deployed with a  PVC  served by this cluster.   First ensure that from an Ansible-perspective the required nodes are reachable  ansible -i /etc/ansible/ocp-with-glusterfs-registry glusterfs_registry -m ping  All 3 OpenShift infrastructure nodes should respond:  infra-3.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\ninfra-1.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}\ninfra-2.lab | SUCCESS =  {\n     changed : false,\n     ping :  pong \n}   Enable the Master to be schedulable:  oadm manage-node master.lab --schedulable=true   Why does the master need to be schedulable?  This is a very simple lab environment :) \nThere is no sophisticated external load-balancing across the infrastructure nodes in place. \nThat s why the OpenShift router will run on the master node. The router will get re-deployed when executing the following playbook. \nSo making the master accept pods again we ensure the re-deployed router finds it s place again.    Run the CNS registry playbook that ships with  openshift-ansible :  ansible-playbook -i /etc/ansible/ocp-with-glusterfs-registry \\\n/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-glusterfs/registry.yml   Disable scheduling on the Master again:  oadm manage-node master.lab --schedulable=false  This will take about 6-7 minutes to complete.   Important  As in Module 2, executing the playbook in  byo/openshift-glusterfs/registry.yml  directly post-deployment is not supported in production as of yet. Special care has been taken in this lab so that it works here. \nThis will change in one of the next releases of  openshift-ansible .   The playbook should succeed without any failures. After it completes you have a Registry that uses CNS to store container images. It has automatically been scaled to 3 pods for high availability too.   Log in as  operator  in the  default  namespace  oc login -u operator -n default   Verify a new version of the registry s  DeploymentConfig  is running:  oc get deploymentconfig/docker-registry  It should say:  NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY\ndocker-registry   2          1         1         config   You can review the details and see how the  PVC  backs the registry by executing this command  oc describe deploymentconfig/docker-registry  Name:       docker-registry\nNamespace:  default\nCreated:    42 minutes ago\nLabels:     docker-registry=default\nAnnotations:     none \nLatest Version: 2\nSelector:   docker-registry=default\nReplicas:   3\nTriggers:   Config\nStrategy:   Rolling\nTemplate:\nPod Template:\n  Labels:       docker-registry=default\n  Service Account:  registry\n  Containers:\n   registry:\n    Image:  mirror.lab:5555/openshift3/ose-docker-registry:v3.6.173.0.21\n    Port:   5000/TCP\n    Requests:\n      cpu:  100m\n      memory:   256Mi\n    Liveness:   http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3\n    Readiness:  http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3\n    Environment:\n      REGISTRY_HTTP_ADDR:                   :5000\n      REGISTRY_HTTP_NET:                    tcp\n      REGISTRY_HTTP_SECRET:                 4wwwdIkD5sKc/AcAQ76BuNaOWF13MvkYge6ltjJobn0=\n      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:    false\n      REGISTRY_HTTP_TLS_KEY:                    /etc/secrets/registry.key\n      REGISTRY_HTTP_TLS_CERTIFICATE:                /etc/secrets/registry.crt\n    Mounts:\n      /etc/secrets from registry-certificates (rw)\n      /registry from registry-storage (rw)\n  Volumes:\n   registry-storage:     Type:   PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)     ClaimName:  registry-claim     ReadOnly:   false\n   registry-certificates:\n    Type:   Secret (a volume populated by a Secret)\n    SecretName: registry-certificates\n    Optional:   false\n\nDeployment #2 (latest):\n    Name:       docker-registry-2\n    Created:    14 minutes ago\n    Status:     Complete\n    Replicas:   3 current / 3 desired\n    Selector:   deployment=docker-registry-2,deploymentconfig=docker-registry,docker-registry=default\n    Labels:     docker-registry=default,openshift.io/deployment-config.name=docker-registry\n    Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed\nDeployment #1:\n    Created:    42 minutes ago\n    Status:     Complete\n    Replicas:   0 current / 0 desired\n\nEvents:\n  FirstSeen LastSeen    Count   From                SubObjectPath   Type        Reason              Message\n  --------- --------    -----   ----                -------------   --------    ------              -------\n  42m       42m     1   deploymentconfig-controller         Normal      DeploymentCreated       Created new replication controller  docker-registry-1  for version 1\n  14m       14m     1   deploymentconfig-controller         Normal      DeploymentCreated       Created new replication controller  docker-registry-2  for version 2\n  14m       14m     1   deploymentconfig-controller         Normal      ReplicationControllerScaled Scaled replication controller  docker-registry-1  from 842477907952 to 3  While the exact output will be different for you it is easy to tell the registry pods are configured to mount a  PersistentVolume  associated with the  PersistentVolumeClaim  named  registry-claim .   Verify this  PVC  exists:  oc get pvc/registry-claim  The  PVC  was automatically generated by  openshift-ansible :  NAME             STATUS    VOLUME            CAPACITY   ACCESSMODES   STORAGECLASS   AGE\nregistry-claim   Bound     registry-volume   5Gi        RWX                          23m  In the OpenShift UI you will see the new Registry configuration when you log on as  operator  and check the  Overview  page in the  default  namespace:   openshift-ansible  generated an independent set of GlusterFS pods, a separate instance of  heketi  and a separate  StorageClass  as well. These components were configured to use the  infra-storage  namespace. Refer to Module 2 to get an understanding of those components if you just skipped to here.   Verify there are at least 3 GlusterFS pods and one  heketi  pod:  oc get pods -n infra-storage  There is now dedicated a CNS stack for OpenShift Infrastructure:  NAME                       READY     STATUS    RESTARTS   AGE\nglusterfs-registry-1jct8   1/1       Running   0          33m\nglusterfs-registry-jp92q   1/1       Running   0          33m\nglusterfs-registry-x2kkm   1/1       Running   0          33m\nheketi-registry-1-k744l    1/1       Running   0          30m  With this you have successfully remediated a single point of failure from your OpenShift installation. Since this setup is entirely automated by  openshift-ansible  you can deploy out of the box an OpenShift environment capable of hosting stateful applications and operate a fault-tolerant registry. All this with no external dependencies or complicated integration of external storage :)", 
            "title": "OpenShift Registry on CNS"
        }, 
        {
            "location": "/module-5-cns-for-infra/#openshift-loggingmetrics-on-cns", 
            "text": "OpenShift Logging (Kibana) and Metrics (Cassandra) are also components that require persistent storage. Typically so far external block storage providers had to be used in order to get these services to work reliably.  It might seem counter-intuitive at first to consider CNS to serve these systems, mainly because:   Kibana and Cassandra are shared-nothing scale-out services  CNS provides shared filesystem storage whereas for Kibana/Cassandra a block storage device formatted with a local filesystem like XFS would be enough   Here are a couple of reasons why it is still a good idea to run those on CNS:   the total amount of storage available infra nodes is typically limited in capacity (CNS can scale beyond that)  the storage type available in infra nodes is most likely not suitable for performant long-term operations of these services (CNS uses aggregate performance of multiple devices and hosts)  without CNS some sort external storage system is required that requires additional manual configuration steps, in any case you should not use  emptyDir    Important  In the next couple of steps we will deploy Logging/Metrics on CNS. This is will be supported with the upcoming release of CNS 3.6 (early October 2017) in production. However the instructions will slightly vary because the  PersistentVolumes  will not be backed by a normal GlusterFS volume but by  gluster-block  to achieve high performance. \nIt will not be supported to run these services on standard  gluster-fuse  based CNS.   To review the required configuration sections in the  openshift-ansible  inventory file, open the standard inventory file  /etc/ansible/hosts/  that was used to deploy this OCP cluster initially:  /etc/ansible/hosts:  [OSEv3:children]  masters  nodes  [OSEv3:vars]  deployment_type = openshift-enterprise  containerized = true  openshift_image_tag = v3.6.173.0.21  openshift_master_identity_providers = [{ name :  htpasswd ,  login :  true ,  challenge :  true ,  kind :  HTPasswdPasswordIdentityProvider ,  filename :  /etc/origin/master/htpasswd }]  openshift_master_htpasswd_users = { developer :  $apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/ , operator :  $apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/ }  openshift_master_default_subdomain = cloudapps.52.59.170.248.nip.io  openshift_router_selector = role=master  openshift_registry_selector = role=infra  openshift_metrics_install_metrics = false  openshift_metrics_hawkular_hostname = hawkular-metrics.{{ openshift_master_default_subdomain }}  openshift_metrics_cassandra_storage_type = pv  openshift_metrics_cassandra_pvc_size = 10Gi  openshift_logging_install_logging = false  openshift_logging_es_pvc_size = 10Gi  openshift_logging_es_pvc_dynamic = true   [... output omitted... ]   The highlighted lines indicate the settings that are required in order to put the Cassandra database of the OpenShift Metrics service on a  PersistentVolume  ( openshift_metrics_cassandra_storage_type=pv ) and how large this volume should be ( openshift_metrics_cassandra_pvc_size=10Gi ). \nSimilarly the backend for the ElasticSearch component of OpenShift Logging is set to  PersistentVolume  ( openshift_logging_es_pvc_dynamic=true ) and the size is specifed ( openshift_logging_es_pvc_size=10Gi ). \nWith these settings in place  openshift-ansible  will request  PVC  object for these services.  Unfortunately  openshift-ansible  today is lacking the ability to specify a certain  StorageClass  with those  PVCs , so we have to make the CNS cluster that was created above temporarily the system-wide default.   Login as  operator  to the  openshift-infra  namespace:  oc login -u operator -n openshift-infra   First, if you deployed the general-purpose CNS cluster in  Module 2 , you need to disable the other  StorageClass   glusterfs-storage  from the other CNS stack as being the default:  oc patch storageclass glusterfs-storage \\\n-p  { metadata : { annotations : { storageclass.kubernetes.io/is-default-class :  false }}}    Then, use the  oc patch  command again to change the definition of the  StorageClass  on the fly:  oc patch storageclass glusterfs-registry \\\n-p  { metadata : { annotations : { storageclass.kubernetes.io/is-default-class :  true }}}    Verify that now the  StorageClass   glusterfs-registry  is the default:  oc get storageclass  NAME                           TYPE glusterfs-registry (default)   kubernetes.io/glusterfs glusterfs-storage              kubernetes.io/glusterfs", 
            "title": "OpenShift Logging/Metrics on CNS"
        }, 
        {
            "location": "/module-5-cns-for-infra/#deploying-openshift-metrics-with-persistent-storage-from-cns", 
            "text": "The inventory file  /etc/ansible/hosts  as explained has all the required options set to run Logging/Metrics on dynamic provisioned storage supplied via a  PVC . The only variable that we need to override is ( openshift_metrics_install_metrics ) to actually invoke the required playbooks the installation.   Execute the Metrics deployment playbook like this:  ansible-playbook -i /etc/ansible/hosts \\\n    -e openshift_metrics_install_metrics=True \\\n    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-metrics.yml  This takes about 1-2 minutes to complete. However the deployment is not quite finished yet.   Use the  watch  command to wait for the  hawkular-metrics  pod to be in  READY  state.  watch oc get pods -l name=hawkular-metrics  Exit out of the watch mode with:  Ctrl  +  c  It will be ready when the database (Cassandra) finished initializing. Alternatively in the UI observe the deployment in the  Overview  pane, focussing on the  hawkular-metrics  deployment:   After 2-3 minutes all 3 pods, that make up the OpenShift Metrics service, should be ready:   Verify all pods in the namespace have a  1/1  in the  READY  column:  oc get pods  NAME                         READY     STATUS    RESTARTS   AGE\nhawkular-cassandra-1-sxctx   1/1       Running   0          5m\nhawkular-metrics-895xz       1/1       Running   0          5m\nheapster-pjxpp               1/1       Running   0          5m  To use the Metrics service you need to logon / reload the OpenShift UI in your browser. You will then see a warning message like this one:   Don t worry - this is due to self-signed SSL certificates in this environment.   Click the  Open Metrics URL  link and accept the self-signed certificate in your new browser tab. You will see the status page of the OpenShift Hawkular Metrics component:    Then go back to the overview page. Next to the pods monitoring graphs for CPU, memory and network consumption will appear:   If you change to the  Storage  menu in the OpenShift UI you will also see the  PVC  that  openshift-ansible  has set up for the Cassandra pod.   Congratulations. You have successfully deploy OpenShift Metrics using scalable, fault-tolerant and persistent storage. The data that you see visualized in the UI is stored on a  PersistentVolume  served by CNS.   Preview  This was a preview of the general process. Note that this will be supported for production with the release of CNS 3.6.", 
            "title": "Deploying OpenShift Metrics with Persistent Storage from CNS"
        }, 
        {
            "location": "/module-5-cns-for-infra/#deploying-openshift-logging-with-persistent-storage-from-cns", 
            "text": "In a very similar fashion you can install OpenShift Logging Services, run by Kibana and ElasticSearch.   As  operator , login in to the  logging  namespace:  oc login -u operator -n logging   Execute the Logging deployment playbook like this:  ansible-playbook -i /etc/ansible/hosts \\\n  -e openshift_logging_install_logging=true \\\n  /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml  After 1-2 minutes the playbook finishes and you have a number of new pods in the  logging  namespace:   List all the ElasticSearch pods that aggregate and store the logs:  oc get pods -l component=es  This pod runs a single ElasticSearch instance.  NAME                                      READY     STATUS    RESTARTS   AGE\nlogging-es-data-master-kcbtgll3-1-vb34h   1/1       Running   0          11m   List the Kibana pod:  oc get pods -l component=kibana  This pod runs the Kibana front-end to query and search through logs:  NAME                     READY     STATUS    RESTARTS   AGE\nlogging-kibana-1-1c2dj   2/2       Running   0          11m   List all Fluentd pods:  oc get pods -l component=fluentd  These pods run as part of a  DaemonSet  and are responsible for collecting and shipping the various logs from all nodes to the ElasticSearch instance.  NAME                    READY     STATUS    RESTARTS   AGE\nlogging-fluentd-3k7nh   1/1       Running   0          5m\nlogging-fluentd-473cf   1/1       Running   0          4m\nlogging-fluentd-9kgsv   1/1       Running   0          5m\nlogging-fluentd-h8fhb   1/1       Running   0          4m\nlogging-fluentd-pb6h8   1/1       Running   0          4m\nlogging-fluentd-q6lv4   1/1       Running   0          4m\nlogging-fluentd-r455n   1/1       Running   0          4m\nlogging-fluentd-v34ll   1/1       Running   0          5m\nlogging-fluentd-vxnd3   1/1       Running   0          5m\nlogging-fluentd-wf3lr   1/1       Running   0          5m  Switch to the OpenShift UI and as  operator  select the  logging  project. In the  Overview  section you ll a  Route  for the Kibana deployment created.  Click  the link on the  Route  to open the Kibana UI in a new browser tab and verify the Kibana deployment is healthy.   The public URL for the Kibana UI will be visible in the  ROUTES  section of the  logging-kibana  deployment.   When you are logging on for the first time Kibana will ask for credentials. \nUse your OpenShift  operator  account with the password  r3dh4t .  After logging in you will see that Kibana has started indexing the database, which is hosted on CNS by ElasticSearch.   This shows that ElasticSearch search running of CNS-provided  PersistentVolume  successfully. It will take some time to complete the first indexing.", 
            "title": "Deploying OpenShift Logging with Persistent Storage from CNS"
        }
    ]
}