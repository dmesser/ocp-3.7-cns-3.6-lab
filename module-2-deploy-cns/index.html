



<!DOCTYPE html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
      
        <link rel="canonical" href="https://dmesser.github.io/ocp-3.7-cns-3.6-lab/module-2-deploy-cns/">
      
      
      
        <meta name="lang:clipboard.copy" content="Copy to clipboard">
      
        <meta name="lang:clipboard.copied" content="Copied to clipboard">
      
        <meta name="lang:search.language" content="en">
      
        <meta name="lang:search.result.none" content="No matching documents">
      
        <meta name="lang:search.result.one" content="1 matching document">
      
        <meta name="lang:search.result.other" content="# matching documents">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-0.17.2, mkdocs-material-2.2.6">
    
    
      
        <title>Module 2 - Deploying Container-Native Storage - Container-Native Storage 3.6 on OpenShift Container Platform 3.7 Hands-on Lab</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.6525f7f6.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.792431c1.css">
      
    
    
      <script src="../assets/javascripts/modernizr.1aa3b519.js"></script>
    
    
      
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700|Roboto+Mono">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
      <link rel="stylesheet" href="https://fonts.googleapis.com/icon?family=Material+Icons">
    
    
    
  </head>
  
    
    
    
      <body data-md-color-primary="deep-orange" data-md-color-accent="indigo">
    
  
    <svg class="md-svg">
      <defs>
        
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="drawer">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="search">
    <label class="md-overlay" data-md-component="overlay" for="drawer"></label>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="https://dmesser.github.io/ocp-3.7-cns-3.6-lab/" title="Container-Native Storage 3.6 on OpenShift Container Platform 3.7 Hands-on Lab" class="md-header-nav__button md-logo">
          
            <img src="../img/shadowman_rgb.png" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            
              <span class="md-header-nav__topic">
                Container-Native Storage 3.6 on OpenShift Container Platform 3.7 Hands-on Lab
              </span>
              <span class="md-header-nav__topic">
                Module 2 - Deploying Container-Native Storage
              </span>
            
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          
            <label class="md-icon md-icon--search md-header-nav__button" for="search"></label>
            
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="search"></label>
  <div class="md-search__inner">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" required placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query">
      <label class="md-icon md-search__icon" for="search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset">&#xE5CD;</button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Type to start searching
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
          
        
      </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
        

  

<nav class="md-tabs md-tabs--active" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  <li class="md-tabs__item">
    
      <a href=".." title="Overview" class="md-tabs__link">
        Overview
      </a>
    
  </li>

      
        
  
  
    <li class="md-tabs__item">
      
        <a href="../module-1-install/" title="Modules" class="md-tabs__link md-tabs__link--active">
          Modules
        </a>
      
    </li>
  

      
    </ul>
  </div>
</nav>
      
      <main class="md-main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="drawer">
    <span class="md-nav__button md-logo">
      
        <img src="../img/shadowman_rgb.png" width="24" height="24">
      
    </span>
    Container-Native Storage 3.6 on OpenShift Container Platform 3.7 Hands-on Lab
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Overview" class="md-nav__link">
      Overview
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active md-nav__item--nested">
    
      <input class="md-toggle md-nav__toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2" checked>
    
    <label class="md-nav__link" for="nav-2">
      Modules
    </label>
    <nav class="md-nav" data-md-component="collapsible" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        Modules
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../module-1-install/" title="Module 1 - Combined OpenShift and CNS Installation" class="md-nav__link">
      Module 1 - Combined OpenShift and CNS Installation
    </a>
  </li>

        
          
          
          

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="toc">
    
    
      <label class="md-nav__link md-nav__link--active" for="toc">
        Module 2 - Deploying Container-Native Storage
      </label>
    
    <a href="./" title="Module 2 - Deploying Container-Native Storage" class="md-nav__link md-nav__link--active">
      Module 2 - Deploying Container-Native Storage
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#review-the-ansible-inventory-configuration" title="Review the Ansible Inventory Configuration" class="md-nav__link">
    Review the Ansible Inventory Configuration
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-the-installer" title="Run the installer" class="md-nav__link">
    Run the installer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-happens-in-the-background" title="What happens in the background" class="md-nav__link">
    What happens in the background
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-container-native-storage" title="Test Container-native Storage" class="md-nav__link">
    Test Container-native Storage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
    
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-3-cns-for-apps/" title="Module 3 - Persistent Storage for Apps" class="md-nav__link">
      Module 3 - Persistent Storage for Apps
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-4-cluster-ops/" title="Module 4 - Cluster Operations" class="md-nav__link">
      Module 4 - Cluster Operations
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../module-5-cns-for-infra/" title="Module 5 - Persistent Storage for Infrastructure" class="md-nav__link">
      Module 5 - Persistent Storage for Infrastructure
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
  
    <label class="md-nav__title" for="toc">Table of contents</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#review-the-ansible-inventory-configuration" title="Review the Ansible Inventory Configuration" class="md-nav__link">
    Review the Ansible Inventory Configuration
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#run-the-installer" title="Run the installer" class="md-nav__link">
    Run the installer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#what-happens-in-the-background" title="What happens in the background" class="md-nav__link">
    What happens in the background
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-container-native-storage" title="Test Container-native Storage" class="md-nav__link">
    Test Container-native Storage
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#summary" title="Summary" class="md-nav__link">
    Summary
  </a>
  
</li>
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  <h1>Module 2 - Deploying Container-Native Storage</h1>
                
                <div class="admonition summary">
<p class="admonition-title">Overview</p>
<p>In this module you will set up Container-Native Storage (CNS) in your OpenShift environment. You will use this in later modules to dynamically provision storage to be available to workloads in OpenShift.<br />
CNS is GlusterFS running in containers, orchestrated by OpenShift via a REST API. GlusterFS in turn is backed by local storage available to the OpenShift nodes.<br />
This module has no pre-requisites.</p>
</div>
<p>All of the following tasks are carried out as the <code>ec2-user</code> from the master node. For copy&amp;paste convenience we will omit the shell prompt unless necessary.</p>
<p>&#8680; Make sure you are logged on as the <code>ec2-user</code> to the master node:</p>
<div class="codehilite"><pre><span></span>hostname -f
</pre></div>


<p>As the output indicates, you should be on the master node:</p>
<div class="codehilite"><pre><span></span>master.lab
</pre></div>


<p>&#8680; First ensure you have the correct openshift-ansible version installed on the system.</p>
<div class="codehilite"><pre><span></span>yum list installed openshift-ansible
</pre></div>


<p>A version higher than or equals to <code>3.6.173.0.5-3</code> is required to utilize the <strong>OpenShift Advanced Installer</strong> to deploy CNS.</p>
<div class="codehilite"><pre><span></span>Installed Packages
openshift-ansible.noarch                   3.6.173.0.21-2.git.0.44a4038.el7                    @rhel-7-server-ose-3.6-rpms
</pre></div>


<hr />
<h2 id="review-the-ansible-inventory-configuration">Review the Ansible Inventory Configuration<a class="headerlink" href="#review-the-ansible-inventory-configuration" title="Permanent link">#</a></h2>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>As of OpenShift Container Platform 3.6 it&rsquo;s possible to deploy CNS using <code>openshift-ansible</code> - the <em>advanced installation method</em> of OpenShift. The method of using the <code>cns-deploy</code> utility to install CNS components in an existing OpenShift cluster remains available but is not covered in this lab.</p>
</div>
<p>Installing CNS with <code>openshift-ansible</code> means all configuration options for CNS are now managed in the Ansible inventory, a text file by which the installer determines what should be installed and where.</p>
<p>An inventory file with the correct settings for CNS has been provided for you in <code>/etc/ansible/ocp-with-glusterfs</code></p>
<p><kbd>/etc/ansible/ocp-with-glusterfs:</kbd></p>
<div class="codehilite"><pre><span></span><span class="k">[OSEv3:children]</span>
<span class="na">masters</span>
<span class="na">nodes</span>
<span class="hll"><span class="na">glusterfs</span>
</span>
<span class="k">[OSEv3:vars]</span>
<span class="na">deployment_type</span><span class="o">=</span><span class="s">openshift-enterprise</span>
<span class="na">containerized</span><span class="o">=</span><span class="s">true</span>
<span class="na">openshift_image_tag</span><span class="o">=</span><span class="s">v3.6.173.0.21</span>
<span class="na">openshift_master_identity_providers</span><span class="o">=</span><span class="s">[{&#39;name&#39;: &#39;htpasswd&#39;, &#39;login&#39;: &#39;true&#39;, &#39;challenge&#39;: &#39;true&#39;, &#39;kind&#39;: &#39;HTPasswdPasswordIdentityProvider&#39;, &#39;filename&#39;: &#39;/etc/origin/master/htpasswd&#39;}]</span>
<span class="na">openshift_master_htpasswd_users</span><span class="o">=</span><span class="s">{&#39;developer&#39;: &#39;$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/&#39;,&#39;operator&#39;: &#39;$apr1$bKWroIXS$/xjq07zVg9XtH6/VKuh6r/&#39;}</span>
<span class="na">openshift_master_default_subdomain</span><span class="o">=</span><span class="s">&#39;cloudapps.35.158.172.55.nip.io&#39;</span>
<span class="na">openshift_router_selector</span><span class="o">=</span><span class="s">&#39;role=master&#39;</span>
<span class="na">openshift_registry_selector</span><span class="o">=</span><span class="s">&#39;role=infra&#39;</span>
<span class="na">openshift_metrics_install_metrics</span><span class="o">=</span><span class="s">false</span>
<span class="na">openshift_metrics_hawkular_hostname</span><span class="o">=</span><span class="s">&quot;hawkular-metrics.{{ openshift_master_default_subdomain }}&quot;</span>
<span class="na">openshift_metrics_cassandra_storage_type</span><span class="o">=</span><span class="s">pv</span>
<span class="na">openshift_metrics_cassandra_pvc_size</span><span class="o">=</span><span class="s">10Gi</span>
<span class="na">openshift_logging_install_logging</span><span class="o">=</span><span class="s">false</span>
<span class="na">openshift_logging_es_pvc_size</span><span class="o">=</span><span class="s">10Gi</span>
<span class="na">openshift_logging_es_pvc_dynamic</span><span class="o">=</span><span class="s">true</span>
<span class="hll"><span class="na">openshift_storage_glusterfs_namespace</span><span class="o">=</span><span class="s">app-storage</span>
</span><span class="hll"><span class="na">openshift_storage_glusterfs_image</span><span class="o">=</span><span class="s">rhgs3/rhgs-server-rhel7</span>
</span><span class="hll"><span class="na">openshift_storage_glusterfs_version</span><span class="o">=</span><span class="s">3.2.0-7</span>
</span><span class="hll"><span class="na">openshift_storage_glusterfs_heketi_image</span><span class="o">=</span><span class="s">rhgs3/rhgs-volmanager-rhel7</span>
</span><span class="hll"><span class="na">openshift_storage_glusterfs_heketi_version</span><span class="o">=</span><span class="s">3.2.0-11</span>
</span><span class="na">openshift_docker_additional_registries</span><span class="o">=</span><span class="s">mirror.lab:5555</span>
<span class="na">openshift_docker_insecure_registries</span><span class="o">=</span><span class="s">mirror.lab:5555</span>
<span class="na">oreg_url</span><span class="o">=</span><span class="s">http://mirror.lab:5555/openshift3/ose-${component}:${version}</span>
<span class="na">openshift_examples_modify_imagestreams</span><span class="o">=</span><span class="s">true</span>
<span class="na">openshift_disable_check</span><span class="o">=</span><span class="s">disk_availability,memory_availability</span>

<span class="k">[masters]</span>
<span class="na">master.lab openshift_public_hostname</span><span class="o">=</span><span class="s">35.158.172.55.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=35.158.172.55</span>

<span class="k">[masters:vars]</span>
<span class="na">openshift_schedulable</span><span class="o">=</span><span class="s">true</span>
<span class="na">openshift_node_labels</span><span class="o">=</span><span class="s">&quot;{&#39;role&#39;: &#39;master&#39;}&quot;</span>

<span class="k">[nodes]</span>
<span class="na">master.lab openshift_public_hostname</span><span class="o">=</span><span class="s">35.158.172.55.nip.io openshift_hostname=master.lab openshift_ip=10.0.1.100 openshift_public_ip=35.158.172.55</span>
<span class="na">infra-1.lab openshift_hostname</span><span class="o">=</span><span class="s">infra-1.lab openshift_ip=10.0.2.101 openshift_node_labels=&quot;{&#39;role&#39;: &#39;infra&#39;}&quot;</span>
<span class="na">infra-2.lab openshift_hostname</span><span class="o">=</span><span class="s">infra-2.lab openshift_ip=10.0.3.102 openshift_node_labels=&quot;{&#39;role&#39;: &#39;infra&#39;}&quot;</span>
<span class="na">infra-3.lab openshift_hostname</span><span class="o">=</span><span class="s">infra-3.lab openshift_ip=10.0.4.103 openshift_node_labels=&quot;{&#39;role&#39;: &#39;infra&#39;}&quot;</span>
<span class="na">node-1.lab openshift_hostname</span><span class="o">=</span><span class="s">node-1.lab openshift_ip=10.0.2.201 openshift_node_labels=&quot;{&#39;role&#39;: &#39;app&#39;}&quot;</span>
<span class="na">node-2.lab openshift_hostname</span><span class="o">=</span><span class="s">node-2.lab openshift_ip=10.0.3.202 openshift_node_labels=&quot;{&#39;role&#39;: &#39;app&#39;}&quot;</span>
<span class="na">node-3.lab openshift_hostname</span><span class="o">=</span><span class="s">node-3.lab openshift_ip=10.0.4.203 openshift_node_labels=&quot;{&#39;role&#39;: &#39;app&#39;}&quot;</span>
<span class="na">node-4.lab openshift_hostname</span><span class="o">=</span><span class="s">node-4.lab openshift_ip=10.0.4.204 openshift_node_labels=&quot;{&#39;role&#39;: &#39;app&#39;}&quot;</span>

<span class="hll"><span class="k">[glusterfs]</span>
</span><span class="hll"><span class="na">node-1.lab glusterfs_ip</span><span class="o">=</span><span class="s">10.0.2.201 glusterfs_zone=1 glusterfs_devices=&#39;[ &quot;/dev/xvdc&quot; ]&#39;</span>
</span><span class="hll"><span class="na">node-2.lab glusterfs_ip</span><span class="o">=</span><span class="s">10.0.3.202 glusterfs_zone=2 glusterfs_devices=&#39;[ &quot;/dev/xvdc&quot; ]&#39;</span>
</span><span class="hll"><span class="na">node-3.lab glusterfs_ip</span><span class="o">=</span><span class="s">10.0.4.203 glusterfs_zone=3 glusterfs_devices=&#39;[ &quot;/dev/xvdc&quot; ]&#39;</span>
</span></pre></div>


<p>The highlighted lines show the settings relevant for CNS deployment. In summary what is provided is:</p>
<ul>
<li>a hostgroup called <code>[glusterfs]</code> is created with all those OpenShift nodes that are designed to run CNS</li>
<li><em>(optional)</em> a custom name for the namespace is provided in which the CNS pods will live</li>
<li><em>(optional)</em> a specific name and version of the required container images to be used</li>
<li>information about available block devices, zone and <em>(optionally)</em> IP addresses for GlusterFS traffic for each host in the <code>[glusterfs]</code> group</li>
</ul>
<p>In every environment the following pre-requisites need to be met:</p>
<ul>
<li>the designated nodes have a valid Red Hat Gluster Storage Subscription</li>
<li>the device names in <code>glusterfs_devices</code> should contain no data or filesystem/LVM structures</li>
<li>there need to be at least 3 nodes in the <code>[glusterfs]</code> host group and these should also be part of the <code>[nodes]</code> group</li>
</ul>
<div class="admonition tip">
<p class="admonition-title">What is the zone ID for?</p>
<p>A zone identifies a failure domain in GlusterFS. In CNS data is by default always replicated 3 times. Reflecting these failure domains by zone IDs as arbitrary but distinct numerical values allows CNS to ensure that two copies are never stored on nodes in the same failure domain.</p>
<p>CNS will also work without zone definitions, but it&rsquo;s less smart. This information is considered when building new volumes, expanding existing volumes or replacing bricks in degraded volumes.<br />
An example for failure domains are AWS Availability Zones or physical servers sharing the same PDU.</p>
<p>In this lab environment we have 3 different zones, because the nodes are residing in 3 distinct AWS Availability Zones.</p>
</div>
<hr />
<h2 id="run-the-installer">Run the installer<a class="headerlink" href="#run-the-installer" title="Permanent link">#</a></h2>
<p>&#8680; First ensure that from an Ansible-perspective the required nodes are reachable:</p>
<div class="codehilite"><pre><span></span>ansible -i /etc/ansible/ocp-with-glusterfs glusterfs -m ping
</pre></div>


<p>All 3 OpenShift application nodes should respond:</p>
<div class="codehilite"><pre><span></span>node-3.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-1.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
node-2.lab | SUCCESS =&gt; {
    &quot;changed&quot;: false,
    &quot;ping&quot;: &quot;pong&quot;
}
</pre></div>


<p>&#8680; Run the CNS installation playbook that ships as part of <code>openshift-ansible</code>:</p>
<div class="codehilite"><pre><span></span>ansible-playbook -i /etc/ansible/ocp-with-glusterfs \
/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-glusterfs/config.yml
</pre></div>


<p>!!! Danger &ldquo;Important&rdquo;:<br />
    In this lab exercise you are directly invoking the CNS-related playbooks of <code>openshift-ansible</code>. This is <strong>not supported</strong> in production as of yet.<br />
    The supported way to deploy CNS with <code>openshift-ansible</code> is to include the configuration in the inventory file from the very beginning and deploy it with OpenShift. Special care has been taken in this lab so that it works with an existint OCP deployment.</p>
<div class="codehilite"><pre><span></span>Official support for post-deploy CNS installation with this method is planned for one of the next minor releases.
</pre></div>


<p>The installation will take approximately 4-5 minutes. In the meantime proceed with the next paragraph.</p>
<hr />
<h2 id="what-happens-in-the-background">What happens in the background<a class="headerlink" href="#what-happens-in-the-background" title="Permanent link">#</a></h2>
<p>CNS provides software-defined storage hosted on OpenShift used by OpenShift. In particular it&rsquo;s based on <em>Red Hat Gluster Storage</em> running in OpenShift pods with direct access the host&rsquo;s network and storage device.<br />
Gluster effectively virtualizes the local storage capacity of each node into a flat namespace providing scale-out, federated file storage transparently as a single mount point across the network.</p>
<p>During the deployment you can either use the web console or the CLI tools to monitor what&rsquo;s created.</p>
<p>When logging to the Web UI as <code>operator</code>, selecting the project called <strong>app-storage</strong> and navigating to <strong>Applications</strong> &gt; <strong>Pods</strong> it will look similar to this:</p>
<p><a href="../img/openshift_cns_deploy_1.png"><img alt="CNS Deployment" src="../img/openshift_cns_deploy_1.png" /></a></p>
<p>When done, going back to the <strong>Overview</strong> page it should look like this:</p>
<p><a href="../img/openshift_cns_deploy_2.png"><img alt="CNS Deployment" src="../img/openshift_cns_deploy_2.png" /></a></p>
<p>The pods named <code>glusterfs-...</code> are running GlusterFS in containers which have super-privileged access to the block device(s) and networking device (shares same IP address, reserves certain ports) of the container host:</p>
<p><a href="../img/cns_diagram_pod.svg"><img alt="GlusterFS pods in CNS in detail." src="../img/cns_diagram_pod.svg" /></a></p>
<p>At least 3, but potentially more, GlusterFS pods form a cluster (a <em>Trusted Storage Pool</em> in GlusterFS terminology) across which storage will transparently be replicated in a synchronous fashion.</p>
<p>After some time you will see a 4th pod come up. <em>heketi</em> is a component that will expose an API for GlusterFS to OpenShift. This allows OpenShift to dynamically allocate storage from CNS in a programmatic fashion. See below for a visualization. Note that for simplicity, in our lab environment heketi may be scheduled on one of the OpenShift App nodes. In production <code>heketi</code> should be configured to run on OpenShift Infrastructure nodes.</p>
<p><a href="../img/cns_diagram_heketi.svg"><img alt="heketi pod running in CNS" src="../img/cns_diagram_heketi.svg" /></a></p>
<p>!!! Tip:<br />
    For the extra curious - or if you still need to beat some time - here is a more detailed list of actions that are performed:</p>
<div class="codehilite"><pre><span></span>- an OpenShift namespace is selected / created for the CNS pods
- the permission to run CNS pods in `privileged` mode is added to the *ServiceAccount* used by CNS
- a JSON structure is created that lays out a map of OpenShift nodes that will run CNS (called a `topology`), including information about their network and available disk devices
- passwords are generated for user and administrative accounts of the CNS API server (`heketi`)
- a set of templates are used to create an intermediary instance of *heketi*
- the designated CNS nodes are labeled with a specific key-value pair
- a `DaemonSet` configuration is created that causes a CNS pod to launch on every node matching that key-value pair
- the intermediary instance of *heketi* uses the JSON-formatted topology to initialize the CNS pods (creating LVM and directory structures on the supplied block devices)
- the intermediary instance of *heketi* is used to initiate GlusterFS peering so the CNS pods form a GlusterFS *Trusted Storage Pool*
- the intermediary instance of *heketi* is used to create a GlusterFS volume to host the *heketi*-internal database (based on `BoltDB`)
- a copy of the database of the intermediary *heketi* instance is created on that volume
- the intermediary instance is terminated and a new *heketi* instance is deployed mounting the GlusterFS volume and the database
- a `Service` is created in OpenShift to expose the API of *heketi* to the network
- a `Route` is created in OpenShift to make the *heketi* pod reachable from the outside
</pre></div>


<hr />
<p>By now the installation should have completed successfully with output similar to the below:</p>
<div class="codehilite"><pre><span></span>PLAY RECAP ***************************************************************************************************************
infra-1.lab                : ok=72   changed=3    unreachable=0    failed=0
infra-2.lab                : ok=72   changed=3    unreachable=0    failed=0
infra-3.lab                : ok=72   changed=3    unreachable=0    failed=0
localhost                  : ok=9    changed=0    unreachable=0    failed=0
master.lab                 : ok=137  changed=34   unreachable=0    failed=0
node-1.lab                 : ok=86   changed=4    unreachable=0    failed=0
node-2.lab                 : ok=86   changed=4    unreachable=0    failed=0
node-3.lab                 : ok=86   changed=4    unreachable=0    failed=0
node-4.lab                 : ok=72   changed=3    unreachable=0    failed=0

Wednesday 20 September 2017  11:44:29 +0000 (0:00:00.157)       0:04:33.834 ***
===============================================================================
openshift_storage_glusterfs : Wait for GlusterFS pods ------------------ 83.80s
openshift_storage_glusterfs : Wait for deploy-heketi pod --------------- 31.64s
openshift_version : Get available atomic-openshift version ------------- 19.64s
openshift_storage_glusterfs : Wait for heketi pod ---------------------- 10.90s
openshift_storage_glusterfs : Wait for copy job to finish -------------- 10.88s
openshift_storage_glusterfs : Delete deploy resources ------------------- 5.20s
openshift_storage_glusterfs : Load heketi topology ---------------------- 4.81s
openshift_facts : Ensure various deps are installed --------------------- 4.57s
openshift_storage_glusterfs : Create heketi DB volume ------------------- 3.55s
openshift_version : Get available atomic-openshift version -------------- 3.17s
openshift_storage_glusterfs : Deploy deploy-heketi pod ------------------ 3.06s
openshift_storage_glusterfs : Deploy heketi pod ------------------------- 3.04s
openshift_storage_glusterfs : Label GlusterFS nodes --------------------- 2.14s
openshift_storage_glusterfs : Deploy GlusterFS pods --------------------- 1.75s
openshift_docker_facts : Set docker facts ------------------------------- 1.61s
openshift_storage_glusterfs : Add service accounts to privileged SCC ---- 1.44s
openshift_storage_glusterfs : Verify target namespace exists ------------ 1.43s
openshift_docker_facts : Set docker facts ------------------------------- 1.39s
openshift_facts : Gather Cluster facts and set is_containerized if needed --- 1.25s
openshift_storage_glusterfs : Create heketi service account ------------- 1.08s
</pre></div>


<p>Notice there are 0 failed tasks on any host.</p>
<hr />
<h2 id="test-container-native-storage">Test Container-native Storage<a class="headerlink" href="#test-container-native-storage" title="Permanent link">#</a></h2>
<p>At this stage you have deployed CNS. Let’s verify all components are in place.</p>
<p>&#8680; If not already there on the CLI on the Master node change back to the <code>app-storage</code> namespace:</p>
<div class="codehilite"><pre><span></span>oc project app-storage
</pre></div>


<p>&#8680; First, verify that a new OpenShift <code>StorageClass</code> has been created:</p>
<div class="codehilite"><pre><span></span>oc get storageclass
</pre></div>


<p>The <code>StorageClass</code> is used later in OpenShift request storage from CNS:</p>
<div class="codehilite"><pre><span></span>NAME                TYPE
glusterfs-storage   kubernetes.io/glusterfs
</pre></div>


<p>&#8680; Next, list all running pods:</p>
<div class="codehilite"><pre><span></span>oc get pods -o wide
</pre></div>


<p>You should see all pods up and running. Highlighted below are pods that run GlusterFS containerized sharing the IP of the OpenShift node they are running on.</p>
<div class="codehilite"><pre><span></span>NAME                      READY     STATUS    RESTARTS   AGE       IP           NODE
<span class="hll">glusterfs-storage-6p5zh   1/1       Running   0          57m       10.0.4.203   node-3.lab
</span><span class="hll">glusterfs-storage-9mx29   1/1       Running   0          57m       10.0.3.202   node-2.lab
</span><span class="hll">glusterfs-storage-ww7s2   1/1       Running   0          57m       10.0.2.201   node-1.lab
</span>heketi-storage-1-h27cg    1/1       Running   0          55m       10.131.2.4   node-4.lab
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The exact pod names will be different in your environment, since they are auto-generated. Also the <em>heketi</em> pod might run on another node with another IP.</p>
</div>
<p>To expose heketi’s API a <code>Service</code> named <em>heketi</em> has been generated in OpenShift.</p>
<p>&#8680; Check the <code>Service</code> with:</p>
<div class="codehilite"><pre><span></span>oc get service/heketi-storage
</pre></div>


<p>The output should look similar to the below:</p>
<div class="codehilite"><pre><span></span>NAME             CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
heketi-storage   172.30.225.240   &lt;none&gt;        8080/TCP   1h
</pre></div>


<p>To also use heketi outside of OpenShift in addition to the <code>Service</code> a <code>Route</code> has been deployed.</p>
<p>&#8680; Display the route with:</p>
<div class="codehilite"><pre><span></span>oc get route/heketi-storage
</pre></div>


<p>The output should look similar to the below:</p>
<div class="codehilite"><pre><span></span>NAME             HOST/PORT                                                   PATH      SERVICES         PORT      TERMINATION   WILDCARD
heketi-storage   heketi-storage-app-storage.cloudapps.35.158.172.55.nip.io             heketi-storage   &lt;all&gt;                   None
</pre></div>


<p>Based on this <em>heketi</em> will be available on the heketi API URL, in this example:<br />
http://<em>heketi-storage-app-storage.cloudapps.35.158.172.55.nip.io</em></p>
<p>!!! Note:<br />
    In your environment the URL will be slightly different. It will contain the public IPv4 address of your deployment, dynamically resolved by the nip.io service.</p>
<p>&#8680; You may verify this trivial health check using <code>curl</code> (and an in-line <code>oc</code> command to dynamically retrieve the <code>route</code> for easy copy&amp;paste):</p>
<div class="codehilite"><pre><span></span>curl http://$(oc get route/heketi-storage -o jsonpath=&#39;{.spec.host}&#39;)/hello
</pre></div>


<p>This should say:</p>
<div class="codehilite"><pre><span></span>Hello from Heketi
</pre></div>


<p>This verifies heketi is running. To ensure it&rsquo;s functional and has been set up with authentication we are going to query it with the heketi CLI client.</p>
<p>First, the client needs to know the heketi API URL above and the password for the built-in <code>admin</code> user.</p>
<p><a name="heketi-env-setup"></a></p>
<p>&#8680; View the generated <code>admin</code> password for <em>heketi</em> from the pod configuration using <strong>YOUR specific pod name</strong>, e.g.</p>
<div class="codehilite"><pre><span></span>oc describe pod/heketi-storage-1-h27cg | grep HEKETI_ADMIN_KEY
</pre></div>


<p>Example output:</p>
<div class="codehilite"><pre><span></span>HEKETI_ADMIN_KEY:           sV7MHQ7S08N7ONJz1nnt/l/wBSK3L3w0xaEqDzG3YM4=
</pre></div>


<p>This information, the heketi user name, the API URL, and the password is needed whenever you want to use the <code>heketi-cli</code> client. So it&rsquo;s a good idea to store this in environment variables.</p>
<p>&#8680; Enter the following lines in your shell to conveniently retrieve and store the heketi API URL, the password of of the <code>admin</code> user and the user name set to <code>admin</code>:</p>
<div class="codehilite"><pre><span></span>HEKETI_POD=$(oc get pods -l glusterfs=heketi-storage-pod -o jsonpath=&quot;{.items[0].metadata.name}&quot;)
export HEKETI_CLI_SERVER=http://$(oc get route/heketi-storage -o jsonpath=&#39;{.spec.host}&#39;)
export HEKETI_CLI_USER=admin
export HEKETI_CLI_KEY=$(oc get pod/$HEKETI_POD -o jsonpath=&#39;{.spec.containers[0].env[?(@.name==&quot;HEKETI_ADMIN_KEY&quot;)].value}&#39;)
</pre></div>


<p>&#8680; You are now able to use the heketi CLI tool:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster list
</pre></div>


<p>This should list at least one cluster by it&rsquo;s UUID:</p>
<div class="codehilite"><pre><span></span>Clusters:
fb67f97166c58f161b85201e1fd9b8ed
</pre></div>


<p>!!! Note:<br />
    The UUID is auto-generated and will be different for you.</p>
<p>&#8680; Use the <strong>UUID unique to your environment</strong> and obtain more information about it:</p>
<div class="codehilite"><pre><span></span>heketi-cli cluster info fb67f97166c58f161b85201e1fd9b8e
</pre></div>


<p>There should be 3 nodes and 1 volume, again displayed with their UUIDs.</p>
<div class="codehilite"><pre><span></span>Cluster id: fb67f97166c58f161b85201e1fd9b8ed
Nodes:
22cbcd136fa40ffe766a13f305cc1e3b
bfc006b571e85a083118054233bfb16d
c5979019ac13b9fe02f4e4e2dc6d62cb
Volumes:
2415fba2b9364a65711da2a8311a663a
</pre></div>


<p>&#8680; To display a comprehensive overview of everything heketi knows about query it&rsquo;s topology:</p>
<div class="codehilite"><pre><span></span>heketi-cli topology info
</pre></div>


<p>You will get lengthy output that shows what nodes and disk devices CNS has used to deploy a containerised GlusterFS cluster.</p>
<div class="codehilite"><pre><span></span>Cluster Id: fb67f97166c58f161b85201e1fd9b8ed

Volumes:

Name: heketidbstorage
Size: 2
Id: 2415fba2b9364a65711da2a8311a663a
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Mount: 10.0.2.201:heketidbstorage
Mount Options: backup-volfile-servers=10.0.3.202,10.0.4.203
Durability Type: replicate
Replica: 3
Snapshot: Disabled

Bricks:
  Id: 55851d8ab270112c07ab7a38d55c8045
  Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick
  Size (GiB): 2
  Node: bfc006b571e85a083118054233bfb16d
  Device: 41b8a921f8e6d31cb04c7dd35b6b4cf2

  Id: 67161e0e607c38677a0ef3f617b8dc1e
  Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick
  Size (GiB): 2
  Node: 22cbcd136fa40ffe766a13f305cc1e3b
  Device: 8ea71174529a35f41fc0d1b288da6299

  Id: a8bf049dcea2d5245b64a792d4b85e6b
  Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick
  Size (GiB): 2
  Node: c5979019ac13b9fe02f4e4e2dc6d62cb
  Device: 2a49883a5cb39c3b845477ff85a729ba


Nodes:

Node Id: 22cbcd136fa40ffe766a13f305cc1e3b
State: online
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Zone: 2
Management Hostname: node-2.lab
Storage Hostname: 10.0.3.102
Devices:
Id:8ea71174529a35f41fc0d1b288da6299   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
  Bricks:
    Id:67161e0e607c38677a0ef3f617b8dc1e   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_8ea71174529a35f41fc0d1b288da6299/brick_67161e0e607c38677a0ef3f617b8dc1e/brick

Node Id: bfc006b571e85a083118054233bfb16d
State: online
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Zone: 3
Management Hostname: node-3.lab
Storage Hostname: 10.0.4.103
Devices:
Id:41b8a921f8e6d31cb04c7dd35b6b4cf2   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
  Bricks:
    Id:55851d8ab270112c07ab7a38d55c8045   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_41b8a921f8e6d31cb04c7dd35b6b4cf2/brick_55851d8ab270112c07ab7a38d55c8045/brick

Node Id: c5979019ac13b9fe02f4e4e2dc6d62cb
State: online
Cluster Id: fb67f97166c58f161b85201e1fd9b8ed
Zone: 1
Management Hostname: node-1.lab
Storage Hostname: 10.0.2.101
Devices:
Id:2a49883a5cb39c3b845477ff85a729ba   Name:/dev/xvdc           State:online    Size (GiB):49      Used (GiB):2       Free (GiB):47
  Bricks:
    Id:a8bf049dcea2d5245b64a792d4b85e6b   Size (GiB):2       Path: /var/lib/heketi/mounts/vg_2a49883a5cb39c3b845477ff85a729ba/brick_a8bf049dcea2d5245b64a792d4b85e6b/brick
</pre></div>


<hr />
<h2 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">#</a></h2>
<p>With this we have deployed a simple 3 node CNS cluster on top of OpenShift running as regular pods on app nodes. We have also verified that the API service is running and has correctly recognized the storage cluster.</p>
                
                  
                
              
              
                
              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../module-1-install/" title="Module 1 - Combined OpenShift and CNS Installation" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Module 1 - Combined OpenShift and CNS Installation
              </span>
            </div>
          </a>
        
        
          <a href="../module-3-cns-for-apps/" title="Module 3 - Persistent Storage for Apps" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Module 3 - Persistent Storage for Apps
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright ©2018 Red Hat, Inc.
          </div>
        
        powered by
        <a href="http://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
        
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.cae2244d.js"></script>
      
      <script>app.initialize({version:"0.17.2",url:{base:".."}})</script>
      
    
    
      
    
  </body>
</html>